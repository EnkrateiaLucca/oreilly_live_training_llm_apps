{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-openai\n",
    "!pip install langchainhub\n",
    "!pip install pypdf\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set OPENAI API Key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your openai key\"\n",
    "\n",
    "# OR (load from .env file)\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# make sure you have python-dotenv installed\n",
    "# load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a study workflow using Jupyter Notebooks, LLMs, and langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 0}, page_content='Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢ Qiushi Sun♠ Qipeng Guo♢\\nJiawen Wu♢ Xipeng Qiu♢∗ Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware, consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗ Corresponding author.\\nUnknows\\nKnowsUnknows\\nKnows\\nKnown Knows Known Unknows\\nUnknown UnknowsUnknown Knows\\nUnlock\\nFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increase\\narXiv:2305.18153v2  [cs.CL]  30 May 2023'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 1}, page_content='this ratio, resulting in improved performance on\\nNLP tasks. We focus on the ratio of “Known Un-\\nknows” to “Unknown Unknows”, which indicates\\nthe model’s self-knowledge level, specifically un-\\nderstanding its own limitations and deficiencies in\\nthe unknows.\\nExisting datasets such as SQuAD2.0 (Rajpurkar\\net al., 2018) and NewsQA (Trischler et al., 2017),\\nwidely used in question answering (QA), have been\\nutilized to test the self-knowledge of models with\\nunanswerable questions. However, these questions\\nare context-specific and could become answerable\\nwhen supplemented with additional information.\\nSrivastava et al. (2022) attempted to address this by\\nevaluating LLMs’ competence in delineating their\\nknowledge boundaries, employing a set of 23 pairs\\nof answerable and unanswerable multiple-choice\\nquestions. They discovered that these models’ per-\\nformance barely surpassed that of random guessing.\\nKadavath et al. (2022) suggested probing the self-\\nknowledge of LLMs through the implementation\\nof a distinct \"Value Head\". Yet, this approach may\\nencounter difficulties when applied across varied\\ndomains or tasks due to task-specific training. Con-\\nsequently, we redirect our focus to the inherent\\nabilities of LLMs, and pose the pivotal question:\\n“Do large language models know what they don’t\\nknow?”.\\nIn this study, we investigate the self-knowledge\\nof LLMs using a novel approach. By gathering\\nreference sentences with uncertain meanings, we\\ncan determine whether the model’s responses re-\\nflect uncertainty using a text similarity algorithm.\\nWe quantified the model’s self-knowledge using\\nthe F1 score. To address the small and idiosyn-\\ncratic limitations of existing datasets, we created\\na new dataset called SelfAware. This dataset com-\\nprises 1,032 unanswerable questions, which are dis-\\ntributed across five distinct categories, along with\\nan additional 2,337 questions that are classified as\\nanswerable. Experimental results on GPT-3, In-\\nstructGPT, LLaMA, and other LLMs demonstrate\\nthat in-context learning and instruction tuning can\\neffectively enhance the self-knowledge of LLMs.\\nHowever, the self-knowledge exhibited by the cur-\\nrent state-of-the-art model, GPT-4, measures at\\n75.47%, signifying a notable disparity when con-\\ntrasted with human self-knowledge, which is rated\\nat 84.93%.\\nOur key contributions to this field are summa-\\nrized as follows:\\n• We have developed a new dataset,SelfAware,\\nthat comprises a diverse range of commonly\\nposed unanswerable questions.\\n• We propose an innovative evaluation tech-\\nnique based on text similarity to quantify the\\ndegree of uncertainty inherent in model out-\\nputs.\\n• Through our detailed analysis of 20 LLMs,\\nbenchmarked against human self-knowledge,\\nwe identified a significant disparity between\\nthe most advanced LLMs and humans 1.\\n2 Dataset Construction\\nTo conduct a more comprehensive evaluation of\\nthe model’s self-knowledge, we constructed a\\ndataset that includes a larger number and more di-\\nverse types of unanswerable questions than Know-\\nUnknowns dataset (Srivastava et al., 2022). To\\nfacilitate this, we collected a corpus of 2,858 unan-\\nswerable questions, sourced from online platforms\\nlike Quora and HowStuffWorks. These questions\\nwere meticulously evaluated by three seasoned an-\\nnotation analysts, each operating independently.\\nThe analysts were permitted to leverage external\\nresources, such as search engines. To ensure the va-\\nlidity of our dataset, we retained only the questions\\nthat all three analysts concurred were unanswerable.\\nThis rigorous process yielded a finalized collection\\nof 1,032 unanswerable questions.\\nIn pursuit of a comprehensive evaluation, we\\nopted for answerable questions drawn from three\\ndatasets: SQuAD (Rajpurkar et al., 2016), Hot-\\npotQA (Yang et al., 2018), and TriviaQA (Joshi\\net al., 2017). Our selection was guided by Sim-\\nCSE (Gao et al., 2021), which allowed us to iden-\\ntify and select the answerable questions semanti-\\ncally closest to the unanswerable ones. From these'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 1}, page_content='et al., 2017). Our selection was guided by Sim-\\nCSE (Gao et al., 2021), which allowed us to iden-\\ntify and select the answerable questions semanti-\\ncally closest to the unanswerable ones. From these\\nsources, we accordingly drew samples of 1,487,\\n182, and 668 questions respectively, amassing a\\ntotal of 2,337. Given that these questions can be\\neffectively addressed using information available\\non Wikipedia, the foundational corpus for the train-\\ning of current LLMs, it is plausible to infer that\\nthe model possesses the requisite knowledge to\\ngenerate accurate responses to these questions.\\nOur dataset, christened SelfAware, incorporates\\n1,032 unanswerable and 2,337 answerable ques-\\ntions. To reflect real-world distribution, our dataset\\n1The code pertinent to our study can be accessed\\nhttps://github.com/yinzhangyue/SelfAware'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 2}, page_content='Category Description Example Percentage\\nNo scientific\\nconsensus\\nThe answer is still up\\nfor debate, with no consensus\\nin scientific community.\\n“Are we alone in the universe,\\nor will we discover alien\\nlife at some point?”\\n25%\\nImagination The question are about people’s\\nimaginations of the future.\\n\"What will the fastest form of\\ntransportation be in 2050?\" 15%\\nCompletely\\nsubjective\\nThe answer depends on\\npersonal preference.\\n\"Would you rather be shot\\ninto space or explore the\\ndeepest depths of the sea?\"\\n27%\\nToo many\\nvariables\\nThe question with too\\nmany variables cannot\\nbe answered accurately.\\n“John made 6 dollars mowing lawns\\nand 18 dollars weed eating.\\nIf he only spent 3 or 5 dollar a week,\\nhow long would the money last him?”\\n10%\\nPhilosophical\\nThe question can yield\\nmultiple responses, but it\\nlacks a definitive answer.\\n“How come god was\\nborn from nothingness?” 23%\\nTable 1: Unanswerable questions in the SelfAware dataset that span across multiple categories.\\ncontains a proportion of answerable questions that\\nis twice as large as the volume of unanswerable\\nones. Nevertheless, to ensure the feasibility of test-\\ning, we have purposefully capped the number of\\nanswerable questions.\\n2.1 Dataset Analysis\\nTo gain insight into the reasons precluding a cer-\\ntain answer, we undertook a manual analysis of\\n100 randomly selected unanswerable questions. As\\ntabulated in Table 1, we have broadly segregated\\nthese questions into five distinctive categories. “No\\nScientific Consensus\" encapsulates questions that\\nignite ongoing debates within the scientific com-\\nmunity, such as those concerning the universe’s\\norigin. “Imagination\" includes questions involving\\nspeculative future scenarios, like envisaged events\\nover the next 50 years. “Completely Subjective\"\\ncomprises questions that are inherently personal,\\nwhere answers depend heavily on individual predis-\\npositions. “Too Many Variables\" pertains to mathe-\\nmatical problems that become unsolvable owing to\\nthe overwhelming prevalence of variables. Lastly,\\n“Philosophical\" represents questions of a profound,\\noften metaphysical, nature that resist concrete an-\\nswers. Ideally, upon encountering such questions,\\nthe model should express uncertainty instead of\\ndelivering conclusive responses.\\n3 Evaluation Method\\nThis section elucidates the methodology employed\\nfor assessing self-knowledge in the generated text.\\nIn order to achieve this, we define a similarity func-\\ntion, fsim, to compute the similarity, S, between\\na given sentence, t, and a collection of reference\\nsentences, U = {u1, u2, . . . , un}, endowed with\\nuncertain meanings.\\nSi = fsim(t, ui). (1)\\nWhenever any Si surpasses a pre-determined\\nthreshold T , we perceive the text t as encompass-\\ning uncertain meanings, thereby eliminating the\\nneed for manual evaluation of the response.\\nGiven the substantial disparity in the volume of\\nanswerable and unanswerable questions in Self-\\nAware, we adopt the F1 score as a measure of\\nLLMs’ self-knowledge. Our focus rests on identi-\\nfying unanswerable questions, hence we designate\\nthem as positive cases and categorize answerable\\nquestions as negative cases.\\n4 Experiment\\n4.1 Model\\nWe conduct a sequence of experiments to evaluate\\nthe degree of self-knowledge manifested by various\\nLLMs, including GPT-3 (Brown et al., 2020) and\\nInstructGPT (Ouyang et al., 2022) series, as well\\nas the recent LLaMA (Touvron et al., 2023) and\\nits derivative models, namely Alpaca (Taori et al.,\\n2023) and Vicuna (Chiang et al., 2023). Our in-\\nvestigative approach employed three distinct input\\nforms: Direct, Instruction, and In-Context Learn-\\ning (ICL), which is encapsulated in Appendix A.4.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 3}, page_content='350M 1.3B 6.7B 175B\\n20\\n30\\n40\\n50\\n60\\n70F1 Scores\\n22.38\\n40.11\\n26.96\\n40.33\\n26.17\\n43.47\\n27.54\\n44.87\\nDirect\\n350M 1.3B 6.7B 175B\\n20\\n30\\n40\\n50\\n60\\n70F1 Scores\\n30.42\\n42.31\\n30.17\\n45.91\\n33.33\\n48.79\\n45.67\\n49.61\\nInstruction\\n350M 1.3B 6.7B 175B\\n20\\n30\\n40\\n50\\n60\\n70F1 Scores\\n34.27\\n47.93\\n36.27\\n48.42 47.24\\n55.81 55.5\\n65.12\\nIn-Context Learning\\nGPT-3\\nInstructGPT\\nModel\\nFigure 2: Experimental results using three different input forms on a series of models from GPT-3(ada, babbage,\\ncurie, and davinci) and InstructGPT(text-ada-001, text-babbage-001, text-curie-001, and text-davinci-001)\\n0 10 20 30 40 50 60 70 80\\nF1 Scores\\ndavinci\\ntext-davinci-001\\ntext-davinci-002\\ntext-davinci-003\\ngpt-3.5-turbo-0301\\ngpt-4-0314\\nHuman\\nModels\\n45.67\\n49.61\\n47.48\\n51.43\\n54.12\\n75.47\\n84.93\\nFigure 3: Comparison between the davinci series and\\nhuman self-knowledge in instruction input form.\\n4.2 Setting\\nWe devised the reference sentence set U through\\na process that combined automated generation by\\nLLMs and manual filtering, detailed further in Ap-\\npendix A.1. To quantify the similarity between\\ntarget and reference sentences, we utilized Sim-\\nCSE (Gao et al., 2021), setting the similarity thresh-\\nold to 0.75 during our experiments. An exploration\\nof threshold ablation is available in Appendix A.2.\\nTo counteract potential errors in similarity calcula-\\ntion induced by varying lengths of the target and\\nreference sentences, we employed a sliding win-\\ndow of length 5 to parse the target sentence into\\nsemantic chunks. During the generation process,\\nwe set the temperature to 0.7. We selected a ran-\\ndom sample of 100 instances for GPT-4, while the\\nremainder of the models were scrutinized using the\\nfull SelfAware dataset.\\n4.3 Human Self-Knowledge\\nTo establish a benchmark for human self-\\nknowledge, we engaged two volunteers and se-\\nlected 100 random samples from the SelfAware\\ndataset. The volunteers has 30 minutes to make\\ndavinci\\ntext-davinci-001text-davinci-002text-davinci-003\\ngpt-3.5-turbo-0301\\nModels\\n0\\n10\\n20\\n30\\n40\\n50\\n60F1 Scores\\n55.5\\n65.12 66.46 66.28\\n60.86\\nFigure 4: Experimental comparison of davinci series in\\nICL input form.\\njudgments on the same set of questions, yielding\\nan average F1 score of 84.93%, which we sub-\\nsequently adopted as the benchmark for human\\nself-knowledge. Detailed scores are available in\\nAppendix A.3.\\n4.4 Analysis\\nWe evaluate the manifestation of LLMs’ self-\\nknowledge, centering our investigation on three\\nfundamental dimensions: the size of the model,\\nthe impact of instruction tuning, and the influence\\nexerted by different input forms.\\nModel Size. Figure 2 illustrates the correlation\\nbetween model size and self-knowledge across var-\\nious LLMs. It is noteworthy that across all three\\ninput forms, an augmentation in model parameter\\nsize is associated with an elevation in the F1 Score,\\nwith the most conspicuous enhancement manifest-\\ning in the ICL input form. Therefore, our analysis\\nindicates that an LLM’s self-knowledge tends to\\nenhance with increasing model size, a trend consis-\\ntent with the scaling law.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 4}, page_content='LLaMA-7B Alpaca-7B Vicuna-7B LLaMA-13B Alpaca-13B Vicuna-13B LLaMA-30B LLaMA-65B\\nModels\\n0\\n10\\n20\\n30\\n40\\n50F1 Scores\\n28.57\\n35.87\\n42.78\\n30.12\\n37.44\\n47.84\\n30.3\\n46.89\\nFigure 5: Experimental results obtained from LLaMA\\nand its derived models, Alpaca and Vicuna in instruction\\ninput form.\\nInstruction Tuning. Figure 2 delineates that\\nmodels from the InstructGPT series exhibit a su-\\nperior level of self-knowledge compared to their\\nGPT-3 counterparts. Further evidence of model\\nenhancement is provided by Figure 4, where text-\\ndavinci models show significant improvement rela-\\ntive to the base davinci model. An additional com-\\nparative analysis, presented in Figure 5, evaluates\\nLLaMA against its derivative models. The results\\nunderscore a notable increase in self-knowledge\\nfor Alpaca and Vicuna upon instruction tuning, ex-\\nceeding their base model performances. Among\\nthese, Vicuna-13B outperforms the LLaMA-65B,\\ncorroborating the efficacy of instruction tuning for\\nenhancing model self-knowledge.\\nInput Forms. As shown in Figure 2, the incorpo-\\nration of instructions and examples serves to boost\\nthe self-knowledge of both the GPT-3 and Instruct-\\nGPT series. Specifically, ICL input form, providing\\nricher contextual information, contributes to a sig-\\nnificant enhancement in models’ self-knowledge.\\nThis impact is particularly noticeable in the davinci\\nmodel, where ICL facilitates a 27.96% improve-\\nment over the direct. Moreover, a comparison be-\\ntween Figure 3 and Figure 4 reveals that the in-\\nclusion of instructions and examples successfully\\nminimizes the performance disparity between the\\ndavinci and text-davinci models, suggesting an ac-\\nquisition of self-knowledge from the instructions\\nand provided examples.\\nCompared with Human. Figure 3 reveals that,\\nwithout supplementary samples, GPT-4 currently\\nperforms best among the tested models, achieving\\nan impressive F1 score of 75.47%. However, a no-\\nticeable gap becomes evident when comparing this\\ntext-ada-001\\ntext-babbage-001\\ntext-curie-001 text-davinci-001 text-davinci-002 text-davinci-003\\ngpt-3.5-turbo-0301\\ngpt-4-0314\\nModels\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40Accuracy\\n2.48\\n4.45 4.7\\n10.61\\n15.7\\n30.25\\n38.29\\n42.64\\nFigure 6: Accuracy of the InstructGPT series when\\nresponding to answerable questions in instruction input\\nform.\\nperformance to the human benchmark of 84.93%.\\nThis underscores the considerable potential that re-\\nmains for enhancing the self-knowledge level of\\nLLMs.\\nAnswerable Questions. Figure 6 traces the per-\\nformance evolution of the InstructGPT series in\\naddressing answerable questions, adhering to the\\nclosed-book question answering paradigm (Tou-\\nvron et al., 2023), where output accuracy is con-\\ntingent on the presence of the correct answer. Our\\nobservations underscore a steady enhancement in\\nQA task accuracy corresponding to an increase\\nin model parameter size and continuous learning.\\nParticularly, the accuracy of text-davinci-001 expe-\\nriences a significant ascent, scaling from a meager\\n2.48% in text-ada-001 to 10.61%, whereas GPT-4\\nmarks an even more striking jump to 42.64%.\\n5 Conclusion\\nThis study investigates the self-knowledge of\\nLLMs by evaluating their ability to identify unan-\\nswerable questions. Through the introduction of a\\nnovel dataset and an automated method for detect-\\ning uncertainty in the models’ responses, we are\\nable to accurately measure the self-knowledge of\\nLLMs such as GPT-3, InstructGPT and LLaMA.\\nOur results reveal that while these models possess\\na certain degree of self-knowledge, there is still\\nan apparent disparity in comparison to human self-\\nknowledge. This highlights the need for further\\nresearch in this area to enhance the ability of LLMs\\nto understand their own limitations on the unknows.\\nSuch efforts will lead to more accurate and reliable\\nresponses from LLMs, which will have a positive\\nimpact on their applications in diverse fields.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 5}, page_content='Limitations\\n• Generalization of reference sentences.At\\npresent, we have selected sentences with un-\\ncertain meanings exclusively from the GPT-3\\nand InstructGPT series, potentially overlook-\\ning uncertainty present in responses generated\\nby other LLMs. However, it is not feasible\\nto catalog all sentences with uncertain mean-\\nings exhaustively. As a direction for future\\nresearch, we propose to concentrate on the\\nautomated acquisition of more accurate refer-\\nence sentences to address this concern.\\n• Limitations of input forms: Our exami-\\nnation was confined to three unique input\\nforms: direct, instruction, and ICL. There\\nis burgeoning research aimed at bridging the\\ngap between models and human-like meth-\\nods of reasoning and problem-solving, includ-\\ning but not limited to approaches like Re-\\nflexion (Shinn et al., 2023), ToT (Yao et al.,\\n2023), MoT (Li and Qiu, 2023). Future en-\\ndeavors will integrate additional cognitive and\\ndecision-making methods to delve deeper into\\nthe self-knowledge exhibited by these LLMs.\\nEthics Statement\\nThe SelfAware dataset, meticulously curated to\\nevaluate LLMs’ ability to discern unanswerable\\nquestions, is composed of unanswerable questions\\nextracted from sources such as Quora and How-\\nStuffWorks, alongside answerable questions pro-\\ncured from three distinct open datasets. Every ques-\\ntion was thoroughly examined for relevance and\\nharmlessness. To ensure content validity, three an-\\nnotation analysts, compensated at local wage stan-\\ndards, dedicated regular working hours to content\\nreview.\\nThroughout our research process, we under-\\nscored the significance of privacy, data security,\\nand strict compliance with dataset licenses. In\\norder to protect data integrity, we implemented\\nanonymization and content filtration mechanisms.\\nOur adherence to OpenAI’s stipulations remained\\nunyielding for the usage of GPT-3 and InstructGPT\\nmodels, and likewise for Meta’s terms pertaining\\nto LLaMA models. We rigorously vetted the li-\\ncenses of the three publicly available datasets for\\ncompliance, ensuring that all our research method-\\nologies were in alignment with ethical standards at\\nthe institutional, national, and global levels.\\nAdhering to the CC-BY-SA-4.0 protocol, the\\ndataset, once publicly released, will be reserved\\nexclusively for research purposes. We pledge to\\npromptly and effectively address any concerns relat-\\ning to the dataset, while concurrently anticipating\\nresearchers to maintain high ethical standards in\\ntheir utilization of this data.\\nAcknowledgement\\nWe wish to express our gratitude to our colleagues\\nin the FudanNLP group whose insightful sugges-\\ntions, perspectives, and thought-provoking discus-\\nsions significantly contributed to this work. Our\\nsincere appreciation also extends to the anonymous\\nreviewers and area chairs, whose constructive feed-\\nback was instrumental in refining the quality of\\nour study. This work was supported by the Na-\\ntional Natural Science Foundation of China (No.\\n62236004 and No. 62022027) and CAAI-Huawei\\nMindSpore Open Fund.\\nReferences\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\\nChen, Eric Chu, Jonathan H. Clark, Laurent El\\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\\nJan Botha, James Bradbury, Siddhartha Brahma,\\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\\nCherry, Christopher A. Choquette-Choo, Aakanksha\\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 5}, page_content='cia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\\nShelby, Ambrose Slone, Daniel Smilkov, David R.\\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\\nVijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,\\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 6}, page_content='ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\\nreport.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In Ad-\\nvances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Process-\\ning Systems 2020, NeurIPS 2020, December 6-12,\\n2020, virtual.\\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\\nWilliam W Cohen. 2022. Program of thoughts\\nprompting: Disentangling computation from reason-\\ning for numerical reasoning tasks. ArXiv preprint,\\nabs/2211.12588.\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\\nsource chatbot impressing gpt-4 with 90%* chatgpt\\nquality.\\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\\nand Tushar Khot. 2022. Complexity-based prompt-\\ning for multi-step reasoning. ArXiv preprint,\\nabs/2210.00720.\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\\nSimCSE: Simple contrastive learning of sentence em-\\nbeddings. In Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Process-\\ning, pages 6894–6910, Online and Punta Cana, Do-\\nminican Republic. Association for Computational\\nLinguistics.\\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pages 1601–1611, Vancouver,\\nCanada. Association for Computational Linguistics.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\\nHenighan, Dawn Drain, Ethan Perez, Nicholas\\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\\nEli Tran-Johnson, et al. 2022. Language models\\n(mostly) know what they know. ArXiv preprint,\\nabs/2207.05221.\\nAitor Lewkowycz, Anders Andreassen, David Dohan,\\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,\\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo\\nGutman-Solo, et al. 2022. Solving quantitative\\nreasoning problems with language models. ArXiv\\npreprint, abs/2206.14858.\\nXiaonan Li and Xipeng Qiu. 2023. Mot: Pre-\\nthinking and recalling enable chatgpt to self-\\nimprove with memory-of-thoughts. ArXiv preprint,\\nabs/2305.05181.\\nOpenAI. 2023. Gpt-4 technical report.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow in-\\nstructions with human feedback. ArXiv preprint,\\nabs/2203.02155.\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\\nKnow what you don’t know: Unanswerable ques-\\ntions for SQuAD. In Proceedings of the 56th Annual\\nMeeting of the Association for Computational Lin-\\nguistics (Volume 2: Short Papers), pages 784–789,\\nMelbourne, Australia. Association for Computational\\nLinguistics.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. SQuAD: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Natu-\\nral Language Processing, pages 2383–2392, Austin,\\nTexas. Association for Computational Linguistics.\\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin\\nGopinath, Karthik Narasimhan, and Shunyu Yao.\\n2023. Reflexion: Language agents with verbal rein-\\nforcement learning.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 6}, page_content='Gopinath, Karthik Narasimhan, and Shunyu Yao.\\n2023. Reflexion: Language agents with verbal rein-\\nforcement learning.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\\nAdam R Brown, Adam Santoro, Aditya Gupta, Adrià\\nGarriga-Alonso, et al. 2022. Beyond the imitation\\ngame: Quantifying and extrapolating the capabilities\\nof language models. ArXiv preprint, abs/2206.04615.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model. https://\\ngithub.com/tatsu-lab/stanford_alpaca.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models. ArXiv\\npreprint, abs/2302.13971.\\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Har-\\nris, Alessandro Sordoni, Philip Bachman, and Kaheer\\nSuleman. 2017. NewsQA: A machine comprehen-\\nsion dataset. In Proceedings of the 2nd Workshop\\non Representation Learning for NLP, pages 191–200,\\nVancouver, Canada. Association for Computational\\nLinguistics.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 7}, page_content='Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\\nproves chain of thought reasoning in language mod-\\nels. ArXiv preprint, abs/2203.11171.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\\nand Denny Zhou. 2022. Chain of thought prompt-\\ning elicits reasoning in large language models. In\\nAdvances in Neural Information Processing Systems.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainable multi-hop question answering.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n2369–2380, Brussels, Belgium. Association for Com-\\nputational Linguistics.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nThomas L Griffiths, Yuan Cao, and Karthik\\nNarasimhan. 2023. Tree of thoughts: Deliberate\\nproblem solving with large language models. ArXiv\\npreprint, abs/2305.10601.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\\nLeast-to-most prompting enables complex reason-\\ning in large language models. ArXiv preprint,\\nabs/2205.10625.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 8}, page_content='A Appendix\\nA.1 Uncertainty Text\\nTo assemble a set of reference sentences, we ran-\\ndomly chose 100 entries from theSelfAwaredataset.\\nFor each model in the GPT-3 and InstructGPT se-\\nries, we conducted a preliminary test using the\\ndirect input form and manually curated sentences\\nthat displayed uncertainty. From this pre-test, we\\nprocured 16 sentences manifesting uncertain con-\\nnotations to serve as our reference sentences. After\\nnormalizing these sentences by eliminating punc-\\ntuation and converting to lowercase, we utilized\\nthem to compute similarity with target sentences\\nthroughout our experimental procedure.\\n1. The answer is unknown.\\n2. The answer is uncertain.\\n3. The answer is unclear.\\n4. There is no scientific evidence.\\n5. There is no definitive answer.\\n6. There is no right answer.\\n7. There is much debate.\\n8. There is no known case.\\n9. There is no concrete answer to this question.\\n10. There is no public information available.\\n11. It is impossible to know.\\n12. It is impossible to answer.\\n13. It is difficult to predict.\\n14. It is not known.\\n15. We do not know.\\n16. I’m not sure.\\nA.2 Threshold ablation\\nWe generated 100 new responses using the text-\\ndavinci-002 with direct input form and manually\\nfiltered out sentences that contained uncertainty.\\nWe then used SimCSE (Gao et al., 2021) to calcu-\\nlate the similarity between these sentences and the\\nreference sentences in Appendix A.1. We tested\\nvarious thresholds for filtering sentences with un-\\ncertain meanings and compared them to manually\\nThreshold Precision Recall F1\\n0.95 100.00 70.00 82.35\\n0.90 100.00 75.00 85.71\\n0.85 100.00 75.00 85.71\\n0.80 100.00 80.00 88.89\\n0.75 100.00 85.00 91.89\\n0.70 89.47 90.00 89.73\\n0.65 86.95 90.00 88.45\\nTable 2: Evaluation results comparing sentences with\\nuncertain meaning filtered by various thresholds.\\nHuman Precision Recall F1\\nV olunteer A 91.52 78.26 84.37\\nV olunteer B 96.36 76.81 85.48\\nTable 3: Evaluation results of 100 responses from two\\nvolunteers.\\nannotated sentences. We considered unanswerable\\nquestions as positive examples and calculated preci-\\nsion, recall, and F1 score. The results in Table 2 in-\\ndicate that a threshold of 0.75 produced the highest\\nF1 score, balancing precision and the inclusion of\\nother uncertain sentences. As a result, we selected\\n0.75 as the similarity threshold for subsequent ex-\\nperiments.\\nA.3 Human Self-Knowledge Test\\nThe evaluation results for the responses from our\\ninvited volunteers are presented in Table 3. The\\nF1 scores for the responses were high, indicating\\nthat both volunteers exhibited a strong level of self-\\nknowledge.\\nA.4 Template\\nThe input templates used in our experiments, Di-\\nrect, Instruction, and ICL, are illustrated in Fig-\\nures 7, 8, and 9, respectively. In the ICL template,\\nwe composed 3 answerable and 3 unanswerable\\nquestions and provided the corresponding answers\\nmanually.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 9}, page_content=\"Q: Are we alone in the universe, or will we discover alien life at some point?\\nA:\\nFigure 7: Direct input form.\\nWhen answering questions, interpret them literally and think them carefully. If the question is \\nunanswerable or unknowable, it is appropriate to say, “The answer is unknown.”\\nQ: Are we alone in the universe, or will we discover alien life at some point?\\nA:\\nFigure 8: Instruction input form.\\nQ: What is the nearest extraterrestrial civilization to us?\\nA: It is currently unknown if there are any extraterrestrial civilizations in proximity to Earth. Scientists \\nhave not yet found definitive evidence of the existence of extraterrestrial civilizations. Thus, the answer is \\nunknown.\\nQ: What is the highest building in New York?\\nA: The highest building in New York is the One World Trade Center. It stands at a height of 1,776 feet (541 \\nmeters). Thus, the answer is the One World Trade Center.\\nQ: Will nuclear war break out in the world in 2050?\\nA: It is impossible to predict with certainty if a nuclear war will break out in the world in 2050. The \\nlikelihood of such an event will depend on a wide range of factors, including international relations, \\npolitical tensions, and military capabilities. Thus, the answer is unknown.\\nQ: Alice's class is going on a field trip to the Ocean Park. There are 39 students and several teachers in the \\nclass. A bus can seat 20 people. How many buses do they need?\\nA: The number of buses needed for the field trip depends on the number of teachers. If there is only one \\nteacher, then two buses will suffice. However, additional buses will be necessary if there are two or more \\nteachers. Thus, the answer is unknown.\\nQ: Are Local H and For Against both from the United States?\\nA: Local H is a rock band from Zion, Illinois, United States. For Against is an American indie rock band \\nformed in 1984 in Lincoln, Nebraska. Both of these bands are from the United States. Thus, the answer is \\nyes.\\nQ: Gjetost is the national cheese of which country?\\nA:It is the national cheese of Norway, and it is a popular ingredient in traditional Norwegian cuisine. Thus, \\nthe answer is Norway.\\nQ: Are we alone in the universe, or will we discover alien life at some point?\\nA:\\nFigure 9: ICL input form.\")]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = \"./assets-resources/llm_paper_know_dont_know.pdf\"\n",
    "loader = PyPDFLoader(pdf_path) # LOAD\n",
    "pdf_docs = loader.load_and_split() # SPLIT\n",
    "pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 0}, page_content='Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢ Qiushi Sun♠ Qipeng Guo♢\\nJiawen Wu♢ Xipeng Qiu♢∗ Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware, consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗ Corresponding author.\\nUnknows\\nKnowsUnknows\\nKnows\\nKnown Knows Known Unknows\\nUnknown UnknowsUnknown Knows\\nUnlock\\nFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increase\\narXiv:2305.18153v2  [cs.CL]  30 May 2023')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_obj = pdf_docs[0]\n",
    "doc_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Do Large Language Models Know What They Don’t Know?\n",
       "Zhangyue Yin♢ Qiushi Sun♠ Qipeng Guo♢\n",
       "Jiawen Wu♢ Xipeng Qiu♢∗ Xuanjing Huang♢\n",
       "♢School of Computer Science, Fudan University\n",
       "♠Department of Mathematics, National University of Singapore\n",
       "{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\n",
       "{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\n",
       "Abstract\n",
       "Large language models (LLMs) have a wealth\n",
       "of knowledge that allows them to excel in vari-\n",
       "ous Natural Language Processing (NLP) tasks.\n",
       "Current research focuses on enhancing their\n",
       "performance within their existing knowledge.\n",
       "Despite their vast knowledge, LLMs are still\n",
       "limited by the amount of information they can\n",
       "accommodate and comprehend. Therefore, the\n",
       "ability to understand their own limitations on\n",
       "the unknows, referred to as self-knowledge,\n",
       "is of paramount importance. This study aims\n",
       "to evaluate LLMs’ self-knowledge by assess-\n",
       "ing their ability to identify unanswerable or\n",
       "unknowable questions. We introduce an auto-\n",
       "mated methodology to detect uncertainty in the\n",
       "responses of these models, providing a novel\n",
       "measure of their self-knowledge. We further in-\n",
       "troduce a unique dataset, SelfAware, consisting\n",
       "of unanswerable questions from five diverse cat-\n",
       "egories and their answerable counterparts. Our\n",
       "extensive analysis, involving 20 LLMs includ-\n",
       "ing GPT-3, InstructGPT, and LLaMA, discov-\n",
       "ering an intrinsic capacity for self-knowledge\n",
       "within these models. Moreover, we demon-\n",
       "strate that in-context learning and instruction\n",
       "tuning can further enhance this self-knowledge.\n",
       "Despite this promising insight, our findings also\n",
       "highlight a considerable gap between the capa-\n",
       "bilities of these models and human proficiency\n",
       "in recognizing the limits of their knowledge.\n",
       "“True wisdom is knowing what you don’t know.”\n",
       "–Confucius\n",
       "1 Introduction\n",
       "Recently, Large Language Models (LLMs) such\n",
       "as GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\n",
       "2023), and LLaMA (Touvron et al., 2023) have\n",
       "shown exceptional performance on a wide range\n",
       "of NLP tasks, including common sense reason-\n",
       "ing (Wei et al., 2022; Zhou et al., 2022) and mathe-\n",
       "∗ Corresponding author.\n",
       "Unknows\n",
       "KnowsUnknows\n",
       "Knows\n",
       "Known Knows Known Unknows\n",
       "Unknown UnknowsUnknown Knows\n",
       "Unlock\n",
       "Figure 1: Know-Unknow Quadrant. The horizontal axis\n",
       "represents the model’s memory capacity for knowledge,\n",
       "and the vertical axis represents the model’s ability to\n",
       "comprehend and utilize knowledge.\n",
       "matical problem-solving (Lewkowycz et al., 2022;\n",
       "Chen et al., 2022). Despite their ability to learn\n",
       "from huge amounts of data, LLMs still have lim-\n",
       "itations in their capacity to retain and understand\n",
       "information. To ensure responsible usage, it is cru-\n",
       "cial for LLMs to have the capability of recognizing\n",
       "their limitations and conveying uncertainty when\n",
       "responding to unanswerable or unknowable ques-\n",
       "tions. This acknowledgment of limitations, also\n",
       "known as “ knowing what you don’t know,” is a\n",
       "crucial aspect in determining their practical appli-\n",
       "cability. In this work, we refer to this ability as\n",
       "model self-knowledge.\n",
       "The Know-Unknow quadrant in Figure 1 il-\n",
       "lustrates the relationship between the model’s\n",
       "knowledge and comprehension. The ratio of\n",
       "“Known Knows” to “Unknown Knows” demon-\n",
       "strates the model’s proficiency in understanding\n",
       "and applying existing knowledge. Techniques\n",
       "such as Chain-of-Thought (Wei et al., 2022), Self-\n",
       "Consistency (Wang et al., 2022), and Complex\n",
       "CoT (Fu et al., 2022) can be utilized to increase\n",
       "arXiv:2305.18153v2  [cs.CL]  30 May 2023"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "Markdown(doc_obj.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings() # EMBED\n",
    "embeddings\n",
    "vectordb = Chroma.from_documents(pdf_docs, embedding=embeddings) # STORE\n",
    "\n",
    "\n",
    "# Definition of a [retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/#:~:text=A%20retriever%20is,Document's%20as%20output.):\n",
    "\n",
    "# > A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.\n",
    "retriever = vectordb.as_retriever() \n",
    "# retriever\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0)\n",
    "# source: https://python.langchain.com/v0.2/docs/tutorials/pdf_qa/#question-answering-with-rag\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# prompt\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What are the key components of the transformer architecture?',\n",
       " 'context': [Document(metadata={'page': 2, 'source': './assets-resources/attention-paper.pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       "  Document(metadata={'page': 1, 'source': './assets-resources/attention-paper.pdf'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence'),\n",
       "  Document(metadata={'page': 8, 'source': './assets-resources/attention-paper.pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       "  Document(metadata={'page': 0, 'source': './assets-resources/attention-paper.pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023')],\n",
       " 'answer': \"The key components of the Transformer architecture include the encoder and decoder stacks, each composed of identical layers. Each encoder layer has a multi-head self-attention mechanism and a position-wise fully connected feed-forward network, while the decoder includes an additional multi-head attention sub-layer that attends to the encoder's output. Residual connections and layer normalization are employed around each sub-layer to facilitate training.\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question_answer_chain\n",
    "# This method `create_stuff_documents_chain` [outputs an LCEL runnable](https://arc.net/l/quote/bnsztwth)\n",
    "query = \"What are the key components of the transformer architecture?\"\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The key components of the Transformer architecture include the encoder and decoder stacks, each composed of identical layers. Each encoder layer has a multi-head self-attention mechanism and a position-wise fully connected feed-forward network, while the decoder includes an additional multi-head attention sub-layer that attends to the encoder's output. Residual connections and layer normalization are employed around each sub-layer to facilitate training."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "final_answer = results[\"answer\"]\n",
    "\n",
    "Markdown(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- The paper introduces the Transformer, a new neural network architecture based solely on attention mechanisms, eliminating the need for recurrence and convolutions.\n",
       "- It demonstrates superior performance in machine translation tasks, achieving state-of-the-art BLEU scores on English-to-German and English-to-French translations.\n",
       "- The Transformer model is shown to generalize well to other tasks, such as English constituency parsing, with significantly reduced training time and costs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_summary = \"Write a simple bullet points summary about this paper\"\n",
    "\n",
    " # adding chat history so the model remembers previous questions\n",
    "output = rag_chain.invoke({\"input\": query_summary})\n",
    "\n",
    "Markdown(output[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  How does the self-attention mechanism in transformers differ from traditional sequence alignment methods?\n",
      "ANSWER The self-attention mechanism in transformers allows for modeling dependencies between all positions in a sequence simultaneously, without regard to their distance, while traditional sequence alignment methods, such as those used in recurrent neural networks (RNNs), typically process sequences in a sequential manner. This means that self-attention can capture relationships between distant positions more effectively and with a constant number of operations, whereas traditional methods may struggle with long-range dependencies due to their sequential nature. Additionally, self-attention enables greater parallelization during training, improving computational efficiency.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The self-attention mechanism in transformers allows for modeling dependencies between all positions in a sequence simultaneously, without regard to their distance, while traditional sequence alignment methods, such as those used in recurrent neural networks (RNNs), typically process sequences in a sequential manner. This means that self-attention can capture relationships between distant positions more effectively and with a constant number of operations, whereas traditional methods may struggle with long-range dependencies due to their sequential nature. Additionally, self-attention enables greater parallelization during training, improving computational efficiency.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_pdf(pdf_qa,query):\n",
    "    print(\"QUERY: \",query)\n",
    "    result = pdf_qa.invoke({\"input\": query})\n",
    "    answer = result[\"answer\"]\n",
    "    print(\"ANSWER\", answer)\n",
    "    return answer\n",
    "\n",
    "\n",
    "ask_pdf(rag_chain,\"How does the self-attention mechanism in transformers differ from traditional sequence alignment methods?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  Quiz me with 3 simple questions on the positional encodings and the role they play in transformers.\n",
      "ANSWER 1. What is the purpose of positional encodings in the Transformer architecture?  \n",
      "2. How do positional encodings help the model understand the order of input sequences?  \n",
      "3. What are the two types of positional encodings mentioned in the context of Transformers?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. What is the purpose of positional encodings in the Transformer architecture?  \\n2. How do positional encodings help the model understand the order of input sequences?  \\n3. What are the two types of positional encodings mentioned in the context of Transformers?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_questions = ask_pdf(rag_chain, \"Quiz me with 3 simple questions on the positional encodings and the role they play in transformers.\")\n",
    "\n",
    "quiz_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "MODEL=\"gpt-4o-mini\"\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class QA(BaseModel):\n",
    "    questions: List[str] = Field(description='List of questions about a given context.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template = f\"You transform unstructured questions about a topic into a structured list of questions.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\"Return ONLY a PYTHON list containing the questions in this text: {questions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain = chat_prompt | llm.with_structured_output(QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QA(questions=['What is the intrinsic capacity for self-knowledge within large language models (LLMs)?', 'How can in-context learning and instruction tuning enhance self-knowledge in LLMs?', 'What is the gap between the capabilities of LLMs and human proficiency in recognizing their knowledge limits?', \"What does the Know-Unknow quadrant illustrate about a model's knowledge and comprehension?\", \"What techniques can be utilized to improve the ratio of 'Known Knows' to 'Unknown Knows'?\", \"What is the significance of 'knowing what you don’t know' in LLMs?\", 'How do existing datasets like SQuAD2.0 and NewsQA test the self-knowledge of models?', \"What were the findings of Srivastava et al. (2022) regarding LLMs' performance on unanswerable questions?\", \"What is the proposed method to probe the self-knowledge of LLMs through a distinct 'Value Head'?\", \"What is the pivotal question posed in the study regarding LLMs' self-knowledge?\", 'How was the self-knowledge of LLMs quantified in this study?', 'What is the composition of the new dataset called SelfAware?', 'What are the key contributions of this study to the field of LLMs?', 'How was the dataset SelfAware constructed and what types of questions does it include?', 'What categories were identified in the manual analysis of unanswerable questions?', 'What is the evaluation method used to assess self-knowledge in the generated text?'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_information = \"\"\"\n",
    "nalysis, involving 20 LLMs includ-\n",
    "ing GPT-3, InstructGPT, and LLaMA, discov-\n",
    "ering an intrinsic capacity for self-knowledge\n",
    "within these models. Moreover, we demon-\n",
    "strate that in-context learning and instruction\n",
    "tuning can further enhance this self-knowledge.\n",
    "Despite this promising insight, our findings also\n",
    "highlight a considerable gap between the capa-\n",
    "bilities of these models and human proficiency\n",
    "in recognizing the limits of their knowledge.\n",
    "“True wisdom is knowing what you don’t know.”\n",
    "–Confucius\n",
    "1 Introduction\n",
    "Recently, Large Language Models (LLMs) such\n",
    "as GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\n",
    "2023), and LLaMA (Touvron et al., 2023) have\n",
    "shown exceptional performance on a wide range\n",
    "of NLP tasks, including common sense reason-\n",
    "ing (Wei et al., 2022; Zhou et al., 2022) and mathe-\n",
    "∗Corresponding author.UnknowsKnowsUnknowsKnowsKnown Knows Known UnknowsUnknown UnknowsUnknown KnowsUnlock\n",
    "Figure 1: Know-Unknow Quadrant. The horizontal axis\n",
    "represents the model’s memory capacity for knowledge,\n",
    "and the vertical axis represents the model’s ability to\n",
    "comprehend and utilize knowledge.\n",
    "matical problem-solving (Lewkowycz et al., 2022;\n",
    "Chen et al., 2022). Despite their ability to learn\n",
    "from huge amounts of data, LLMs still have lim-\n",
    "itations in their capacity to retain and understand\n",
    "information. To ensure responsible usage, it is cru-\n",
    "cial for LLMs to have the capability of recognizing\n",
    "their limitations and conveying uncertainty when\n",
    "responding to unanswerable or unknowable ques-\n",
    "tions. This acknowledgment of limitations, also\n",
    "known as “knowing what you don’t know,” is a\n",
    "crucial aspect in determining their practical appli-\n",
    "cability. In this work, we refer to this ability as\n",
    "model self-knowledge.\n",
    "The Know-Unknow quadrant in Figure 1 il-\n",
    "lustrates the relationship between the model’s\n",
    "knowledge and comprehension. The ratio of\n",
    "“Known Knows” to “Unknown Knows” demon-\n",
    "strates the model’s proficiency in understanding\n",
    "and applying existing knowledge. Techniques\n",
    "such as Chain-of-Thought (Wei et al., 2022), Self-\n",
    "Consistency (Wang et al., 2022), and Complex\n",
    "CoT (Fu et al., 2022) can be utilized to increase\n",
    "arXiv:2305.18153v2  [cs.CL]  30 May 2023\n",
    "this ratio, resulting in improved performance on\n",
    "NLP tasks. We focus on the ratio of “Known Un-\n",
    "knows” to “Unknown Unknows”, which indicates\n",
    "the model’s self-knowledge level, specifically un-\n",
    "derstanding its own limitations and deficiencies in\n",
    "the unknows.\n",
    "Existing datasets such as SQuAD2.0 (Rajpurkar\n",
    "et al., 2018) and NewsQA (Trischler et al., 2017),\n",
    "widely used in question answering (QA), have been\n",
    "utilized to test the self-knowledge of models with\n",
    "unanswerable questions. However, these questions\n",
    "are context-specific and could become answerable\n",
    "when supplemented with additional information.\n",
    "Srivastava et al. (2022) attempted to address this by\n",
    "evaluating LLMs’ competence in delineating their\n",
    "knowledge boundaries, employing a set of 23 pairs\n",
    "of answerable and unanswerable multiple-choice\n",
    "questions. They discovered that these models’ per-\n",
    "formance barely surpassed that of random guessing.\n",
    "Kadavath et al. (2022) suggested probing the self-\n",
    "knowledge of LLMs through the implementation\n",
    "of a distinct \"Value Head\". Yet, this approach may\n",
    "encounter difficulties when applied across varied\n",
    "domains or tasks due to task-specific training. Con-\n",
    "sequently, we redirect our focus to the inherent\n",
    "abilities of LLMs, and pose the pivotal question:\n",
    "“Do large language models know what they don’t\n",
    "know?”.\n",
    "In this study, we investigate the self-knowledge\n",
    "of LLMs using a novel approach. By gathering\n",
    "reference sentences with uncertain meanings, we\n",
    "can determine whether the model’s responses re-\n",
    "flect uncertainty using a text similarity algorithm.\n",
    "We quantified the model’s self-knowledge using\n",
    "the F1 score. To address the small and idiosyn-\n",
    "cratic limitations of existing datasets, we created\n",
    "a new dataset called SelfAware. This dataset com-\n",
    "prises 1,032 unanswerable questions, which are dis-\n",
    "tributed across five distinct categories, along with\n",
    "an additional 2,337 questions that are classified as\n",
    "answerable. Experimental results on GPT-3, In-\n",
    "structGPT, LLaMA, and other LLMs demonstrate\n",
    "that in-context learning and instruction tuning can\n",
    "effectively enhance the self-knowledge of LLMs.\n",
    "However, the self-knowledge exhibited by the cur-\n",
    "rent state-of-the-art model, GPT-4, measures at\n",
    "75.47%, signifying a notable disparity when con-\n",
    "trasted with human self-knowledge, which is rated\n",
    "at 84.93%.\n",
    "Our key contributions to this field are summa-\n",
    "rized as follows:\n",
    "• We have developed a new dataset, SelfAware,\n",
    "that comprises a diverse range of commonly\n",
    "posed unanswerable questions.\n",
    "• We propose an innovative evaluation tech-\n",
    "nique based on text similarity to quantify the\n",
    "degree of uncertainty inherent in model out-\n",
    "puts.\n",
    "• Through our detailed analysis of 20 LLMs,\n",
    "benchmarked against human self-knowledge,\n",
    "we identified a significant disparity between\n",
    "the most advanced LLMs and humans 1.\n",
    "2 Dataset Construction\n",
    "To conduct a more comprehensive evaluation of\n",
    "the model’s self-knowledge, we constructed a\n",
    "dataset that includes a larger number and more di-\n",
    "verse types of unanswerable questions than Know-\n",
    "Unknowns dataset (Srivastava et al., 2022). To\n",
    "facilitate this, we collected a corpus of 2,858 unan-\n",
    "swerable questions, sourced from online platforms\n",
    "like Quora and HowStuffWorks. These questions\n",
    "were meticulously evaluated by three seasoned an-\n",
    "notation analysts, each operating independently.\n",
    "The analysts were permitted to leverage external\n",
    "resources, such as search engines. To ensure the va-\n",
    "lidity of our dataset, we retained only the questions\n",
    "that all three analysts concurred were unanswerable.\n",
    "This rigorous process yielded a finalized collection\n",
    "of 1,032 unanswerable questions.\n",
    "In pursuit of a comprehensive evaluation, we\n",
    "opted for answerable questions drawn from three\n",
    "datasets: SQuAD (Rajpurkar et al., 2016), Hot-\n",
    "potQA (Yang et al., 2018), and TriviaQA (Joshi\n",
    "et al., 2017). Our selection was guided by Sim-\n",
    "CSE (Gao et al., 2021), which allowed us to iden-\n",
    "tify and select the answerable questions semanti-\n",
    "cally closest to the unanswerable ones. From these\n",
    "sources, we accordingly drew samples of 1,487,\n",
    "182, and 668 questions respectively, amassing a\n",
    "total of 2,337. Given that these questions can be\n",
    "effectively addressed using information available\n",
    "on Wikipedia, the foundational corpus for the train-\n",
    "ing of current LLMs, it is plausible to infer that\n",
    "the model possesses the requisite knowledge to\n",
    "generate accurate responses to these questions.\n",
    "Our dataset, christened SelfAware, incorporates\n",
    "1,032 unanswerable and 2,337 answerable ques-\n",
    "tions. To reflect real-world distribution, our dataset\n",
    "1The code pertinent to our study can be accessed\n",
    "https://github.com/yinzhangyue/SelfAware\n",
    "Category Description Example Percentage\n",
    "No scientific\n",
    "consensus\n",
    "The answer is still up\n",
    "for debate, with no consensus\n",
    "in scientific community.\n",
    "“Are we alone in the universe,\n",
    "or will we discover alien\n",
    "life at some point?”\n",
    "25%\n",
    "Imagination The question are about people’s\n",
    "imaginations of the future.\n",
    "\"What will the fastest form of\n",
    "transportation be in 2050?\" 15%\n",
    "Completely\n",
    "subjective\n",
    "The answer depends on\n",
    "personal preference.\n",
    "\"Would you rather be shot\n",
    "into space or explore the\n",
    "deepest depths of the sea?\"\n",
    "27%\n",
    "Too many\n",
    "variables\n",
    "The question with too\n",
    "many variables cannot\n",
    "be answered accurately.\n",
    "“John made 6 dollars mowing lawns\n",
    "and 18 dollars weed eating.\n",
    "If he only spent 3 or 5 dollar a week,\n",
    "how long would the money last him?”\n",
    "10%\n",
    "Philosophical\n",
    "The question can yield\n",
    "multiple responses, but it\n",
    "lacks a definitive answer.\n",
    "“How come god was\n",
    "born from nothingness?” 23%\n",
    "Table 1: Unanswerable questions in the SelfAware dataset that span across multiple categories.\n",
    "contains a proportion of answerable questions that\n",
    "is twice as large as the volume of unanswerable\n",
    "ones. Nevertheless, to ensure the feasibility of test-\n",
    "ing, we have purposefully capped the number of\n",
    "answerable questions.\n",
    "2.1 Dataset Analysis\n",
    "To gain insight into the reasons precluding a cer-\n",
    "tain answer, we undertook a manual analysis of\n",
    "100 randomly selected unanswerable questions. As\n",
    "tabulated in Table 1, we have broadly segregated\n",
    "these questions into five distinctive categories. “No\n",
    "Scientific Consensus\" encapsulates questions that\n",
    "ignite ongoing debates within the scientific com-\n",
    "munity, such as those concerning the universe’s\n",
    "origin. “Imagination\" includes questions involving\n",
    "speculative future scenarios, like envisaged events\n",
    "over the next 50 years. “Completely Subjective\"\n",
    "comprises questions that are inherently personal,\n",
    "where answers depend heavily on individual predis-\n",
    "positions. “Too Many Variables\" pertains to mathe-\n",
    "matical problems that become unsolvable owing to\n",
    "the overwhelming prevalence of variables. Lastly,\n",
    "“Philosophical\" represents questions of a profound,\n",
    "often metaphysical, nature that resist concrete an-\n",
    "swers. Ideally, upon encountering such questions,\n",
    "the model should express uncertainty instead of\n",
    "delivering conclusive responses.\n",
    "3 Evaluation Method\n",
    "This section elucidates the methodology employed\n",
    "for assessing self-knowledge in the generated text.\n",
    "In order to achieve this, we define a similarity func-\n",
    "tion, fsim, to compute the similarity, S, between\n",
    "a given sentence, t, and a collection of reference\n",
    "sentences, U ={u1, u2, . . . , un}, endowed with\n",
    "uncertain meanings\n",
    "\"\"\"\n",
    "questions_list = quiz_chain.invoke({\"questions\": context_information})\n",
    "questions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = questions_list.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the intrinsic capacity for self-knowledge within large language models (LLMs)?',\n",
       " 'How can in-context learning and instruction tuning enhance self-knowledge in LLMs?',\n",
       " 'What is the gap between the capabilities of LLMs and human proficiency in recognizing their knowledge limits?',\n",
       " \"What does the Know-Unknow quadrant illustrate about a model's knowledge and comprehension?\",\n",
       " \"What techniques can be utilized to improve the ratio of 'Known Knows' to 'Unknown Knows'?\",\n",
       " \"What is the significance of 'knowing what you don’t know' in LLMs?\",\n",
       " 'How do existing datasets like SQuAD2.0 and NewsQA test the self-knowledge of models?',\n",
       " \"What were the findings of Srivastava et al. (2022) regarding LLMs' performance on unanswerable questions?\",\n",
       " \"What is the proposed method to probe the self-knowledge of LLMs through a distinct 'Value Head'?\",\n",
       " \"What is the pivotal question posed in the study regarding LLMs' self-knowledge?\",\n",
       " 'How was the self-knowledge of LLMs quantified in this study?',\n",
       " 'What is the composition of the new dataset called SelfAware?',\n",
       " 'What are the key contributions of this study to the field of LLMs?',\n",
       " 'How was the dataset SelfAware constructed and what types of questions does it include?',\n",
       " 'What categories were identified in the manual analysis of unanswerable questions?',\n",
       " 'What is the evaluation method used to assess self-knowledge in the generated text?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  What is the intrinsic capacity for self-knowledge within large language models (LLMs)?\n",
      "ANSWER Large language models (LLMs) exhibit an intrinsic capacity for self-knowledge, which allows them to identify unanswerable or unknowable questions. This ability is assessed through their responses, reflecting uncertainty when faced with such questions. However, there remains a significant gap between the self-knowledge of LLMs and that of humans.\n",
      "QUERY:  How can in-context learning and instruction tuning enhance self-knowledge in LLMs?\n",
      "ANSWER In-context learning and instruction tuning can enhance self-knowledge in large language models (LLMs) by providing richer contextual information and structured guidance, which helps the models better understand their limitations. These techniques improve the models' ability to recognize unanswerable questions and convey uncertainty in their responses. Experimental results indicate that models like InstructGPT show significant improvements in self-knowledge when these methods are applied.\n",
      "QUERY:  What is the gap between the capabilities of LLMs and human proficiency in recognizing their knowledge limits?\n",
      "ANSWER The gap between the capabilities of large language models (LLMs) and human proficiency in recognizing their knowledge limits is significant, with LLMs like GPT-4 achieving a self-knowledge score of 75.47%, while humans score 84.93%. This indicates that LLMs still have room for improvement in understanding their own limitations. Further research is needed to enhance LLMs' self-knowledge capabilities.\n",
      "QUERY:  What does the Know-Unknow quadrant illustrate about a model's knowledge and comprehension?\n",
      "ANSWER The Know-Unknow quadrant illustrates the relationship between a model's memory capacity for knowledge and its ability to comprehend and utilize that knowledge. It categorizes knowledge into \"Known Knows,\" \"Unknown Knows,\" \"Known Unknows,\" and \"Unknown Unknows,\" highlighting the model's proficiency in understanding and applying existing knowledge as well as its self-knowledge regarding limitations. This framework emphasizes the importance of recognizing what a model does not know for its practical applicability.\n",
      "QUERY:  What techniques can be utilized to improve the ratio of 'Known Knows' to 'Unknown Knows'?\n",
      "ANSWER Techniques such as Chain-of-Thought, Self-Consistency, and Complex CoT can be utilized to improve the ratio of 'Known Knows' to 'Unknown Knows' in large language models. These methods enhance the models' ability to understand and apply existing knowledge effectively. By focusing on these techniques, the self-knowledge of the models can be significantly increased.\n",
      "QUERY:  What is the significance of 'knowing what you don’t know' in LLMs?\n",
      "ANSWER 'Knowing what you don’t know' is significant in LLMs as it reflects their self-knowledge, which is crucial for recognizing their limitations and conveying uncertainty when faced with unanswerable questions. This ability enhances the responsible usage of LLMs in various applications by ensuring they do not provide misleading or incorrect information. Moreover, improving self-knowledge can lead to more accurate and reliable responses, bridging the gap between LLMs and human proficiency in understanding knowledge boundaries.\n",
      "QUERY:  How do existing datasets like SQuAD2.0 and NewsQA test the self-knowledge of models?\n",
      "ANSWER Existing datasets like SQuAD2.0 and NewsQA test the self-knowledge of models by including unanswerable questions that require the models to recognize their limitations. These questions are context-specific and can become answerable with additional information, challenging the models to delineate their knowledge boundaries. The performance of models on these datasets is evaluated to see how well they can identify when they do not have sufficient information to provide an answer.\n",
      "QUERY:  What were the findings of Srivastava et al. (2022) regarding LLMs' performance on unanswerable questions?\n",
      "ANSWER Srivastava et al. (2022) found that the performance of large language models (LLMs) on delineating their knowledge boundaries with unanswerable questions barely surpassed random guessing. They evaluated LLMs using a set of 23 pairs of answerable and unanswerable multiple-choice questions. This indicated a significant limitation in the models' self-knowledge regarding unanswerable questions.\n",
      "QUERY:  What is the proposed method to probe the self-knowledge of LLMs through a distinct 'Value Head'?\n",
      "ANSWER The proposed method to probe the self-knowledge of LLMs through a distinct \"Value Head\" involves implementing a specialized component within the model that assesses its understanding of its own knowledge limitations. However, this approach may face challenges when applied across different domains or tasks due to the task-specific training of the models. The study ultimately shifts focus to the inherent abilities of LLMs rather than solely relying on this method.\n",
      "QUERY:  What is the pivotal question posed in the study regarding LLMs' self-knowledge?\n",
      "ANSWER The pivotal question posed in the study regarding LLMs' self-knowledge is: \"Do large language models know what they don’t know?\"\n",
      "QUERY:  How was the self-knowledge of LLMs quantified in this study?\n",
      "ANSWER The self-knowledge of LLMs was quantified using the F1 score, which focused on identifying unanswerable questions as positive cases and answerable questions as negative cases. Additionally, a novel evaluation technique based on text similarity was employed to assess the degree of uncertainty in the model's responses. This approach allowed for a systematic measurement of the models' ability to recognize their limitations.\n",
      "QUERY:  What is the composition of the new dataset called SelfAware?\n",
      "ANSWER The SelfAware dataset comprises a total of 3,369 questions, including 1,032 unanswerable questions and 2,337 answerable questions. The unanswerable questions are categorized into five distinct types, while the answerable questions are drawn from three existing datasets: SQuAD, HotpotQA, and TriviaQA. This dataset aims to evaluate the self-knowledge of large language models.\n",
      "QUERY:  What are the key contributions of this study to the field of LLMs?\n",
      "ANSWER The key contributions of this study include the development of a new dataset called SelfAware, which contains a diverse range of unanswerable questions, and the introduction of an innovative evaluation technique based on text similarity to quantify uncertainty in model outputs. Additionally, the study provides a detailed analysis of 20 LLMs, benchmarking their self-knowledge against human performance, highlighting significant disparities. These efforts aim to enhance the understanding of LLMs' self-knowledge and improve their reliability in various applications.\n",
      "QUERY:  How was the dataset SelfAware constructed and what types of questions does it include?\n",
      "ANSWER The SelfAware dataset was constructed by collecting 2,858 unanswerable questions from online platforms, which were then evaluated by three independent analysts to retain only those unanimously deemed unanswerable, resulting in 1,032 such questions. Additionally, 2,337 answerable questions were sourced from datasets like SQuAD, HotpotQA, and TriviaQA, selected for their semantic closeness to the unanswerable ones. The dataset includes five categories of unanswerable questions: \"No Scientific Consensus,\" \"Imagination,\" \"Completely Subjective,\" \"Too Many Variables,\" and \"Philosophical.\"\n",
      "QUERY:  What categories were identified in the manual analysis of unanswerable questions?\n",
      "ANSWER The manual analysis identified five categories of unanswerable questions: No Scientific Consensus, Imagination, Completely Subjective, Too Many Variables, and Philosophical. Each category reflects different reasons why the questions cannot be definitively answered. These categories encompass ongoing debates, speculative scenarios, personal preferences, complex variables, and profound philosophical inquiries.\n",
      "QUERY:  What is the evaluation method used to assess self-knowledge in the generated text?\n",
      "ANSWER The evaluation method involves defining a similarity function to compute the similarity between a given sentence and a collection of reference sentences with uncertain meanings. If the similarity exceeds a predetermined threshold, the text is considered to contain uncertain meanings. The F1 score is then used to measure the model's self-knowledge by identifying unanswerable questions as positive cases and answerable questions as negative cases.\n"
     ]
    }
   ],
   "source": [
    "# the questions variable was created within the string inside the `questions_list` variable.\n",
    "answers = []\n",
    "for q in questions:\n",
    "    answers.append(ask_pdf(rag_chain,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  Is this: Large language models (LLMs) exhibit an intrinsic capacity for self-knowledge, which allows them to identify unanswerable or unknowable questions. This ability is assessed through their responses, reflecting uncertainty when faced with such questions. However, there remains a significant gap between the self-knowledge of LLMs and that of humans. the correct answer to this question: What is the intrinsic capacity for self-knowledge within large language models (LLMs)?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: In-context learning and instruction tuning can enhance self-knowledge in large language models (LLMs) by providing richer contextual information and structured guidance, which helps the models better understand their limitations. These techniques improve the models' ability to recognize unanswerable questions and convey uncertainty in their responses. Experimental results indicate that models like InstructGPT show significant improvements in self-knowledge when these methods are applied. the correct answer to this question: How can in-context learning and instruction tuning enhance self-knowledge in LLMs?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: The gap between the capabilities of large language models (LLMs) and human proficiency in recognizing their knowledge limits is significant, with LLMs like GPT-4 achieving a self-knowledge score of 75.47%, while humans score 84.93%. This indicates that LLMs still have room for improvement in understanding their own limitations. Further research is needed to enhance LLMs' self-knowledge capabilities. the correct answer to this question: What is the gap between the capabilities of LLMs and human proficiency in recognizing their knowledge limits?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: The Know-Unknow quadrant illustrates the relationship between a model's memory capacity for knowledge and its ability to comprehend and utilize that knowledge. It categorizes knowledge into \"Known Knows,\" \"Unknown Knows,\" \"Known Unknows,\" and \"Unknown Unknows,\" highlighting the model's proficiency in understanding and applying existing knowledge as well as its self-knowledge regarding limitations. This framework emphasizes the importance of recognizing what a model does not know for its practical applicability. the correct answer to this question: What does the Know-Unknow quadrant illustrate about a model's knowledge and comprehension?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: Techniques such as Chain-of-Thought, Self-Consistency, and Complex CoT can be utilized to improve the ratio of 'Known Knows' to 'Unknown Knows' in large language models. These methods enhance the models' ability to understand and apply existing knowledge effectively. By focusing on these techniques, the self-knowledge of the models can be significantly increased. the correct answer to this question: What techniques can be utilized to improve the ratio of 'Known Knows' to 'Unknown Knows'?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: 'Knowing what you don’t know' is significant in LLMs as it reflects their self-knowledge, which is crucial for recognizing their limitations and conveying uncertainty when faced with unanswerable questions. This ability enhances the responsible usage of LLMs in various applications by ensuring they do not provide misleading or incorrect information. Moreover, improving self-knowledge can lead to more accurate and reliable responses, bridging the gap between LLMs and human proficiency in understanding knowledge boundaries. the correct answer to this question: What is the significance of 'knowing what you don’t know' in LLMs?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER '''YES'''\n",
      "QUERY:  Is this: Existing datasets like SQuAD2.0 and NewsQA test the self-knowledge of models by including unanswerable questions that require the models to recognize their limitations. These questions are context-specific and can become answerable with additional information, challenging the models to delineate their knowledge boundaries. The performance of models on these datasets is evaluated to see how well they can identify when they do not have sufficient information to provide an answer. the correct answer to this question: How do existing datasets like SQuAD2.0 and NewsQA test the self-knowledge of models?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: Srivastava et al. (2022) found that the performance of large language models (LLMs) on delineating their knowledge boundaries with unanswerable questions barely surpassed random guessing. They evaluated LLMs using a set of 23 pairs of answerable and unanswerable multiple-choice questions. This indicated a significant limitation in the models' self-knowledge regarding unanswerable questions. the correct answer to this question: What were the findings of Srivastava et al. (2022) regarding LLMs' performance on unanswerable questions?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: The proposed method to probe the self-knowledge of LLMs through a distinct \"Value Head\" involves implementing a specialized component within the model that assesses its understanding of its own knowledge limitations. However, this approach may face challenges when applied across different domains or tasks due to the task-specific training of the models. The study ultimately shifts focus to the inherent abilities of LLMs rather than solely relying on this method. the correct answer to this question: What is the proposed method to probe the self-knowledge of LLMs through a distinct 'Value Head'?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: The pivotal question posed in the study regarding LLMs' self-knowledge is: \"Do large language models know what they don’t know?\" the correct answer to this question: What is the pivotal question posed in the study regarding LLMs' self-knowledge?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: The self-knowledge of LLMs was quantified using the F1 score, which focused on identifying unanswerable questions as positive cases and answerable questions as negative cases. Additionally, a novel evaluation technique based on text similarity was employed to assess the degree of uncertainty in the model's responses. This approach allowed for a systematic measurement of the models' ability to recognize their limitations. the correct answer to this question: How was the self-knowledge of LLMs quantified in this study?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: The SelfAware dataset comprises a total of 3,369 questions, including 1,032 unanswerable questions and 2,337 answerable questions. The unanswerable questions are categorized into five distinct types, while the answerable questions are drawn from three existing datasets: SQuAD, HotpotQA, and TriviaQA. This dataset aims to evaluate the self-knowledge of large language models. the correct answer to this question: What is the composition of the new dataset called SelfAware?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: The key contributions of this study include the development of a new dataset called SelfAware, which contains a diverse range of unanswerable questions, and the introduction of an innovative evaluation technique based on text similarity to quantify uncertainty in model outputs. Additionally, the study provides a detailed analysis of 20 LLMs, benchmarking their self-knowledge against human performance, highlighting significant disparities. These efforts aim to enhance the understanding of LLMs' self-knowledge and improve their reliability in various applications. the correct answer to this question: What are the key contributions of this study to the field of LLMs?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: The SelfAware dataset was constructed by collecting 2,858 unanswerable questions from online platforms, which were then evaluated by three independent analysts to retain only those unanimously deemed unanswerable, resulting in 1,032 such questions. Additionally, 2,337 answerable questions were sourced from datasets like SQuAD, HotpotQA, and TriviaQA, selected for their semantic closeness to the unanswerable ones. The dataset includes five categories of unanswerable questions: \"No Scientific Consensus,\" \"Imagination,\" \"Completely Subjective,\" \"Too Many Variables,\" and \"Philosophical.\" the correct answer to this question: How was the dataset SelfAware constructed and what types of questions does it include?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: The manual analysis identified five categories of unanswerable questions: No Scientific Consensus, Imagination, Completely Subjective, Too Many Variables, and Philosophical. Each category reflects different reasons why the questions cannot be definitively answered. These categories encompass ongoing debates, speculative scenarios, personal preferences, complex variables, and profound philosophical inquiries. the correct answer to this question: What categories were identified in the manual analysis of unanswerable questions?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: The evaluation method involves defining a similarity function to compute the similarity between a given sentence and a collection of reference sentences with uncertain meanings. If the similarity exceeds a predetermined threshold, the text is considered to contain uncertain meanings. The F1 score is then used to measure the model's self-knowledge by identifying unanswerable questions as positive cases and answerable questions as negative cases. the correct answer to this question: What is the evaluation method used to assess self-knowledge in the generated text?         according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " \"'''YES'''\",\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for q,a in zip(questions, answers):\n",
    "    # Check for results\n",
    "    evaluations.append(ask_pdf(rag_chain,f\"Is this: {a} the correct answer to this question: {q} \\\n",
    "        according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\"))\n",
    "\n",
    "evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.75%\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "yes_count = evaluations.count('YES')\n",
    "score = str(yes_count/len(evaluations) * 100) + \"%\"\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more 'langchain way' to do this would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt_eval_template = \"\"\"\n",
    "You take in context, a question and a generated answer and you output ONLY a score of YES if the answer is correct,\n",
    "or NO if the answer is not correct.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "<context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "<question>\n",
    "\n",
    "<answer>\n",
    "{answer}\n",
    "<answer>\n",
    "\"\"\"\n",
    "\n",
    "prompt_eval = ChatPromptTemplate.from_template(prompt_eval_template)\n",
    "\n",
    "answer_eval_chain = (\n",
    "    {\n",
    "        'context': lambda x: format_docs(x['context']),\n",
    "        'question': lambda x: x['question'],\n",
    "        'answer': lambda x: x['answer']\n",
    "        }\n",
    "    ) | prompt_eval | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = []\n",
    "for q,a in zip(questions, answers):\n",
    "    evaluations.append(answer_eval_chain.invoke({'context': pdf_docs, 'question': q, 'answer': a}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES',\n",
       " 'YES']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "yes_count = evaluations.count('YES')\n",
    "score = str(yes_count/len(evaluations) * 100) + \"%\"\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example notebook we introduced a few interesting ideas:\n",
    "1. Structured outputs\n",
    "2. Some simple evaluation of rag answers using the 'llm-as-a-judge' strategy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-chatgpt-apps",
   "language": "python",
   "name": "oreilly-chatgpt-apps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
