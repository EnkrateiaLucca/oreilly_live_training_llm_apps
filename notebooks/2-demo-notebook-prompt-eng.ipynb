{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Presentation:** \n",
    "\n",
    "# Introduction to Prompt Engineering and the ChatGPT API\n",
    "\n",
    "- Introduction to the ChatGPT API\n",
    "- Prompt basics \n",
    "- Prompt engineering guide\n",
    "- Demos & Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ChatGPT API\n",
    "\n",
    "- Where does ChatGPT fit into this chaotic universe?\n",
    "- The ChatGPT API (what’s the deal?)\n",
    "- How to use it, basics, parameters, simple examples, etc…."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request body for the CHATGPT API involves many parameters, but let's focus on the following:\n",
    "\n",
    "- model: ID of the model to use.\n",
    "- messages: a list of messages comprising the conversation up to that point\n",
    "- temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
    "- n: number of chat completion choices to generate for each input message\n",
    "- max_tokens: the maximum number of tokens to generate in the chat completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8hdk2baVwaPNpOIhAsED7aZC2EqEH', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The meaning of life is a profound and age-old question that has been pondered by philosophers, theologians, and seekers of wisdom throughout history. While different people and cultures may have their own interpretations, the answer to this question ultimately depends on one's personal beliefs and perspectives.\\n\\nFrom a philosophical standpoint, the meaning of life can be seen as subjective and created by individuals themselves. It is about finding purpose, fulfillment, and happiness in one's own existence. This can involve pursuing personal goals, meaningful relationships\", role='assistant', function_call=None, tool_calls=None))], created=1705411562, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=100, prompt_tokens=34, total_tokens=134))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The meaning of life is a profound and age-old question that has been pondered by philosophers, theologians, and seekers of wisdom throughout history. While different people and cultures may have their own interpretations, the answer to this question ultimately depends on one's personal beliefs and perspectives.\\n\\nFrom a philosophical standpoint, the meaning of life can be seen as subjective and created by individuals themselves. It is about finding purpose, fulfillment, and happiness in one's own existence. This can involve pursuing personal goals, meaningful relationships\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response ID: chatcmpl-8hdk2baVwaPNpOIhAsED7aZC2EqEH\n",
      "Choices: [Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The meaning of life is a profound and age-old question that has been pondered by philosophers, theologians, and seekers of wisdom throughout history. While different people and cultures may have their own interpretations, the answer to this question ultimately depends on one's personal beliefs and perspectives.\\n\\nFrom a philosophical standpoint, the meaning of life can be seen as subjective and created by individuals themselves. It is about finding purpose, fulfillment, and happiness in one's own existence. This can involve pursuing personal goals, meaningful relationships\", role='assistant', function_call=None, tool_calls=None))]\n",
      "Created: 1705411562\n",
      "Model: gpt-3.5-turbo-0613\n",
      "System Fingerprint: None\n",
      "Object Type: chat.completion\n",
      "Usage: CompletionUsage(completion_tokens=100, prompt_tokens=34, total_tokens=134)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://platform.openai.com/docs/api-reference/chat/object\n",
    "\n",
    "\n",
    "### The chat completion object\n",
    "\n",
    "Represents a chat completion response returned by model, based on the provided input.\n",
    "\n",
    "__id__\n",
    "- string\n",
    "- A unique identifier for the chat completion.\n",
    "\n",
    "__choices__\n",
    "- array\n",
    "- A list of chat completion choices. Can be more than one if n is greater than 1.\n",
    "\n",
    "__created__\n",
    "- integer\n",
    "- The Unix timestamp (in seconds) of when the chat completion was created.\n",
    "\n",
    "__model__\n",
    "- string\n",
    "- The model used for the chat completion.\n",
    "\n",
    "__system_fingerprint__\n",
    "- string\n",
    "- This fingerprint represents the backend configuration that the model runs with.\n",
    "\n",
    "- Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.\n",
    "\n",
    "__object__\n",
    "- string\n",
    "- The object type, which is always chat.completion.\n",
    "\n",
    "__usage__\n",
    "- object\n",
    "- Usage statistics for the completion request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panda eats bamboo\n",
      "Black and white bear in the snow\n",
      "Chill and eat all day\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Guide\n",
    "\n",
    "What is prompt engineering?\n",
    "\n",
    "Prompt engineering is a reference to a discipline concerned with stablishing the rules for obtaining the most deterministic outputs possible from a LLM by employing engineering techniques and protocols to enture reproducibility and consistency.\n",
    "\n",
    "***In a simplified way, prompt engineering is the means by which LLMs can be programmed through prompting.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic goal of prompt engineering is designing appropriate inputs for prompting methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting Basics\n",
    "\n",
    "A prompt is a piece of text that conveys to a LLM what the user wants. What the user wants can be many things like:\n",
    "\n",
    "- Asking a question\n",
    "- Giving an instruction\n",
    "- Etc...\n",
    "\n",
    "The key components of a prompt are:\n",
    "1. Instruction: where you describe what you want\n",
    "2. Context: additional information to help with performance\n",
    "3. Input data: data the model has not seem to illustrate what you need\n",
    "4. Output indicator: How you prime the model to output what you want, for example asking the model [\"Let's think step by step\" and the end of a prompt can boost reasoning performance](https://arxiv.org/pdf/2201.11903.pdf). You can also write \"Output:\" to prime the model to just write the output and nothing else.\n",
    "\n",
    "[Prompts can also be seen as a form of programming that can customize the outputs and interactions with an LLM.](https://ar5iv.labs.arxiv.org/html/2302.11382#:~:text=prompts%20are%20also%20a%20form%20of%20programming%20that%20can%20customize%20the%20outputs%20and%20interactions%20with%20an%20llm.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Data & Context Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Prompt Engineering Strategies\n",
    "\n",
    "- Strategy 1: Write clear instructions\n",
    "- Strategy 2: Provide reference text\n",
    "- Strategy 3: Break tasks into subtasks\n",
    "- Strategy 4: Give the model time to think\n",
    "- Strategy 5: Use external tools\n",
    "- Strategy 6: Test changes systematically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question from AJ!\n",
    "\n",
    "difference between system message and including the message in the user's prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OREILLY live-training course output: The capital of France is Paris.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response_with_system_message(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"Everytime you answer a question you start by saying: OREILLY live-training course output:\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "get_response_with_system_message(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OREILLY live-training course output: The capital of Canada is Ottawa.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response_with_system_message(\"What is the capital of Canada?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system message is applied in the overall behavior of the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OREILLY live-training course output. The capital of France is Paris.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helfpul assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "get_response(prompt_question=\"Everytime you answer a question you start by saying: OREILLY live-training course output. What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy 1 is\n",
    "\n",
    "Write Clear instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_example = \"What is the meaning of life?\"\n",
    "clear_instructions_example = \"Write an essay about Aristotle's views on ethics regarding eudaimonia. Make it about what makes a good life.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Notice of Holiday Absence\n",
      "\n",
      "Dear [Peer's Name],\n",
      "\n",
      "I hope this email finds you well. I wanted to inform you in advance that I will be away on holiday for a period of [number of days/weeks/months]. My vacation is scheduled to start from [start date] and end on [end date].\n",
      "\n",
      "During this time, I will not be available to handle any work-related tasks or respond to emails promptly. Therefore, I kindly request you to refrain from assigning any urgent or time-sensitive projects to me during this period. If there are any impending deadlines or tasks that require attention, please inform me as soon as possible so that I can plan accordingly to complete them before my departure.\n",
      "\n",
      "In my absence, I would highly appreciate if you could kindly consider taking care of any urgent matters that may arise and keeping me informed via email. If there are any important meetings, discussions, or decisions that need to be made during my time away, please feel free to reach out to me beforehand so that we can discuss how to handle them appropriately.\n",
      "\n",
      "Please note that I have informed our supervisor about my holiday plans and have made all necessary arrangements to ensure a smooth workflow during my absence. I have also updated my out of office notification to provide alternative contact details for urgent matters. You can reach out to [colleague's name and email] in case of any immediate assistance or situations that require attention.\n",
      "\n",
      "I apologize for any inconvenience this may cause, and I assure you that I will make every effort to ensure a seamless transition before my departure and upon my return. I will prioritize catching up on any missed work as soon as I am back. If there are any changes or extensions to my holiday plans, I will inform you promptly.\n",
      "\n",
      "Thank you for your understanding and cooperation. I appreciate your support in managing my workload while I am away. If there are any questions or concerns, please do not hesitate to reach out to me before my departure, or leave a message that I can address upon my return.\n",
      "\n",
      "Wishing you a productive and successful period in my absence.\n",
      "\n",
      "Kind regards,\n",
      "[Your Name]\n",
      "Subject: Request for Leave Approval\n",
      "\n",
      "Dear [Boss's Name],\n",
      "\n",
      "I hope this email finds you well. I am writing to request your approval for a period of leave from [start date] to [end date].\n",
      "\n",
      "The reason for the leave is [briefly explain the reason for the leave - personal, medical, etc.]. During this time, I will be [mention any plans or arrangements you have made].\n",
      "\n",
      "I understand the impact my absence may have on the team and our work, and I would like to assure you that I have taken the necessary steps to minimize any disruption. I have discussed my workload and pending projects with [colleague's name] and we have drafted a plan to ensure the smooth continuation of tasks in my absence. [Mention any arrangements you have made, such as delegating tasks or providing detailed instructions to colleagues].\n",
      "\n",
      "I am confident that the team, under your leadership and with the support of my colleagues, will be able to manage without me during this period. I will also be available through email or phone, should any urgent matters arise that require my attention.\n",
      "\n",
      "I have attached a copy of the leave request form for your convenience. Kindly review and provide your approval at your earliest convenience. If you require any additional information or need further clarification, please do not hesitate to reach out to me.\n",
      "\n",
      "Thank you for considering my request. I appreciate your understanding and support. I am confident that my time away will not result in any significant disruption to our work and I will ensure a smooth transition before I leave.\n",
      "\n",
      "Sincerely,\n",
      "[Your Name]\n",
      "[Your Designation]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "good_example_from_EP = \"Can you write a mail to my peer exaplinging that i will be out on holiday.\"\n",
    "bad_example_from_EP = \"write an mail to boss\"\n",
    "\n",
    "print(get_response(good_example_from_EP))\n",
    "\n",
    "print(get_response(bad_example_from_EP))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_example_KA = \"What was the version of Ubuntu that qualified as a LTS release in August 2020?\"\n",
    "system_message = \"Answer as an expert linux system engineer.\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of August 2020, the Ubuntu LTS (Long-Term Support) release version was Ubuntu 20.04, also known as \"Focal Fossa\". Ubuntu releases with LTS designation are supported for a longer duration of 5 years instead of the usual 9 months for non-LTS releases.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_response(prompt_question, system_message):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=[{\"role\": \"system\", \"content\":system_message },\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "get_response(prompt_question=good_example_KA, system_message=system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Audi offers a range of models with varying power outputs. The horsepower (PS) of an Audi can range from around 110 PS for smaller models like the Audi A1 to over 600 PS for high-performance models like the Audi R8. The exact horsepower of an Audi will depend on the specific model and variant.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_example_from_JH = \"how much ps does an audi has\" \n",
    "# good: how much ps does an audi rs3 model year 2024 has\n",
    "\n",
    "get_response(prompt_question=bad_example_from_JH, system_message=\"You are a helpful assistant about car information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Audi RS3 model year 2022 has a powerful 2.5-liter inline-five engine that generates 395 horsepower (400 metric horsepower or PS).'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_example_JH = \"how much ps does an audi rs3 model year 2022 has\"\n",
    "\n",
    "get_response(prompt_question=good_example_JH, system_message=\"You are a helpful assistant about car information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't have access to real-time information about your location. Could you please provide me with your location so that I can give you an accurate answer?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_example_AF = \"Is it cold?\"\n",
    "good_example_AF = \"What is the temperature in New York\"\n",
    "\n",
    "get_response(prompt_question=bad_example_AF, system_message=\"You are a helpful assistant about weather information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I am not able to provide real-time information. To get the current temperature in New York, I would suggest checking a weather website or using a weather app on your phone.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(prompt_question=good_example_AF, system_message=\"You are a helpful assistant about weather information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy 2: Providing reference text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lucas is an O'Reilly instructor.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(prompt_question=\"'''Information: Lucas is an Oreilly instructor; Lucas likes taking walks on the beach,'''\\\n",
    "    What does Lucas does?\", system_message=\"Your answers should only use the information inside triple quotes ''''''\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Can you explain a little more about strategy 6? How do you evaluate the responses based on a given prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 4: Give the model time to think\n",
    "system_message = \"\"\n",
    "time_to_think_prompt = \"\"\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": time_to_think_prompt}\n",
    "])\n",
    "print(\"\\nStrategy 4 - Give the Model Time to Think:\\n\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 5: Use external tools (e.g., Python code execution)\n",
    "system_message = \"\"\n",
    "external_tools_prompt = \"\"\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": external_tools_prompt}\n",
    "])\n",
    "print(\"\\nStrategy 5 - Use External Tools:\\n\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 6: Test changes systematically\n",
    "system_message = \"\"\n",
    "test_changes_systematically_prompt = \"\"\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": test_changes_systematically_prompt}\n",
    "])\n",
    "print(\"\\nStrategy 6 - Test Changes Systematically:\\n\", response.choices[0].essage.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Template for Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stablish a concrete and atomic task\n",
    "- Define a set of prompt candidates\n",
    "- Define a clear metric for evaluation\n",
    "- Test\n",
    "- Evaluate\n",
    "- Compare\n",
    "- Find the best prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Practical Case Study\n",
    "\n",
    "Now, let's take the concepts and ideas discussed in this lesson, and apply them to an actual problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple example, imagine you want to extract dates from text. You might set up a LLM to do that by first creating a set of examples of phrases with dates, something we can start with ChatGPT itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sure, here\\'s a paragraph that contains a complete date (day, month, and year) formatted in different ways:\\n\\n\"On the 27th of July 2022, we embarked on a journey to explore the historical city of Rome. Little did we know that this 27-Jul-2022 adventure would leave a lasting impression on us. As we wandered through the cobblestone streets, we couldn\\'t help but admire the magnificent architecture that has stood the test of time since the 27th July 2022. It was a truly unforgettable experience.\"',\n",
       " 'On October 21, 2022, John and Sarah embarked on a thrilling adventure to explore the vibrant streets of Paris. Little did they know that this date would mark the beginning of a lifelong journey together, filled with love, laughter, and unforgettable memories.',\n",
       " 'On July 12th, 2022, Sara embarked on her dream journey to explore the stunning landscapes of New Zealand.',\n",
       " 'On November 15th, 2021, I went to the park with my friends and enjoyed a beautiful autumn afternoon.',\n",
       " 'On the 10th of October 2022, we will be hosting our annual gala event at the luxurious Grand Ballroom.',\n",
       " 'The grand opening of the new shopping center is scheduled to take place on October 15th, 2022, and it will be a momentous occasion for the community.',\n",
       " 'On May 22nd, 2022, thousands gathered at the city park to celebrate the annual summer festival.',\n",
       " 'On the 10th of October 2021, we gathered together to celebrate the start of a new era.',\n",
       " 'On December 7th, 2022, the team gathered for a meeting to discuss the upcoming project deadlines, and it was decided that the final submission must be completed by the 15th of May, 2023.',\n",
       " 'Sure, here\\'s a phrase containing a complete date formatted in different ways:\\n\\n\"On January 15th, 2022, we gathered to celebrate the launch of our new product. The event was a huge success, with attendees from all over the world. We look forward to the opportunities that lie ahead in the year 2022!\"']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(model=\"gpt-3.5-turbo-1106\", \n",
    "                             messages=\n",
    "                             [\n",
    "                                 {\"role\": \"system\", \"content\": \"You are a savy guru with knowledge about existence and the secrets of life.\"},\n",
    "                                 {\"role\": \"user\", \"content\": prompt}   \n",
    "                             ],\n",
    "                             max_tokens=100,\n",
    "                             temperature=0.9,\n",
    "                             n = 1)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "num_samples = 10\n",
    "phrases_with_dates = []\n",
    "prompt = \"Create a 1 paragraph phrase containing a complete date (day month  and year) anywhere in the text formatted in different ways.\"\n",
    "for i in range(num_samples):\n",
    "    phrases_with_dates.append(get_response(prompt))\n",
    "phrases_with_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok perfect! Now that we have this evaluation set, we can set up a simple experiment by first creating a demonstration set with our prompt candidates.\n",
    "\n",
    "We'll begin with a baseline using only zero-shot prompt examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
