{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-07-24-10-52-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple QA System for Chatting with a PDF\n",
    "\n",
    "This part of the training will be mostly hands on with the code for building the qa PDF system with langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ChatVectorDBChain\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500, chunk_overlap = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = './persist_directory/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../resources_and_assets/llm_paper_know_dont_know.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(pdf_path)\n",
    "pdf_doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = text_splitter.split_documents(pdf_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(all_splits, embedding=embedding,\\\n",
    "        persist_directory=persist_directory)\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucassoares/anaconda3/envs/oreilly_env/lib/python3.11/site-packages/langchain/chains/conversational_retrieval/base.py:379: UserWarning: `ChatVectorDBChain` is deprecated - please use `from langchain.chains import ConversationalRetrievalChain`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pdf_qa = ChatVectorDBChain.from_llm(ChatOpenAI(temperature=0,\\\n",
    "    model_name=\"gpt-3.5-turbo\"), vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the main contributions of this paper with regards to self-knowledge in LLMs?',\n",
       " 'chat_history': [],\n",
       " 'answer': 'The main contributions of this paper with regards to self-knowledge in LLMs are the findings that these models do possess a certain degree of self-knowledge, but there is a disparity when compared to human self-knowledge. The paper also highlights the need for further research to enhance the ability of LLMs to understand their own limitations. This could lead to more accurate and reliable responses from LLMs, positively impacting their applications in diverse fields.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the main contributions of this paper with regards to self-knowledge in LLMs?\"\n",
    "pdf_qa({\"question\": query, \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the Q&A app for pdfs in `qa_pdf.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the app with:\n",
    "\n",
    "`streamlit run qa_pdf.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Long context issue in LLMs](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "\n",
    "Long Context. One of the main drawbacks of Transformerbased language models is the context length is limited due to the involved quadratic computational costs in both time\n",
    "and memory. Meanwhile, there is an increasing demand\n",
    "for LLM applications with long context windows, such as\n",
    "in PDF processing and story writing [217]. ChatGPT has\n",
    "recently released an updated variant with a context window\n",
    "size of up to 16K tokens, which is much longer than the\n",
    "initial one, i.e., 4K tokens. Additionally, GPT-4 was launched\n",
    "with variants with context window of 32K tokens [46]. Next,\n",
    "we discuss two important factors that support long context\n",
    "modeling for LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb \n",
    "Below are notebook from openai cookbook on these topics of search and embeddings:\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Code_search.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb\n",
    "- https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "- [In-context learning abilities of ChatGPT models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "- [Issue with long context](https://arxiv.org/pdf/2303.18223.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly_env",
   "language": "python",
   "name": "oreilly_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
