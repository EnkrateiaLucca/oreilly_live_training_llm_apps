{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tunning ChatGPT to Make Precise Youtube Chapters Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the documentation on the [OPENAI website](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset) to learn how to fine tune a chatgpt model to a specific use case using a custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the basics, we need an example format, which should be as such:\n",
    "\n",
    "```\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to fine tune a ChatGPT model we'll need to create a dataset organized in the format shown above. Essentially we'll have a .json file with several (as many as the number of inputs for the fine tunning) dictionaries that have this format of:\n",
    "\n",
    "```\n",
    "{\"<messages key>\"}: [{\"role\": \"system\", \"<content key>\": \"<some text content>\"},\n",
    "{\"<messages key>\"}: {\"role\": \"user\", \"<content key>\": \"<some text content>\"},\n",
    "{\"<messages key>\"}: {\"role\": \"assistant\", \"<content key>\": \"<some text content>\"}]\n",
    "```\n",
    "\n",
    "Here we'll assume that we will only fine tune actual ChatGPT, so we won't cover the formats for models like 'babbage-002' and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, given this format, let's create our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quote from the actual documentation:\n",
    "\n",
    "\n",
    "> To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples with gpt-3.5-turbo but the right number varies greatly based on the exact use case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get at least 30 examples for our custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in regular fine tunning with any Machine Learning model, its advisable to get stablish a train-test split so that you are on top of your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, its extremely important to be aware of token limits and token costs, given that this will be a paid API.\n",
    "\n",
    "Each training example will be limited to the context length of ChatGPT (therefore 4096 tokens), so its good to set some good practices in place to avoid issues with large inputs, the [OpenAI documentation](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset#:~:text=Each%20training%20example,the%20OpenAI%20cookbook.) recommends you check that the total token count in the message contents are under 4000 as a good rule of thumb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOr the example I want to try out, I have to first father all the relevant urls from which to create a relevant dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.youtube.com/watch?v=j9CHA3hgA10&t=615s\n",
    "- https://www.youtube.com/watch?v=W8QwNXbO5dk\n",
    "- https://www.youtube.com/watch?v=xKKnWcJo0vE\n",
    "- https://www.youtube.com/watch?v=MXBxNUnC7iY\n",
    "- https://www.youtube.com/watch?v=2r37fvpGnMo\n",
    "- https://www.youtube.com/watch?v=Q2U-tt1JffY\n",
    "- https://www.youtube.com/watch?v=5VAuGvefzEI\n",
    "- https://www.youtube.com/watch?v=p_MQRWH5Y6k\n",
    "- https://www.youtube.com/watch?v=H3s5fx7CsZg\n",
    "- https://www.youtube.com/watch?v=s-VlDoL1F6M\n",
    "- https://www.youtube.com/watch?v=srLYR_PMX1o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's create this dataset by creating input prompts that ask for the creation of the Youtube Chapters section we desire, followed by a formatting description and the actual transcription of the original video.\n",
    "\n",
    "Since I already have the chapters sections for those videos, I can use those as the standard desired generated output from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://www.youtube.com/watch?v=srLYR_PMX1o\",\n",
    "\"https://www.youtube.com/watch?v=s-VlDoL1F6M\",\n",
    "\"https://www.youtube.com/watch?v=H3s5fx7CsZg\",\n",
    "\"https://www.youtube.com/watch?v=p_MQRWH5Y6k\",\n",
    "\"https://www.youtube.com/watch?v=5VAuGvefzEI\",\n",
    "\"https://www.youtube.com/watch?v=Q2U-tt1JffY\",\n",
    "\"https://www.youtube.com/watch?v=MXBxNUnC7iY\",\n",
    "\"https://www.youtube.com/watch?v=2r37fvpGnMo\",\n",
    "\"https://www.youtube.com/watch?v=xKKnWcJo0vE\",\n",
    "\"https://www.youtube.com/watch?v=j9CHA3hgA10&t=615s\",\n",
    "\"https://www.youtube.com/watch?v=W8QwNXbO5dk\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of these URLS what we want is to gather their corresponding video transcriptions and the chapters sections to serve as input and output respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOr that, let's make our lives easier by creating a few helper functions that can automate part of this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def get_num_tokens(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Calculates the number of tokens in a text prompt\"\"\"    \n",
    "\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    return len(enc.encode(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def get_response(prompt_question,):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and\\\n",
    "            programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- The video is about setting up an OCR (Optical Character Recognition) search project to detect text within images in your local folders using Python.\\n- The presenter begins with initial project setup requirements and package installations such as EasyOCR and Torch for Python version 3.10.\\n- Necessary libraries such as OS and typing are imported.\\n- The presenter then sets up two primary functions: one to scan images for text and another to loop over images within a folder.\\n- They also set up the EasyOCR reader for text detection, specifying language as English. It is set to use a GPU if available, else falls back to CPU.\\n- The first function, OCR scan, is programmed to run an OCR on an image and detect text, while the second function is setup to loop over images in a specified folder.\\n- The presenter initially tests the OCR scan function on a sample image text, effectively demonstrating its function.\\n- The recognition of detected text is processed and returned by the OCR scan function.\\n- They then set up the function \"Search Images\", which takes a directory and a keyword, searches for the keyword within the text of each image in the directory, and returns a list of image paths containing the keyword.\\n- After successfully testing this function, the presenter transitions the code to make it callable from a terminal, making the whole process a CLI tool.\\n- This involves importing argparse for handling command line arguments and setting up the main function to handle both directory search for a specific keyword and single image keyword recognition.\\n- The finalized application is then demonstrated and tested in terminal, confirming successful setup.\\n- The final output is a program that performs OCR scan over local images either through directory or a single image, and detects whether a certain keyword exists within these images. The paths of matching images are then returned to the user.\\n- The video concludes with the presenter reviewing the application development process and emphasising the utility of such an OCR based text search tool.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(f\"Summarize this youtube video in bullet points: {transcription_example2_with_timestamps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, great! Even for large videos we are still under the 4000 tokens limit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, at this point I think its note worthy to stop and just think about what are we trying to achieve here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point is good to stop, take a step back and think about what we have to do:\n",
    "\n",
    "1. We need to get the timestamped transcriptions of the text (we'll use the actual Youtube website for that).\n",
    "2. Then, we need to craft a good and all encompassing prompt to attach to each of these transcrtiptions which will surve as the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, I have the transcriptions now in my local folder along with the matching urls and chapters section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./chapters_section_1.txt',\n",
       " './chapters_section_2.txt',\n",
       " './chapters_section_3.txt',\n",
       " './chapters_section_4.txt',\n",
       " './chapters_section_5.txt',\n",
       " './chapters_section_6.txt',\n",
       " './chapters_section_7.txt',\n",
       " './chapters_section_8.txt',\n",
       " './chapters_section_9.txt',\n",
       " './chapters_section_10.txt',\n",
       " './chapters_section_11.txt']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "# Sample data\n",
    "\n",
    "chapters_files = natsorted([file_path for file_path in glob.glob(\"./chapters*.txt\")])\n",
    "transcription_files = natsorted([file_path for file_path in glob.glob(\"./transcription*.txt\")])\n",
    "\n",
    "chapters_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./transcription_1.txt',\n",
       " './transcription_2.txt',\n",
       " './transcription_3.txt',\n",
       " './transcription_4.txt',\n",
       " './transcription_5.txt',\n",
       " './transcription_6.txt',\n",
       " './transcription_7.txt',\n",
       " './transcription_8.txt',\n",
       " './transcription_9.txt',\n",
       " './transcription_10.txt',\n",
       " './transcription_11.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the dictionaries\n",
    "data = []\n",
    "\n",
    "# Loop through the urls, chapters_files, and transcription_files\n",
    "for url, chapter_file, transcription_file in zip(urls, chapters_files, transcription_files):\n",
    "    # Initialize an empty dictionary to store data for each set\n",
    "    entry = {}\n",
    "    \n",
    "    entry[\"url\"] = url\n",
    "    \n",
    "    # Read the chapter file and store its contents\n",
    "    with open(chapter_file, \"r\") as f:\n",
    "        entry[\"chapters_section\"] = f.read()\n",
    "    \n",
    "    # Read the transcription file and store its contents\n",
    "    with open(transcription_file, \"r\") as f:\n",
    "        entry[\"transcription\"] = f.read()\n",
    "        \n",
    "    # Append the dictionary to the list\n",
    "    data.append(entry)\n",
    "\n",
    "# Write the list of dictionaries to a JSON file\n",
    "with open(\"yt_chapters_dataset.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'url': 'https://www.youtube.com/watch?v=srLYR_PMX1o', 'chapters_section': '📚 Chapters:\\n00:00 - Introduction \\n00:01 - Creating Anki Flashcards with ChatGPT\\n00:06 - Importing Dependencies\\n00:14 - Setting Up OpenAI API Key\\n00:29 - Getting Content from Clipboard\\n00:46 - Running the Script\\n00:57 - Creating Conversation with ChatGPT\\n01:05 - Setting Up System Role\\n01:24 - Setting Up User Message\\n01:38 - Formatting for Anki Import\\n01:46 - Sending Request to API\\n02:35 - Writing the flashcards to file\\n02:50 - Creating Anki flashcards\\n03:58 - Importing flashcards to Anki', 'transcription': \"0:00\\nIn this video, you're going to learn how to create Anki flashcards using ChatGPT.\\n0:05\\nOkay, so we're going to start by importing our dependencies. So essentially, I'm going to import\\n0:09\\nOS to access the OpenAI API key in my environment as my environment variable. I'm going to import\\n0:15\\nOpenAI clipboard to get the content from my clipboard. And then Json to save stuff as a Json.\\n0:24\\nHere's what we're going to do. First, we're going to set up an OpenAI API key.\\n0:29\\nThen we're going to get the content from my clipboard. So here I'm getting the content\\n0:35\\nfrom my clipboard. And the idea is I'm going to copy some text from whatever from a paper,\\n0:41\\nfrom a website, etc. And I'm going to run the script. And that's it. I'm going to copy some text,\\n0:47\\nrun the script. And when I run the script, it's going to access the content on my clipboard.\\n0:51\\nAnd from there is going to create the Enki flashcards by sending a request to the ChatGPT\\n0:57\\nAPI. So let's create a conversation with the ChatGPT. I'm going to set up the role for the\\n1:06\\nsystem as a helpful assistant. Before I had set up this as your helpful assistant that creates\\n1:12\\nAnki flashcards. But I found that it had just because of this little change in the system prompt,\\n1:18\\nit had problem creating Enki flashcards in the format that I wanted. And then I'm going to set\\n1:25\\nup the message for the user. And the message for the user is going to be something like create\\n1:29\\nEnki flashcards with the following text using a format like question answer, which is the format\\n1:34\\nthat's more easily importable to Enki. And I'm going to give it the clipboard content. And that's\\n1:42\\nit. That's the message. And now I can just set up the request to the API. So I'm just sending a\\n1:49\\nrequest to the check completion class, access the create method, and then saying, I'm specifying the\\n1:55\\nmodel to GPT for I'm specifying the messages that I just defined in this variable here, that are\\n2:01\\nhave the system message and the user message, the system message describing the behavior that I\\n2:06\\nexpect, and the user message describing what I want specifically, right, that's my actual prompt.\\n2:12\\nAnd then I'm going to set up the temperature to 0.7, which is like traditionally is a good number\\n2:17\\nto set for these kinds of things. I'm going to set up the max tokens. And I'm going to create,\\n2:24\\nI'm going to save the answer from the model to a variable called generate flashcards.\\n2:30\\nAnd from there, I'm going to save them to a file. And that's pretty much it. I actually\\n2:37\\ndidn't use Jason. So let's just remove this. And now let's test this out. So what we're going to do\\n2:43\\nis I'm going to have I have a paper here. And I'm just going to copy the contents from let's say,\\n2:53\\nthe beginning of this paper, let's say this was a paper that I was studying and I already read. And\\n2:58\\nnow I want to kind of like, okay, so I want the some information that's here, I want to save it.\\n3:03\\nBut I don't want to be creating each Enki flashcard, right? So I copied. So now let's check that I\\n3:09\\ncopied. Perfect. It's in my clipboard. So now I can just come here and run the script. So I'm\\n3:15\\ngoing to run flashcards, check it to be perfect. And now let's see the magic happen. So I run the\\n3:23\\nscript, it takes like a few seconds to finish sending the request and getting the response.\\n3:30\\nAll right, perfect. So now I finished running. So I can access that file. And as you can see,\\n3:37\\nit created the flashcards. So what is camel camel stands for communicative agents for mind\\n3:43\\nexploration. What is the goal of camel? What is role playing framework? What is inception prompting?\\n3:50\\nAll useful information that was present in the text. Now I can go to Enki.\\n3:55\\nSo I can go to Enki right here. And I can import those flashcards directly. You see, it created\\n4:03\\nthe flashcards front, back, everything is correct. And I can just click import. And now I have the\\n4:09\\nflashcards to train to access and etc. So yeah, this approach is really cool. I really like this.\\n4:16\\nI like I really like doing this because you know, you can also do it directly with the\\n4:22\\ncharge PT demo or the charge PT website, which also works really well. But having it as a script\\n4:29\\nthat copies, I can just, you know, stop not think about it too much and just have it on my clipboard\\n4:36\\nand then just run the script. And I can automate the running of the script. So I can just do it\\n4:41\\nwith a few commands. And I have a bunch of Enki flashcards that usually would have taken me\\n4:46\\nquite a few minutes to create. So if you like this video, don't forget to like and subscribe\\n4:52\\nand see you next time. Cheers.\"}, {'url': 'https://www.youtube.com/watch?v=s-VlDoL1F6M', 'chapters_section': '📚 Chapters\\n00:01 - Introduction to the PDF Summarization App\\n00:30 - Setting up Imports and Loading Language Model\\n01:03 - Creating Functions for Summarizing PDFs\\n03:27 - Defining the Main Function for Building the App\\n04:02 - Setting up Inputs and Outputs in the Gradio Interface\\n05:13 - Launching the App and Testing\\n07:05 - Conclusion and Next Steps', 'transcription': \"0:00\\nin this video you're going to learn how to build  a simple PDF summarization app okay to get started  \\n0:05\\nlet's set up our Imports so first we're going  to import gradu to have the app which is going  \\n0:12\\nto be the framework that we're going to use  to actually build the actual app then we're  \\n0:16\\ngoing to use Link Chain to do the summarization  stuff here I'm importing open AI to access the  \\n0:21\\nlarge language model and the prompt template  to do custom prompts for the summarization  \\n0:27\\nthen we're going to set up a character we're  going to import character splitter which we're  \\n0:31\\ngoing to use to split the text document into a  PDF into text chunks then we're going to load  \\n0:39\\nthe summarization chain from link chain and the  pi PDF loader to load the PDF so now I'm going  \\n0:46\\nto be setting up the large language model from  an open AI this is essentially charger PT and  \\n0:51\\nI'm setting temperature to 0 to be as precise as  possible and I'm setting up the text splitter to  \\n0:58\\nsplit the document into chunks now we're going  to do two functions for this one to summarize  \\n1:03\\nthe PDF and one to do custom summaries so first  we're going to be setting up the summarize PDF  \\n1:09\\nfunction which is going to take in the PDF file  path and a custom prompt which is going to be an  \\n1:15\\nempty string if I'm not doing a custom summary  but if I'm doing a custom summary this is going  \\n1:20\\nto carry that custom prompt then inside this  function I'm going to load the PDF using the pi  \\n1:27\\nPDF loader class then I'm going to load and split  the documents that I just I loaded the PDF with  \\n1:35\\nloader now I'm splitting that into chunks and I'm  going to save that to a variable called Docs then  \\n1:40\\nI'm going to set up a summarization chain with  the load some rise chain function from link chain  \\n1:46\\nand I'm going to set the Ching type to map  reduce there are many ways to do this there's  \\n1:52\\nlike the refine method there are a few chain  types that you can experiment with but I found  \\n1:56\\nthat this one works pretty well and then we're  going to run our summarization on the document  \\n2:02\\nand here I'm setting up a simple if where  if the custom prompt is not an empty string  \\n2:11\\nI'm going to do a custom summarization so that  means what that means that I'm gonna get the  \\n2:18\\ncustom prompt from the keyword argument in the  function and I'm going to build my prompt template  \\n2:26\\nusing the prompt template class from link chain  I'm fitting in the prompt template that I built  \\n2:33\\nusing the custom proc that I fed to this function  and I'm setting input variable Stacks which are  \\n2:39\\ngoing to be the uh essentially is what this  function is going to be taking in as the custom  \\n2:45\\nprompt and placing it um placing it as the text to  summarize then I'm going to summarize the content  \\n2:55\\nand in the load summarization chain I'm going  to map The Prompt that I just built to that low  \\n3:03\\nsummarization chain function so this variable  goes into here so that this thing understands  \\n3:08\\nthat it has to do a custom summary then I'm just  gonna run my custom summarization and that's it  \\n3:17\\nfinally here if there's no custom prompt I'm  just gonna set custom summary as an empty  \\n3:26\\nstring because this function returns both  a regular summary and a customized summary  \\n3:32\\nnow essentially before I had this as a separate  function like the custom summary like this but I  \\n3:40\\ndidn't really like this approach so although it's  here we're not going to be using this function  \\n3:47\\nand finally what I'm going to be doing is I'm  going to define a main function where we're  \\n3:51\\ngoing to actually build the app which will have  as a backend this function that's going to be  \\n3:56\\ndoing all the summarization for us so to build  the actual app we have to set up our inputs and  \\n4:01\\noutputs so the input is going to be the PDF  file path as a string so I'm going to feed it  \\n4:07\\nto a text box in gradu then I'm going to set up  an input for the custom prompt which is going to  \\n4:14\\nbe the same thing a text boxing radio and then I'm  going to set up the output as the output summary  \\n4:20\\nvariable which is also going to be a text box  where it's going to contain the actual summary  \\n4:26\\nand then an output custom summary which um  I think I'm not going to be using this one  \\n4:34\\nyeah so I did this a few versions and now I'm  just going through the line by line with the  \\n4:40\\nwith the code that I built before but [Music] um  now what I'm doing is I'm setting up a nice face  \\n4:47\\nvariable which um takes in a radio interface  class and then it feeds the function summarize  \\n4:55\\nPDF which is the function that we built here  then it's setting up inputs and outputs so as  \\n5:01\\ninputs it's a list with the input to the PDF  file path and an input to the custom prompt  \\n5:07\\nwhich is a just the text defining the custom  prompt that we want and as an output we have  \\n5:13\\noutput summary and output custom summary which  are right here they're going to be the text boxes  \\n5:22\\nthat are going to be storing the outputs from the  summarize PDF remember that summarize PDF function  \\n5:28\\nis returning a regular summary and a custom  summary if the custom prompt is fed to this  \\n5:34\\nfunction custom prompt I'm sure there are better  ways to do it but this was the the way that I  \\n5:40\\nfound worked the best then I'm going to launch my  app and that's pretty much it now I'm just saying  \\n5:48\\num now I'm just going to run the main function and  let's test this out so I already I already tested  \\n5:55\\nbefore so it's already working here but let's test  it again and see if everything is working great\\n6:03\\nokay so now I click the link with the URL  okay that looks good now what we're going  \\n6:10\\nto be doing is I'm going to get a path for a  PDF this is something that before people had  \\n6:16\\nstruggled with regarding the finding  the path to the PDF so I'm just gonna\\n6:25\\nseed PDF file path right here and then I'm  not going to give a custom prompt I'm just  \\n6:32\\ngoing to submit this file path and now it's  running so it's going to take a few seconds  \\n6:36\\nand once I think usually in my machine which is  not exactly a super amazing machine it takes a  \\n6:44\\nlittle bit a little while but boom there we  go this paper introduces video Transformer  \\n6:51\\npre-trained video model that captures semantics  and structure of instructional videos it's  \\n6:54\\nevaluated on six Downstream tasks including  step classification mistake step detection  \\n6:59\\netc etc now we can cross check with the actual  paper and this is the paper learning and  \\n7:05\\nverification of test structure instructional  videos video task former is the name of the  \\n7:11\\nmodel that they developed so this summary  is perfectly aligned with the actual paper  \\n7:16\\nand that's it that's how you build a very simple  PDF serverization app using radio and linking if  \\n7:24\\nyou like this video don't forget to like  And subscribe and see you next time cheers\"}, {'url': 'https://www.youtube.com/watch?v=H3s5fx7CsZg', 'chapters_section': '📚Chapters\\n0:00 - Introduction\\n0:02 - Building the transcription app with Whisper & Gradio\\n0:31 - Importing dependencies\\n0:43 - Writing the transcribe_audio function\\n1:26 - Setting up the main function\\n2:00 - Defining the Gradio interface\\n3:02 - Adding a title & description for the app\\n3:42 - Launching the interface\\n4:05 - Testing the app\\n5:20 - Conclusion and follow up', 'transcription': \"Introduction\\n0:00\\ntoday let's build another transcription\\nBuilding the transcription app with Whisper & Gradio\\n0:02\\napp using whisper and gradio\\n0:04\\nso a while back I did a video on\\n0:07\\nbuilding another transcription app using\\n0:09\\nwhisper and streamlit and it got great\\n0:11\\nresponse and a lot of comments which was\\n0:13\\nreally great and so I decided to do a\\n0:17\\nsecond video on the same topic using\\n0:19\\nradio this time to make it a little bit\\n0:21\\nsimpler and easier to run also I\\n0:24\\npromised someone in the comments to do\\n0:25\\nthat so I always try to keep my promises\\n0:27\\nokay so we're going to start by simply\\n0:29\\nimporting the dependencies that we need\\nImporting dependencies\\n0:31\\nso we're going to import radio as gr and\\n0:34\\nwe're going to import whisper for doing\\n0:37\\nthe audio transcription now the first\\n0:39\\nthing we're going to do is we're going\\n0:40\\nto write a function called transcribe\\nWriting the transcribe_audio function\\n0:43\\naudio which will take in a file path and\\n0:47\\noutput the transcription of that file\\n0:49\\nthe audio file path then we're going to\\n0:52\\nset up the model from whisper by just\\n0:54\\ndoing\\n0:55\\nwhisper.load model and we're going to\\n0:58\\nuse the base model because you know it\\n1:01\\nworks really well for English and now\\n1:03\\nwe're gonna just set up the result as\\n1:06\\nthe calling the model.transcribe\\n1:09\\nfunction from The Whisper model that we\\n1:11\\nloaded into that transcribed function\\n1:13\\nwe're going to feed the audio file which\\n1:17\\nis just going to be the file path to\\n1:18\\nthat audio and finally we're going to\\n1:20\\nreturn the transcription so that's easy\\n1:23\\nenough\\n1:24\\nnow we're going to set up our main\\nSetting up the main function\\n1:26\\nfunction\\n1:27\\nand the main function is going to be for\\n1:29\\nbuilding the interfacing gradient and\\n1:32\\nthat will take in a nodule input which\\n1:34\\nis going to be gradual input audio\\n1:39\\nwhich is something that we can just drag\\n1:41\\nand drop an audio file or we can just\\n1:43\\nclick it and then upload from our local\\n1:44\\ndesktop the source is going to be called\\n1:48\\nupload and the type is going to be\\n1:51\\ncalled file path because that's what\\n1:54\\nthis function accepts in order to run\\n1:57\\nand now we're going to say output text\\nDefining the Gradio interface\\n2:00\\nis equal to gr dot\\n2:05\\noutputs.txt box in white Xbox because\\n2:08\\nit's the text box that's going to be\\n2:11\\ncarrying it's going to be having the um\\n2:13\\nit's going to have the transcription of\\n2:15\\nthe audio file path\\n2:17\\nfinally once we've set that up we're\\n2:19\\njust going to set up our interface as\\n2:22\\nradio dot interface and then we're going\\n2:26\\nto say that the function for our\\n2:28\\ninterface is the transcribe audio\\n2:31\\nfunction\\n2:32\\nwhich is the function we just defined\\n2:34\\nover there\\n2:36\\nnow we're going to set up the inputs as\\n2:39\\nwe're going to set up the inputs as the\\n2:42\\naudio input right and we're going to set\\n2:45\\nup the outputs as the output that we\\n2:48\\njust Define here this variable that\\n2:51\\ncarries the output from the text box\\n2:54\\nokay that's perfect so now what we can\\n2:57\\ndo is\\n2:58\\nI can come here and I can say\\nAdding a title & description for the app\\n3:02\\num let's set up a title for our app it's\\n3:06\\njust going to be audio transcription\\n3:10\\napp because you know why not and a quick\\n3:15\\ndescription is just going to be\\n3:17\\nupload an audio file\\n3:20\\nand hit the\\n3:24\\nsubmit button\\n3:28\\nthere we go\\n3:30\\nperfect now we got it nice let's have\\n3:35\\nthis thing over here perfect\\n3:37\\nnow that we have that we have to launch\\n3:39\\nour interface so we're going to just\\n3:41\\ncall lunch\\nLaunching the interface\\n3:42\\nand that's our app in a nutshell because\\n3:45\\npython is amazing and we can build super\\n3:48\\ncomplex and interesting stuff with just\\n3:50\\na few lines of code and now I can just\\n3:53\\nsay if name equal main I'm going to call\\n3:56\\nthe main function which is going to set\\n3:59\\nup my\\n4:00\\ntranscription or the transcription app\\n4:03\\nand now that I have all of that we can\\nTesting the app\\n4:05\\ntest it so I can come here and say\\n4:07\\npython Auto transcription app I already\\n4:10\\nhave all radio and Whisper installed so\\n4:12\\nif you don't run pip install radio\\n4:14\\nwhisper Etc\\n4:16\\nso now we're running the audio\\n4:18\\ntranscription app which should open here\\n4:20\\non the browser on my right\\n4:22\\nso let's take a look at that\\n4:24\\nwe'll click on the link that's provided\\n4:26\\nand now this is our Simple app\\n4:29\\nand I'm going to click here on the Audio\\n4:31\\nI already have an audio file set up we\\n4:34\\ncan listen to it\\n4:39\\nawesome and now I can hit submit and I\\n4:42\\nwill do a magic trick and there we have\\n4:45\\nit here's our transcription for the\\n4:48\\naudio input that we set up I'm going to\\n4:50\\nset up a nicely organized GitHub repo\\n4:54\\nwith this project so that you can run it\\n4:56\\njust by running one command and I want\\n4:59\\nto make it easy for you guys because you\\n5:01\\nguys gave me a lot of cool input and a\\n5:03\\nbunch of awesome comments for the last\\n5:05\\nvideo that I did on audio transcription\\n5:07\\nso I wanted to do that again in a\\n5:09\\nsimpler way and make it easy so that for\\n5:12\\nthose of you that want to use it you can\\n5:14\\njust go to my repo and just clone and\\n5:18\\nrun and so thanks for watching guys if\\nConclusion and follow up\\n5:20\\nyou like this video don't forget to like\\n5:22\\nAnd subscribe and see you next time\\n5:24\\ncheers\"}, {'url': 'https://www.youtube.com/watch?v=p_MQRWH5Y6k', 'chapters_section': '📚 Chapters:\\n00:00 - Intro\\n00:32 - Tasks Database\\n01:05 - Imports\\n01:17 - Setting up the ChatGPT API\\n01:52 - Setting the OpenAI API key\\n02:00 - Loading Tasks DB from csv file\\n02:51 - Main prompt for ChatGPT\\n03:34 - Writing Output to file\\n04:19 - Visualizing the task schedule\\n04:51 - Comparing with GPT-4\\n07:00 - Final Thoughts', 'transcription': \"Intro\\n0:00\\nall right so today we're going to be\\n0:01\\nbuilding the automation for daily\\n0:03\\nscheduling so scheduling your to-do list\\n0:05\\nfor today essentially the idea is to\\n0:07\\ngrab a bunch of tasks from a database of\\n0:10\\ntasks that you have to do and then you\\n0:12\\nknow filter them according to some\\n0:13\\nparameters and create a tailored\\n0:17\\nschedule for that day and the tasks that\\n0:20\\nyou have to do sorry about my voice but\\n0:22\\nI was cheering for a friend of mine who\\n0:24\\nwent to fight in an MMA event and now I\\n0:27\\nlost my voice so it is what it is so\\n0:29\\nlet's get started so for the first task\\n0:31\\nof automating daily scheduling we're\\nTasks Database\\n0:33\\ngoing to be based on this idea of a task\\n0:37\\ndatabase where I have a test database\\n0:39\\ncomposed of a task a type content\\n0:43\\ncreation personal work learning Etc it\\n0:45\\ndid an effort which is an estimation of\\n0:48\\nthe number of hours that takes to\\n0:49\\ncomplete a task then a date which is a\\n0:52\\ndeadline for the task if the task has a\\n0:55\\ndeadline finally the task the database\\n0:58\\nwill also have a priority column\\n1:00\\nindicating the level of importance of\\n1:02\\nthe task where one means very low\\nImports\\n1:05\\npriority and five means high priority\\n1:07\\nokay perfect okay guys so first things\\n1:10\\nfirst we're going to import our\\n1:11\\ndependencies so we're going to be\\n1:12\\nimporting open AI to access the\\n1:14\\nchargeable TPI then we're going to have\\n1:16\\na few other Imports to handle some\\nSetting up the ChatGPT API\\n1:17\\nprocessing then what we're going to do\\n1:19\\nis we're going to first Define the get\\n1:21\\nresponse function to access the\\n1:22\\nchargeable TPI so here we're essentially\\n1:24\\njust using the open AI API and then the\\n1:26\\ncheck completion class and the create\\n1:28\\nmethod to access in this case we're\\n1:30\\ngoing to be accessing GPT 3.5 turbo but\\n1:34\\nI actually want to test with both models\\n1:36\\nto see which one works the best then\\n1:37\\nwe're going to set up the message for\\n1:40\\nthe system and in this case I'm just\\n1:42\\nsaying you're helpful research and\\n1:44\\nprogramming assistant and then I will\\n1:47\\nsend the prompt question which will be\\n1:49\\nfed to the function as an argument then\\n1:51\\nI'm going to respond return the response\\nSetting the OpenAI API key\\n1:53\\nof the model so pretty boilerplate and\\n1:55\\nI'm going to set up the open air API key\\n1:57\\nthen I'm going to read a CSV file with\\n1:59\\nthe database that I exported from notion\\nLoading Tasks DB from csv file\\n2:02\\nso essentially I'm exporting this\\n2:04\\ndatabase to a CSV file then I unzip it\\n2:07\\nand save it to a file called Tesla CSV\\n2:11\\nfinally I'm gonna get today's date which\\n2:14\\nI'm going to be using to feed the model\\n2:17\\nwith the information of which day is\\n2:18\\ntoday so that it can do proper\\n2:20\\nscheduling of the tasks according to the\\n2:22\\nday and then I'm going to transform that\\n2:25\\ntask to a string because here this is an\\n2:29\\ninteresting part I'm not going to be\\n2:30\\nusing link chain or I'm not going to be\\n2:33\\nvectorizing the test database I wanted\\n2:36\\nto do something a little bit simpler and\\n2:38\\nso I'm just going to feed the string\\n2:40\\nbecause the string is not going to be\\n2:41\\nthat grip that big with this amount of\\n2:44\\ninformation because I don't want to do\\n2:45\\nlike a super planning of like a month I\\n2:48\\njust want to do like a basic\\n2:49\\ndaily planning of scheduling some tasks\\nMain prompt for ChatGPT\\n2:53\\nfinally we're going to Define our prompt\\n2:55\\nnow here is where it gets interesting so\\n2:58\\nthe prompt is going to be consider that\\n3:00\\ntoday is someday X then write a test\\n3:03\\nschedule for tomorrow consider the\\n3:06\\nfollowing effort column represents\\n3:08\\nprojection of hours to complete the task\\n3:11\\nthe goal should be to focus more on work\\n3:13\\nand content creation but do other tasks\\n3:16\\nif there is time respect to priority\\n3:18\\nlevels and make this schedule as\\n3:19\\noptimized as possible tasks with\\n3:22\\ndeadlines should be performed before or\\n3:23\\nat the day of the deadline and the\\n3:25\\noutperformance should be start time and\\n3:27\\ntime task and the task database is this\\n3:30\\nso essentially I give as much context as\\n3:33\\nI can to the model\\nWriting Output to file\\n3:34\\nand that I print the result of the term\\n3:36\\nLoop\\n3:37\\nand that's pretty much it I also want to\\n3:40\\nsave it as a schedule so let's just save\\n3:44\\nit as daily schedule.xc\\n3:49\\nI'm going to access and then put\\n3:52\\nnow we're going to say after right and\\n3:55\\nthen I'm going to say\\n3:57\\nah\\n3:58\\nwe're going to take this as out of the\\n4:00\\nmodel so I'll put output schedule secret\\n4:04\\nto this and then we're going to print it\\n4:07\\nand we're also going to write a file\\n4:09\\nbeautiful and that's pretty much it so\\n4:11\\nlet's give it a test run to this version\\n4:14\\nof the automation automating the\\n4:16\\nschedule let's take a look alright cool\\n4:18\\nso we got our response back\\nVisualizing the task schedule\\n4:21\\nso let's take a look let's save this\\n4:23\\nactually I'm gonna rename this as daily\\n4:26\\nschedule\\n4:27\\ngp3 uh 3.5 turbo so this is the output\\n4:31\\nthat I got so 9 to 10 30 finish\\n4:33\\nremaining minor that's work okay that\\n4:35\\nmakes sense this is a work task okay 12\\n4:38\\nto 1 say your health Matrix I'll cry\\n4:41\\nI'll cry and then to five or last up\\n4:43\\nwords yeah it had all sorts of tests in\\n4:44\\nthe database note that the priority\\n4:46\\nlevels have been considered in all the\\n4:48\\ntasks have been scheduled okay so this\\n4:50\\nlooks pretty good but now what I would\\nComparing with GPT-4\\n4:51\\nlike to do is compare that to be using\\n4:54\\ngpt4 so I'm Gonna Save the same thing\\n4:57\\nand I'm gonna say with gpt4 and I'm\\n5:01\\ngonna run this again so I'm going to\\n5:03\\nsave this and run it again let's take a\\n5:05\\nlook to see what it looks like all right\\n5:06\\nperfect so we got a response and this is\\n5:10\\nactually looking pretty good so we have\\n5:13\\nthis format that I asked for the only\\n5:15\\nthing the model got around was the date\\n5:17\\nbut that's my mistake because I fed it\\n5:19\\nin the wrong format yeah I should have\\n5:23\\nsaid month should have said month day\\n5:26\\nyear that's why we can assess the output\\n5:29\\nrecord the short Monday Tuesday video\\n5:31\\nand then there's some learning tasks\\n5:33\\nokay I mean the model is assuming that\\n5:35\\nI'm going to be working from 8 AM to 10\\n5:37\\no'clock but that's okay\\n5:39\\nand now we can take a look at both\\n5:42\\nschedules side by side so one thing that\\n5:46\\nI notice I like the concisiveness the\\n5:48\\ncon how concise the output from gpd4 is\\n5:51\\nso I like that\\n5:54\\num\\n5:55\\nI feel like in terms of for example work\\n5:59\\npresentation is a one hour task in\\n6:02\\nYouTube before said that it would take\\n6:05\\ntwo hours while let's see well this was\\n6:09\\na task that actually gbt 3.5 didn't even\\n6:12\\nconsider now one thing I didn't like was\\n6:15\\nthat the um\\n6:17\\npriorities don't seem to be that\\n6:19\\nstraight because\\n6:21\\nthe most important task was\\n6:25\\nuh the most important task was\\n6:29\\nhere exactly\\n6:30\\nso there was this one that was a big\\n6:32\\ntask\\n6:34\\nwhich was to do some modifications on an\\n6:37\\nAPI thing that I have to do for work\\n6:39\\nand then finishing build changes then\\n6:42\\nfinishing remaining minor tasks\\n6:44\\nbut it's okay I mean the the fact that I\\n6:47\\nhad an effort of 12 hours for the big\\n6:49\\ntask makes sense that it would not\\n6:51\\nschedule when I don't have the time to\\n6:53\\ncomplete it so I would say that this is\\n6:55\\na pretty cool application of charger PT\\n6:58\\nand using the API I think that one of\\nFinal Thoughts\\n7:00\\nthe ideas that I had was to connect this\\n7:01\\nwith notion and I bet that you can do\\n7:03\\nthat with zapier and stuff but I can\\n7:05\\nalso do it with python code which is\\n7:07\\ngreat because then you can have this\\n7:09\\nrunning and then you just you know\\n7:11\\ncreate a schedule for the day and you\\n7:13\\njust go through the day and you don't\\n7:14\\nhave to think about it so this is a\\n7:16\\npretty good example of an application\\n7:18\\nusing charge BT that's it hope you guys\\n7:20\\nlike this video if you like it don't\\n7:21\\nforget to like And subscribe and see you\\n7:23\\nnext time cheers\"}, {'url': 'https://www.youtube.com/watch?v=5VAuGvefzEI', 'chapters_section': '📚 Chapters:\\n\\n00:00 - Intro\\n00:32 - Tasks Database\\n01:05 - Imports\\n01:17 - Setting up the ChatGPT API\\n01:52 - Setting the OpenAI API key\\n02:00 - Loading Tasks DB from csv file\\n02:51 - Main prompt for ChatGPT\\n03:34 - Writing Output to file\\n04:19 - Visualizing the task schedule\\n04:51 - Comparing with GPT-4\\n07:00 - Final Thoughts', 'transcription': \"Intro\\n0:00\\nall right so today we're going to be\\n0:01\\nbuilding the automation for daily\\n0:03\\nscheduling so scheduling your to-do list\\n0:05\\nfor today essentially the idea is to\\n0:07\\ngrab a bunch of tasks from a database of\\n0:10\\ntasks that you have to do and then you\\n0:12\\nknow filter them according to some\\n0:13\\nparameters and create a tailored\\n0:17\\nschedule for that day and the tasks that\\n0:20\\nyou have to do sorry about my voice but\\n0:22\\nI was cheering for a friend of mine who\\n0:24\\nwent to fight in an MMA event and now I\\n0:27\\nlost my voice so it is what it is so\\n0:29\\nlet's get started so for the first task\\n0:31\\nof automating daily scheduling we're\\nTasks Database\\n0:33\\ngoing to be based on this idea of a task\\n0:37\\ndatabase where I have a test database\\n0:39\\ncomposed of a task a type content\\n0:43\\ncreation personal work learning Etc it\\n0:45\\ndid an effort which is an estimation of\\n0:48\\nthe number of hours that takes to\\n0:49\\ncomplete a task then a date which is a\\n0:52\\ndeadline for the task if the task has a\\n0:55\\ndeadline finally the task the database\\n0:58\\nwill also have a priority column\\n1:00\\nindicating the level of importance of\\n1:02\\nthe task where one means very low\\nImports\\n1:05\\npriority and five means high priority\\n1:07\\nokay perfect okay guys so first things\\n1:10\\nfirst we're going to import our\\n1:11\\ndependencies so we're going to be\\n1:12\\nimporting open AI to access the\\n1:14\\nchargeable TPI then we're going to have\\n1:16\\na few other Imports to handle some\\nSetting up the ChatGPT API\\n1:17\\nprocessing then what we're going to do\\n1:19\\nis we're going to first Define the get\\n1:21\\nresponse function to access the\\n1:22\\nchargeable TPI so here we're essentially\\n1:24\\njust using the open AI API and then the\\n1:26\\ncheck completion class and the create\\n1:28\\nmethod to access in this case we're\\n1:30\\ngoing to be accessing GPT 3.5 turbo but\\n1:34\\nI actually want to test with both models\\n1:36\\nto see which one works the best then\\n1:37\\nwe're going to set up the message for\\n1:40\\nthe system and in this case I'm just\\n1:42\\nsaying you're helpful research and\\n1:44\\nprogramming assistant and then I will\\n1:47\\nsend the prompt question which will be\\n1:49\\nfed to the function as an argument then\\n1:51\\nI'm going to respond return the response\\nSetting the OpenAI API key\\n1:53\\nof the model so pretty boilerplate and\\n1:55\\nI'm going to set up the open air API key\\n1:57\\nthen I'm going to read a CSV file with\\n1:59\\nthe database that I exported from notion\\nLoading Tasks DB from csv file\\n2:02\\nso essentially I'm exporting this\\n2:04\\ndatabase to a CSV file then I unzip it\\n2:07\\nand save it to a file called Tesla CSV\\n2:11\\nfinally I'm gonna get today's date which\\n2:14\\nI'm going to be using to feed the model\\n2:17\\nwith the information of which day is\\n2:18\\ntoday so that it can do proper\\n2:20\\nscheduling of the tasks according to the\\n2:22\\nday and then I'm going to transform that\\n2:25\\ntask to a string because here this is an\\n2:29\\ninteresting part I'm not going to be\\n2:30\\nusing link chain or I'm not going to be\\n2:33\\nvectorizing the test database I wanted\\n2:36\\nto do something a little bit simpler and\\n2:38\\nso I'm just going to feed the string\\n2:40\\nbecause the string is not going to be\\n2:41\\nthat grip that big with this amount of\\n2:44\\ninformation because I don't want to do\\n2:45\\nlike a super planning of like a month I\\n2:48\\njust want to do like a basic\\n2:49\\ndaily planning of scheduling some tasks\\nMain prompt for ChatGPT\\n2:53\\nfinally we're going to Define our prompt\\n2:55\\nnow here is where it gets interesting so\\n2:58\\nthe prompt is going to be consider that\\n3:00\\ntoday is someday X then write a test\\n3:03\\nschedule for tomorrow consider the\\n3:06\\nfollowing effort column represents\\n3:08\\nprojection of hours to complete the task\\n3:11\\nthe goal should be to focus more on work\\n3:13\\nand content creation but do other tasks\\n3:16\\nif there is time respect to priority\\n3:18\\nlevels and make this schedule as\\n3:19\\noptimized as possible tasks with\\n3:22\\ndeadlines should be performed before or\\n3:23\\nat the day of the deadline and the\\n3:25\\noutperformance should be start time and\\n3:27\\ntime task and the task database is this\\n3:30\\nso essentially I give as much context as\\n3:33\\nI can to the model\\nWriting Output to file\\n3:34\\nand that I print the result of the term\\n3:36\\nLoop\\n3:37\\nand that's pretty much it I also want to\\n3:40\\nsave it as a schedule so let's just save\\n3:44\\nit as daily schedule.xc\\n3:49\\nI'm going to access and then put\\n3:52\\nnow we're going to say after right and\\n3:55\\nthen I'm going to say\\n3:57\\nah\\n3:58\\nwe're going to take this as out of the\\n4:00\\nmodel so I'll put output schedule secret\\n4:04\\nto this and then we're going to print it\\n4:07\\nand we're also going to write a file\\n4:09\\nbeautiful and that's pretty much it so\\n4:11\\nlet's give it a test run to this version\\n4:14\\nof the automation automating the\\n4:16\\nschedule let's take a look alright cool\\n4:18\\nso we got our response back\\nVisualizing the task schedule\\n4:21\\nso let's take a look let's save this\\n4:23\\nactually I'm gonna rename this as daily\\n4:26\\nschedule\\n4:27\\ngp3 uh 3.5 turbo so this is the output\\n4:31\\nthat I got so 9 to 10 30 finish\\n4:33\\nremaining minor that's work okay that\\n4:35\\nmakes sense this is a work task okay 12\\n4:38\\nto 1 say your health Matrix I'll cry\\n4:41\\nI'll cry and then to five or last up\\n4:43\\nwords yeah it had all sorts of tests in\\n4:44\\nthe database note that the priority\\n4:46\\nlevels have been considered in all the\\n4:48\\ntasks have been scheduled okay so this\\n4:50\\nlooks pretty good but now what I would\\nComparing with GPT-4\\n4:51\\nlike to do is compare that to be using\\n4:54\\ngpt4 so I'm Gonna Save the same thing\\n4:57\\nand I'm gonna say with gpt4 and I'm\\n5:01\\ngonna run this again so I'm going to\\n5:03\\nsave this and run it again let's take a\\n5:05\\nlook to see what it looks like all right\\n5:06\\nperfect so we got a response and this is\\n5:10\\nactually looking pretty good so we have\\n5:13\\nthis format that I asked for the only\\n5:15\\nthing the model got around was the date\\n5:17\\nbut that's my mistake because I fed it\\n5:19\\nin the wrong format yeah I should have\\n5:23\\nsaid month should have said month day\\n5:26\\nyear that's why we can assess the output\\n5:29\\nrecord the short Monday Tuesday video\\n5:31\\nand then there's some learning tasks\\n5:33\\nokay I mean the model is assuming that\\n5:35\\nI'm going to be working from 8 AM to 10\\n5:37\\no'clock but that's okay\\n5:39\\nand now we can take a look at both\\n5:42\\nschedules side by side so one thing that\\n5:46\\nI notice I like the concisiveness the\\n5:48\\ncon how concise the output from gpd4 is\\n5:51\\nso I like that\\n5:54\\num\\n5:55\\nI feel like in terms of for example work\\n5:59\\npresentation is a one hour task in\\n6:02\\nYouTube before said that it would take\\n6:05\\ntwo hours while let's see well this was\\n6:09\\na task that actually gbt 3.5 didn't even\\n6:12\\nconsider now one thing I didn't like was\\n6:15\\nthat the um\\n6:17\\npriorities don't seem to be that\\n6:19\\nstraight because\\n6:21\\nthe most important task was\\n6:25\\nuh the most important task was\\n6:29\\nhere exactly\\n6:30\\nso there was this one that was a big\\n6:32\\ntask\\n6:34\\nwhich was to do some modifications on an\\n6:37\\nAPI thing that I have to do for work\\n6:39\\nand then finishing build changes then\\n6:42\\nfinishing remaining minor tasks\\n6:44\\nbut it's okay I mean the the fact that I\\n6:47\\nhad an effort of 12 hours for the big\\n6:49\\ntask makes sense that it would not\\n6:51\\nschedule when I don't have the time to\\n6:53\\ncomplete it so I would say that this is\\n6:55\\na pretty cool application of charger PT\\n6:58\\nand using the API I think that one of\\nFinal Thoughts\\n7:00\\nthe ideas that I had was to connect this\\n7:01\\nwith notion and I bet that you can do\\n7:03\\nthat with zapier and stuff but I can\\n7:05\\nalso do it with python code which is\\n7:07\\ngreat because then you can have this\\n7:09\\nrunning and then you just you know\\n7:11\\ncreate a schedule for the day and you\\n7:13\\njust go through the day and you don't\\n7:14\\nhave to think about it so this is a\\n7:16\\npretty good example of an application\\n7:18\\nusing charge BT that's it hope you guys\\n7:20\\nlike this video if you like it don't\\n7:21\\nforget to like And subscribe and see you\\n7:23\\nnext time cheers\"}, {'url': 'https://www.youtube.com/watch?v=Q2U-tt1JffY', 'chapters_section': '📚 Chapters\\n00:00 - Introduction  \\n00:01 - Setting up the environment  \\n00:07 - Importing modules and API authentication  \\n00:22 - Cloning and loading the repository  \\n01:53 - Loading the files in the repository  \\n02:14 - Chunking the files  \\n03:00 - Vectorizing the documents  \\n04:07 - Creating a dataset and adding documents  \\n04:53 - Accessing the database  \\n05:23 - Setting up the retriever  \\n06:15 - Setting up a chat with OpenAI  \\n07:05 - Querying the codebase  \\n08:02 - Printing the answers  \\n08:46 - Conclusion', 'transcription': \"0:00\\nin this video you're going to learn how\\n0:01\\nto ask questions to a code base using\\n0:03\\nlink chain let's go so first things\\n0:05\\nfirst to set up this environment you're\\n0:07\\ngoing to need to install the um a few\\n0:11\\npackages that you know are just common\\n0:13\\nwhen you're using linking so in this\\n0:15\\ncase link change Deep Lake open Ai and\\n0:18\\ntick token\\n0:19\\nso then for our Imports we're going to\\n0:22\\nimport OS we're going to import get pass\\n0:24\\nwhich actually we're not going to be\\n0:25\\nusing because we're going to do the\\n0:26\\nauthentication with the apis just by\\n0:29\\nsetting the environment variables we're\\n0:31\\ngoing to then import open AI embeddings\\n0:33\\nto do our embeddings of the code base\\n0:36\\nthat we're going to be loading we're\\n0:38\\ngoing to do the vectorization using deep\\n0:40\\nlake which is the approach they used in\\n0:42\\nthe documentation\\n0:43\\nthen you're gonna set up your active\\n0:46\\nLoop token API key which you can do by\\n0:50\\njust going to the Blake website setting\\n0:53\\nup an account and acquiring your API key\\n0:57\\nhere you could also set up your open AI\\n1:00\\nAPI key I already have mine as an\\n1:02\\nenvironment variable in my system and\\n1:05\\nthen we're going to load the embeddings\\n1:08\\nfor using the open air embeddings class\\n1:13\\nthen what we're going to be doing is\\n1:14\\nwe're going to clone the repository that\\n1:16\\nwe're interested in in this case I'm\\n1:18\\ncloning a repository called motion\\n1:20\\ncanvas which is a typescript framework\\n1:22\\nto generate animations through code and\\n1:25\\nI'm really interested in this framework\\n1:26\\nbecause I'm learning how to code\\n1:27\\nanimations so I but there's like a lot\\n1:31\\nof things in this specific repo they're\\n1:33\\na bit confusing and a bit complicated so\\n1:36\\neven if you follow their actual tutorial\\n1:39\\nso what I'm doing here is I'm just gonna\\n1:41\\nsee fling Ching maybe can help me\\n1:43\\nthrough this question answering\\n1:46\\nsetup\\n1:48\\nokay so following\\n1:50\\num\\n1:51\\nmoving on we're going to be loading all\\n1:53\\nthe files inside the repo we're going to\\n1:55\\nbe using the text loader to do the\\n1:58\\nloading of the tax documents which will\\n2:01\\nbe the files in the code base\\n2:03\\nwe're going to set up the root directory\\n2:05\\nas the directory that contains the um\\n2:08\\ncode that we cloned\\n2:11\\nthen we're going to set up a empty list\\n2:14\\nfor storing the docs that we're gonna\\n2:17\\nload from the code base then we're going\\n2:20\\nto Loop through that code base and for\\n2:22\\neach of the files we're going to try to\\n2:23\\nload it with the text loader\\n2:26\\nand then\\n2:28\\num append that at that two hour uh docs\\n2:32\\nlist which will be a list with a bunch\\n2:34\\nof document objects that will be the\\n2:36\\nresult of loading each file with this\\n2:37\\ntext loader\\n2:39\\nthen if that doesn't work we're going to\\n2:41\\nset up an exception and we're gonna move\\n2:43\\non all right perfect so now after that\\n2:47\\nwhat we're going to be doing is here I'm\\n2:49\\njust\\n2:50\\num making sure that the docs list is\\n2:53\\nactually filled with documents so I'm\\n2:55\\njust printing the length of that list in\\n2:58\\nthis case I have 1 321 files that were\\n3:00\\nloaded and if I came here and I did\\n3:03\\nsomething like okay so print\\n3:06\\nlength of docs but then I showed you\\n3:08\\nguys what docs is it's just gonna be a\\n3:11\\nlist full of document objects that\\n3:14\\ncontains the files and the content of\\n3:16\\nthe files from the code base\\n3:18\\nokay so then we're gonna chunk those\\n3:21\\nfiles and we're going to do that using\\n3:23\\nthe character text Splitter from link\\n3:25\\nchain and that's as easy as just calling\\n3:28\\ncharacter text splitter we're gonna set\\n3:30\\nup some chunk size and some chunk\\n3:33\\noverlap it's important to note that for\\n3:34\\nthis example I didn't tune none of these\\n3:37\\nparameters but you can do that yourself\\n3:39\\nif you think that it's not working for\\n3:41\\nyour particular use case\\n3:43\\nthen we're going to split the documents\\n3:46\\nfrom the docs and save that to a\\n3:49\\nvariable called texts\\n3:51\\nso then here it was creating all the\\n3:53\\nchunks with different sizes from all\\n3:56\\nthose documents from the list\\n3:57\\nand then once we've done that now it's\\n4:00\\nthe vectorization bit so now we're going\\n4:02\\nto set up\\n4:04\\num we're going to create a public data\\n4:07\\nset although right now I'm not 100 sure\\n4:09\\nit's public just because I'm not setting\\n4:11\\npublic here because I think that they\\n4:13\\nremoved this parameter from a later\\n4:16\\nrelease of the Blake I don't know\\n4:19\\nhowever we're just going to be setting\\n4:22\\nup the Blake and then we're going to set\\n4:24\\nup the data set and the data set is\\n4:25\\ngoing to be based on this\\n4:28\\num the code base that we're trying to\\n4:30\\nthat were that we embedded or indexed\\n4:32\\nthen we're going to add all the\\n4:34\\ndocuments that we just chunked\\n4:36\\nhere\\n4:39\\nand\\n4:40\\nthat is pretty much it that's pretty\\n4:43\\nmuch we get what we got to do to do the\\n4:46\\nvectorization part so then it's going to\\n4:48\\nbe doing all the indexing\\n4:50\\nand once we are done\\n4:53\\nwe can then access it so now I'm\\n4:56\\naccessing that database\\n4:58\\nand\\n5:00\\nnow here I'm accessing the database and\\n5:03\\nonce we've accessed the database it's\\n5:05\\ngoing to print something like the data\\n5:06\\nthat can be visualized in Jupiter\\n5:07\\nnotebook Etc uh this data set was loaded\\n5:11\\nsuccessfully it is saved in whatever\\n5:14\\nPlace\\n5:15\\netc etc and then it's going to give some\\n5:17\\nmeta information about the data set like\\n5:19\\nthe size of the embedding and stuff like\\n5:20\\nthat\\n5:21\\nand now we can set up our retriever\\n5:23\\nwhich is going to be the thing that's\\n5:25\\ngoing to allow us to ask questions to\\n5:27\\nthe code base\\n5:29\\nyep so for the retriever we can set up\\n5:33\\nsome search keyword arguments like the\\n5:36\\ndistance metric which in this case if\\n5:38\\nI'm not mistaken this is cosine\\n5:39\\nsimilarity\\n5:41\\nand then the number of things that it\\n5:43\\nfetches if I'm not mistaking there's a\\n5:45\\nhundred uh and marks more marginal\\n5:48\\nrelevance to be 100 honest I'm not sure\\n5:50\\nwhat it is\\n5:52\\nuh but okay so this is I think this\\n5:55\\npretty much that this will return the\\n5:58\\ntop 10K answers and then from those it\\n6:02\\nwill give the the best one I think it's\\n6:04\\nsomething like that not a hundred\\n6:06\\npercent sure\\n6:07\\nokay but we set up these search keyword\\n6:09\\narguments for the retriever and then we\\n6:13\\ncan now just set up the chat with open\\n6:15\\nAI so that we can ask the questions and\\n6:17\\nthen just get the responses so we're\\n6:20\\ngonna load Channel open a chat open AI\\n6:22\\nfrom the linkchang.chat models and we're\\n6:25\\ngonna load the conversational retrieval\\n6:28\\nchain\\n6:29\\nfrom link Chang dot chains so we set up\\n6:31\\nour model in this case I'm loading\\n6:33\\ndirectly gpt4 you could load gp3 3.5\\n6:37\\nturbo like hbt it's up to you and you\\n6:41\\nsee what works best for your case\\n6:43\\nand then we're going to set up the\\n6:45\\nconversational retrieval chain and then\\n6:47\\nwe're gonna call the from llm method and\\n6:51\\nwe're going to feed that model that we\\n6:53\\njust loaded here and the retriever that\\n6:55\\nwe just set up in the previous cell\\n6:58\\nright here\\n7:00\\nso now with this we have all we need to\\n7:03\\nstart querying\\n7:05\\nto start querying the code base so I'm\\n7:08\\ngoing to set up a list\\n7:10\\nwith all the questions that I have for\\n7:11\\nthe code base in this case I just asked\\n7:13\\nchoose simple questions like how to\\n7:14\\ncreate an animation of a moving Square\\n7:16\\nmotion canvas and what are the core\\n7:20\\ncomponents to create an animation in\\n7:22\\nmotion canvas\\n7:23\\nuh and when I and once I've\\n7:28\\nset up these questions I can now set up\\n7:31\\nan empty list that will store the chat\\n7:33\\nhistory between me and the code base\\n7:35\\nthroughout this conversation\\n7:37\\nand then for each question in my\\n7:40\\nquestions list I'm gonna\\n7:42\\nquery the database I'm going to query\\n7:45\\nthat retriever chain\\n7:46\\nand then I'm gonna append the question\\n7:51\\nand the answer to my chat history\\n7:54\\nin print the out print the question and\\n7:56\\nprint the answer so it's as simple as\\n7:58\\nthat and now we can I can run this\\n8:02\\nand get the response\\n8:06\\noh sometimes it takes a little bit to\\n8:08\\nload I'm sorry to to run\\n8:12\\nso I'm going to do a magic trick\\n8:14\\nchildren or I guess so it took a while\\n8:17\\nbut here we have our answers so it went\\n8:21\\nas far as output the entire code that I\\n8:24\\nneed to do what I wanted to do in motion\\n8:26\\ngavis for my first question and then for\\n8:29\\nthe second question it's it discusses\\n8:32\\nthe main components of the motion motion\\n8:35\\ncamera so these are sets of elements\\n8:37\\nlet's play on the screen along with\\n8:38\\nanimations Etc by combining these\\n8:40\\ncomponents you can create complex and\\n8:41\\ninformative Vector animations all right\\n8:44\\ncool we can see that in a text editor\\n8:46\\nso this was the question what are the\\n8:49\\ncomponents to create an animation then\\n8:51\\nthe answer\\n8:52\\nto create you need the following group a\\n8:54\\nproject configuration project yeah\\n8:56\\nthat's all correct array of scenes\\n8:59\\nthat's correct scenes yeah that's\\n9:02\\nperfect\\n9:03\\nso it works it's amazing so I'm very\\n9:06\\nsatisfied with this answer all right\\n9:08\\nguys that's it that's how you query a\\n9:10\\ncode base using link chain if you like\\n9:11\\nthis video don't forget to like And\\n9:13\\nsubscribe and see you next time\"}, {'url': 'https://www.youtube.com/watch?v=MXBxNUnC7iY', 'chapters_section': \"📚 Chapters\\n00:00 - Introduction\\n00:01 - Overview of the Research Paper\\n00:14 - The Idea of Thought Sequences\\n00:29 - System's Reasoning and Decision-Making Capabilities\\n00:49 - Comparing Different Prompting Approaches\\n01:02 - Chain of Thought Prompting\\n01:14 - Chain of Thought with Self-Consistency\\n01:21 - Problems with Current Approaches\\n01:43 - Decomposition Stage\\n01:50 - Generation Stage\\n02:10 - Evaluation Stage\\n02:34 - Search Stage\\n02:59 - Research Results\\n03:10 - Game 24 Task\\n03:19 - Creative Writing Task\\n03:27 - Crossword Tasks\\n04:00 - Evaluating the Tree of Thought Approach\\n04:26 - Creative Writing Task Results\\n04:48 - Crossword Puzzle Task Discussion\\n05:39 - Success Rate of Tree of Thought Approach\\n06:03 - Related Works\\n06:19 - Modularity and Flexibility of the Approach\\n06:37 - Improved Interpretability\\n06:43 - System One and System Two Discussion\\n07:06 - Conclusion\", 'transcription': \"Introduction\\n0:00\\ntoday we're looking at this paper three\\n0:01\\nof thoughts deliberate problem solving\\n0:03\\nwith large language models now\\n0:05\\nessentially the authors developed this\\n0:07\\nframework that injects planning and\\n0:09\\ndecision making capabilities into large\\n0:11\\nlanguage models they do that by setting\\n0:14\\nup a system that leverages the\\n0:16\\ngeneration of what they call thought\\n0:17\\nsequences as well as self-evaluation in\\n0:21\\norder for the model to be able to\\n0:23\\nevaluate the quality of those thoughts\\n0:25\\nbased on how much they progress the\\n0:27\\nproblem forward it's really interesting\\nFramework\\n0:29\\nbecause this framework allows the system\\n0:31\\nto reason over possible paths towards\\n0:34\\nsolving a problem and have have a way to\\n0:38\\npick one path that seems more promising\\n0:41\\nthan the other as well as look ahead or\\n0:43\\nbacktrack to make better decisions and\\n0:45\\nto adapt in different situations now\\n0:47\\nthey start by comparing similar\\n0:49\\nprompting approaches like input output\\n0:51\\nprompt where you're just directly\\n0:53\\nprompting the model to solve the problem\\n0:55\\ngiving a few Test instructions or some\\n0:58\\nfew shot examples and then they talk\\n1:00\\nabout Chain of Thought prompting where\\n1:02\\nthe key idea is to introduce a chain of\\n1:04\\nthoughts between the input X and the\\n1:06\\noutput Y where these thoughts are\\n1:08\\nlanguage sequences that somehow move the\\n1:10\\nproblem forward finally they talk about\\n1:12\\nChain of Thought with self-consistency\\n1:14\\nwhere essentially you're sampling from a\\n1:17\\nchain of thoughts and then picking the\\n1:18\\noutput that's the most frequent now the\\n1:20\\nauthors argue that there are two\\n1:21\\nproblems with existing approaches first\\n1:23\\nthere's a lack of local exploration of\\n1:26\\nthought of thought sequences secondly\\n1:28\\nthey argue that current approaches lack\\n1:30\\nthe incorporation of decision making and\\n1:32\\nplanning abilities nor do they have the\\n1:34\\nability to backtrack or look ahead to\\n1:36\\nhelp them evaluate different options now\\n1:38\\nwith this dot framework the author\\n1:40\\ndivides the problem into four stages you\\n1:43\\nhave the decomposition stage the\\n1:45\\ngeneration stage the evaluation stage\\n1:47\\nand the search stage now in the\\n1:50\\ndecomposition stage the properties of\\n1:52\\nthe problem are used to design and\\n1:54\\ndecompose thought steps where each\\n1:56\\nthought should be small enough so that\\n1:58\\nllms can generate a diverse range of\\n2:02\\npossible thought sequences from that\\n2:04\\noriginal thought as well as big enough\\n2:06\\nso the large language models can\\n2:08\\nevaluate them meaningfully in terms of\\n2:10\\nhow much they progress the problem\\nGeneration\\n2:12\\nforward in the generation stage you're\\n2:14\\nsampling from a distribution of possible\\n2:16\\nthoughts which works well When the\\n2:19\\nThought space is Rich or you use a\\n2:21\\nproposed prompt to propose a set of\\n2:22\\npossible thoughts from that state then\\n2:25\\nyou go into your evaluation stage where\\n2:27\\nyou're going to evaluate the quality of\\n2:29\\nthose thoughts in terms of how much they\\n2:31\\nget you closer to solving the problem\\n2:33\\nnow in the evaluation State you can\\n2:34\\neither value each state independently or\\n2:37\\nyou can use votes where the model is\\n2:39\\ngoing to be comparing States and picking\\n2:42\\nthe one that seems more likely to\\n2:44\\nsucceed and get you closer to a solution\\nEvaluation\\n2:46\\nand then for the search stage the\\n2:48\\nauthors argue that you can just plug in\\n2:50\\nplay different search to research\\n2:52\\nalgorithms and they chose the simplest\\n2:54\\nones so they chose a breakfast search\\n2:56\\nand that first search now for the\\n2:59\\nresults they did they use three data\\n3:01\\nsets sort of because they scrape much of\\n3:04\\nthe data to the checking for like\\n3:05\\nimprovised benchmarks or something like\\n3:07\\nthat so the first one is game 24 where\\n3:10\\nyou you're given four numbers and you\\n3:12\\ncan use mathematical operations to reach\\n3:15\\nthe number 24 and then they created a\\n3:17\\ncreative writing task where essentially\\n3:19\\nthe models are given four random\\n3:21\\nsentences and they have to form passage\\n3:23\\nof four paragraphs ending in those four\\n3:25\\nsentences and finally they also tested\\n3:27\\nin a set of crossword tasks here for\\n3:29\\nfigure two they're showing how the\\n3:31\\nproposed prompt is proposing a set of\\n3:33\\nthought steps from that initial input in\\n3:37\\nthis case 4 9 10 and 13. so then the\\n3:39\\nthought generation is just generating\\n3:41\\npossible options of what you can do with\\n3:43\\nthese numbers given the problem in the\\n3:45\\nfirst place in this case the problem of\\n3:47\\narranging a set of mathematical\\n3:48\\noperations around those numbers to\\n3:50\\nobtain the number 24 and then in the\\n3:52\\nvalue prompt the model is evaluating how\\n3:54\\nmuch each of those possible thoughts is\\n3:57\\ngetting us closer to obtaining the goal\\n4:00\\nwhich is the number 24 and then as for\\nResults\\n4:02\\nthe results they show that the tree of\\n4:03\\nthought approach had a 74 success in\\n4:06\\nthis case in this task in comparison to\\n4:08\\nthe other methods there I think there\\n4:09\\nare some things that we could consider\\n4:11\\nabout like how they use the other\\n4:13\\napproaches because they did some very\\n4:15\\nspecific type of prompts for the i o\\n4:18\\napproach the Chain of Thought and\\n4:19\\nself-consistency but I thought 74 in\\n4:22\\nthis game of 24 was a really cool result\\n4:23\\nnow for the creative writing desk they\\n4:26\\nhad this input where the would be like\\n4:29\\nto prompt the model to Riker in four\\n4:31\\nparagraphs ending those four sentences\\n4:33\\nand then the model would generate plans\\n4:35\\nof writing options and then the LM\\n4:38\\nitself would vote over those options to\\n4:41\\ndecide which plan was best and then they\\n4:43\\nshowed that the three or thought\\n4:44\\napproach had a higher coherence score\\n4:45\\nnow for the crossword puzzle they argue\\n4:48\\nthat they chose this task not not\\n4:49\\nbecause they wanted something that could\\n4:51\\nsolve it better than anything else\\n4:52\\nbecause they they said that there are\\n4:54\\nother natural language approaches a\\n4:56\\nnatural language processing approaches\\n4:57\\nthat could do this better but they\\n4:58\\nwanted to show that the more although\\n5:00\\nhad General capabilities in terms of\\n5:02\\nproblem that involves looking ahead\\n5:04\\nmaking plans deciding having to go back\\n5:07\\nEtc because the crossword puzzle is a\\n5:09\\nproblem that involves somewhat\\n5:10\\ndeliberate reasoning and looking ahead\\n5:13\\nbacktracking adapting and then they show\\n5:16\\nthat you know the the true thought\\n5:18\\napproach would propose thoughts based on\\n5:20\\nthe input to get the crossword puzzle\\n5:22\\nright and then use something like a\\n5:24\\nsearch algorithm like DFS in order to\\n5:27\\nsearch the space of possible options and\\n5:29\\nthen have a state evaluator classify\\n5:32\\neach of those options with something\\n5:34\\nlike sure impossible maybe in terms of\\n5:37\\nhow much it would progress the problem\\n5:39\\nforward and solve the crossword puzzle\\n5:40\\nthey claim that the tot approach\\n5:43\\nimproved significantly all the metrics\\n5:46\\nwith comparison to the other approaches\\n5:48\\nand they had a success rate of 60\\n5:50\\nfinally arguing that such an improvement\\n5:53\\nis not surprising given that I O and cot\\n5:55\\nlack mechanisms to try different Clues\\n5:57\\nand make changes to decisions or\\n5:59\\nbacktrack track which is something that\\n6:00\\nthis approach has now they finish up by\\n6:03\\ntalking about related work where they\\n6:04\\nargue that the three of thought\\n6:05\\nframework extends existing Planning\\n6:07\\nformulations by considering multiple\\n6:09\\npotentially feasible plans\\n6:11\\nsimultaneously at each problem-solving\\n6:13\\nstep and proceeding with the most\\n6:14\\npromising ones then they finish up by\\nRelated Work\\n6:17\\ntalking about how this approach is\\n6:19\\nmodular and flexible adaptable and\\n6:22\\ndespite the fact that you need more\\n6:24\\nresources like you know the cost of the\\n6:26\\ngpt4 API to run an approach like this uh\\n6:30\\nthey argue that their modularity allows\\n6:33\\nyou to customize the performance and\\n6:35\\ncost trade-off finally they also argue\\n6:37\\nthat tot improves interpretability which\\n6:39\\nis a big topic right now because of\\n6:40\\nalignment and they finish up talking\\n6:43\\nabout this idea of system one and system\\n6:44\\n2 where the contest of the three or\\n6:46\\nthought framework the associative system\\n6:48\\none can be beneficially augmented by\\n6:50\\nSystem 2 based on searching a tree of\\n6:52\\npossible paths to the solution of a\\n6:53\\nproblem so you had system one generate a\\n6:56\\nset of possible options and then system\\n6:58\\ntwo B this kind of search over that\\n7:01\\nspace to find one that better reaches\\n7:05\\nthe solution but yeah overall I really\\n7:06\\nlike this paper I think this approach is\\n7:08\\nquite interesting I think the the\\n7:10\\narguing for problem solving like framing\\n7:15\\nproblem solving as three search over a\\n7:18\\ncombinatorial space of possible states\\n7:21\\nthat will get you to the solution it's\\n7:22\\nnot exactly a new approach which is\\n7:24\\nsomething that the authors comment on\\n7:26\\nthe beginning that they inherited this\\n7:28\\napproach from Newell and that's\\n7:31\\nsomething that's not new I think one of\\n7:33\\nthe main key contributions of this paper\\n7:35\\nis this ability to self-evaluate through\\n7:38\\nthis combination of proposing possible\\n7:40\\noptions and then heuristically searching\\n7:43\\nthrough the best ones by evaluating\\n7:46\\nwhich ones get you closer to the\\n7:47\\nsolution and the way they do the\\n7:49\\nevaluation by using this by using a\\n7:52\\nmechanism where the model is reasoning\\n7:56\\nover potential States you're leveraging\\n7:59\\nthe yellow capabilities to not only\\n8:02\\ngenerate options to solve the problem\\n8:04\\nbut to evaluate their quality and then\\n8:07\\nmove the problem forward and I think\\n8:09\\nthis approach is really cool so thanks\\n8:11\\nfor watching don't forget to like And\\n8:12\\nsubscribe and see you next time cheers\"}, {'url': 'https://www.youtube.com/watch?v=2r37fvpGnMo', 'chapters_section': '📚Chapters\\n\\n0:02 - Introduction to Flashcards 🎴\\n0:06 - The Traditional Method of Creating Flashcards 📝\\n0:27 - Introduction to Prompt Engineering and Experiments 🚀\\n0:43 - The Process of Automating Flashcards with GPT 🤖\\n0:56 - Problem with the Traditional Approach 🚧\\n1:09 - How to Bypass these Issues using Decomposition 🛠️\\n1:26 - Extracting Information using GPT-4 🧠\\n1:50 - Why We Opted for ChatGPT in our Automation Process 💼\\n2:04 - Creation of Prompt Candidates for Testing 🧪\\n2:21 - Usage of Lan-chain to Create Prompt Templates 💡\\n2:31 - Evaluating the Quality of Outputs without Manual Checking 📈\\n2:55 - Visualization of Scores using a Bar Chart 📊\\n3:06 - Creating Anki Flashcards with the Best Prompt 🎯\\n3:17 - Creating Questions and Answers with ChatGPT 📝\\n3:33 - Displaying the Final Automated Process 💫\\n3:38 - Integrating Space Repetition Systems with Large Language Models 🔄\\n3:51 - Discussion on Metrics Evaluation and Potential Issues 🔍\\n4:00 - Conclusion and Future Directions 🌈', 'transcription': \"0:31\\nfind the best prompt to create optimal\\n0:34\\nInky flashcards use a combination of\\n0:36\\nchargeptee Lan ching and some simple\\n0:39\\nprompt engineering experiments to\\n0:41\\nautomate the process of creating Anki\\n0:43\\nflash cards now a while back I used\\n0:45\\nchurch PT to automate this process of\\n0:48\\ncreating flashcards with a simple crop\\n0:50\\nlike create Inky flashcards from this\\n0:52\\npiece of text now the problem with this\\n0:54\\napproach is that you might run into\\n0:56\\nproblems like a reliable questions and\\n0:58\\nanswers the Fletcher might not reflect\\n1:01\\nall the information present in the text\\n1:02\\nand things like repeated questions which\\n1:05\\nis something I heard from this person\\n1:07\\nthat commented on the channel to bypass\\n1:09\\nthese issues what I decided to do is use\\n1:11\\na concept from prompt engineering called\\n1:13\\ndecomposition where essentially to\\n1:15\\nfacilitate the process for the large\\n1:17\\nlanguage model you break down the\\n1:19\\nproblem into subtasks in this case I\\n1:22\\nbroke it down into two simple tasks\\n1:24\\nfirst I would like to extract all the\\n1:26\\nrelevant information from a giveaway\\n1:28\\npiece of text and then based on those\\n1:30\\nextracted facts create the questions and\\n1:33\\nanswers that I could then import to enki\\n1:35\\nI started by extracting all the facts\\n1:38\\nfrom a piece of text using gpt4 the most\\n1:41\\nadvanced large language model to date\\n1:43\\nfrom open AI which served to me as a\\n1:45\\nreference for a correct answer but my\\n1:47\\nultimate goal was to actually use chat\\n1:50\\nGPT for this automation since it's a\\n1:52\\nmuch faster model and much cheaper to\\n1:54\\nuse when we're using the actual API to\\n1:57\\ndo that I needed to make sure that the\\n1:59\\nanswers obtained with T would be\\n2:01\\nreliable so I also prompted gbt4 to\\n2:04\\ncreate a set of prompt candidates that I\\n2:07\\nwould then test using the actual\\n2:09\\nchargept API so I created a set of\\n2:11\\nprompt candidates and also added a few\\n2:13\\nmore with variations and I ended up with\\n2:15\\na set of 25 prompt candidates so then I\\n2:19\\nused Lane chain to create a set of\\n2:21\\nprompt templates that I would use to\\n2:23\\nprogrammatically query the charger TBI\\n2:27\\non a given text so after doing that I\\n2:30\\nhad the following problem to solve how\\n2:31\\ncan I evaluate the quality of the\\n2:33\\noutputs from church PT without manually\\n2:36\\nchecking each fact individually and\\n2:38\\ncomparing it to the original article so\\n2:40\\nmy solution to this was to use gpd4 as a\\n2:43\\nkind of a judge that would give a score\\n2:45\\nto each of the outputs of the thing\\n2:47\\nwhich hbt in order to rank them\\n2:50\\naccording to how well they extracted\\n2:53\\neffects from the text now based on these\\n2:55\\nscores I first generated a part chart to\\n2:57\\nshow all these scores from all the\\n2:59\\ndifferent pumps that I used and then I\\n3:01\\njust selected the best prompt given the\\n3:04\\nscores obtained using gpt4 now with this\\n3:06\\nbest prompt in mind my second task was\\n3:09\\nto actually create the Yankee flashcards\\n3:10\\nthe thing here is that creating the\\n3:13\\nactual questions and answers based on\\n3:14\\nfacts is a much simpler task for chech\\n3:17\\nPT than actually extracting the facts so\\n3:19\\nI came up with this prompt that actually\\n3:21\\nworked quite well and I created\\n3:23\\nsomething called a sequential chain\\n3:25\\nusing linking in order to concatenate\\n3:28\\nboth of these prompts into one automated\\n3:31\\nprocess that I could just run like I'm\\n3:33\\nshowing here after running I was quite\\n3:35\\nsatisfied with the results and I\\n3:36\\nconsider this task finished now I really\\n3:38\\nlike this approach where we can\\n3:40\\nintegrate space repetition systems with\\n3:42\\nlarge language models and I think\\n3:44\\nthere's a lot of potential to explore in\\n3:46\\nthese kinds of augmented Integrations\\n3:48\\nnow I wasn't 100 sure regarding how I\\n3:51\\nwas using gpt4 to evaluate the metrics\\n3:54\\nbecause that could view problems but\\n3:56\\nsince I didn't know exactly which was\\n3:58\\nthe best approach to take I decided to\\n4:00\\njust go with this one and see what\\n4:02\\nhappens for the future I intend to\\n4:04\\nexplore more these use cases of large\\n4:06\\nlanguage models as tools to automate\\n4:09\\nlower level processes if you like this\\n4:11\\nvideo don't forget to like And subscribe\\n4:13\\nand see you next time cheers\"}, {'url': 'https://www.youtube.com/watch?v=xKKnWcJo0vE', 'chapters_section': '📚 Chapters:\\n00:00 - Introduction\\n00:26 - Questioning transformer-based models\\n00:43 - Lack of ability to perform compositional reasoning\\n01:00 - Three tasks to assess capabilities\\n01:33 - Hypotheses for the paper\\n02:17 - Formulating compositional tasks as computation graphs\\n03:00 - Three tasks explained\\n03:13 - Zero shot accuracy and complexity metrics\\n03:48 - Poor generalization and partial solutions\\n04:03 - Ratio of correct nodes and decreasing correct answers\\n04:13 - Mathematical argument for reducing correct answers\\n04:39 - Conclusion and final thoughts', 'transcription': \"Introduction\\n0:00\\nSo we all know that large language models make silly mistakes from big multiplications\\n0:04\\nto hallucinating information.\\n0:05\\nNow, what the authors of this paper, Faith and Faith, Limits of Transformers in Compositionality\\n0:10\\nasked is whether or not transformer based models are actually uncovering rules for problem\\n0:16\\nsolving and exploring true reasoning paths when they're solving problems or are they\\n0:21\\njust doing some fancy pattern matching between input and output features.\\nQuestioning transformer-based models\\n0:26\\nThe authors argue that these silly trivial mistakes might indicate a lack of ability to\\n0:31\\nperform compositional reasoning in transformer based models, arguing that actually what\\n0:36\\ntransformers are doing are just reducing multi step compositional reasoning into some linearized\\nLack of ability to perform compositional reasoning\\n0:43\\ngraph matching to evaluate the compositional reasoning abilities of transformers.\\n0:47\\nThey set on three tasks to assess these capabilities.\\n0:50\\nA long form multiplication task, the Einstein puzzle and a dynamic programming problem.\\n0:55\\nNow these tasks were chosen based on the fact that they're centered around compositionality\\n0:59\\nand that they require the combination of reasoning operations with following a computational\\nThree tasks to assess capabilities\\n1:04\\npath to arrive at correct solution.\\n1:06\\nThey form two hypotheses for this paper.\\n1:08\\nThe first one is that transformers are actually reducing multi step compositional reasoning\\n1:12\\nto linearized graph matching, which is something that is taught to contrast with multi step\\n1:16\\nreasoning that learns to apply computational rules to reach correct answers.\\n1:21\\nThe second hypothesis error propagation, which is this idea that because transformers may\\n1:24\\nhave inherent limitations in solving high complexity compositional tasks that exhibit\\n1:28\\nnovel patterns, errors in the early stages may lead to substantial compounding errors\\nHypotheses for the paper\\n1:33\\nin these subsequent steps, preventing the models from reaching correct solutions.\\n1:37\\nTo investigate these hypotheses, they formulated compositional tasks as computation graphs,\\n1:41\\nbreaking down problem solving to sub modular functional steps, enabling structured measurements\\n1:46\\nof complexity and the verbalization of computational steps as input sequences to language models.\\n1:52\\nNow here in figure one, they show an example of a computation graph for a long form multiplication\\n1:56\\nalgorithm A with inputs seven and 49.\\n1:59\\nThey used graph metrics to quantify complexity.\\n2:01\\nThey used reasoning depth as a proxy for the maximum level of multi hop reasoning required\\n2:06\\nto solve the test, reasoning with as the maximum number of variables required to keep in parallel\\n2:13\\nduring the computation and average parallelism, which is the average with new computation\\nFormulating compositional tasks as computation graphs\\n2:17\\nthrough the graph.\\n2:18\\nThey used this approach of predicting surface patterns to relative information gain to assess\\n2:22\\nthe strategies for partially correct answers models used in incorrect responses.\\n2:27\\nYou had three tasks for this paper.\\n2:29\\nThe first one was multi-digit multiplication, where you have to manipulate symbols through\\n2:33\\nthe usage of procedural rules.\\n2:35\\nYou had the Einstein puzzle, which involves a list of houses and a list of attributes,\\n2:39\\nand you have to associate the attributes with the houses to the usage of clues and constraints.\\n2:45\\nAnd you had a dynamic programming problem that reads as given a sequence of integers,\\n2:50\\nfind a subsequence with the highest sum such the no true numbers of the subsequence are\\n2:53\\nadjacent in the original sequence.\\n2:55\\nNow here in figure two, they show that a zero shot accuracy of the models decreased as the\\n2:59\\nproblem sized increase in each of the tasks.\\nThree tasks explained\\n3:02\\nAnd they also showed that the average parallelism, which is one of the metrics they used to quantify\\n3:06\\ncomplexity was negatively correlated with accuracy of the models.\\n3:09\\nNow here in figure three, they showed that the GP3 fine tune exhaustively on task specific\\nZero shot accuracy and complexity metrics\\n3:13\\ndata only did well in examples present in the distribution.\\n3:17\\nNow one of the things the authors wanted to understand is that why is it that transformers\\n3:20\\ncan predict partially correct answers when the overall response is incorrect.\\n3:24\\nNow they hypothesized that that was the case because of peculiarities in the task distribution\\n3:29\\nthat allowed the model to guess partial answers without performing the multi-step reasoning\\n3:34\\nrequired to solve the task.\\n3:36\\nHere in figure four, they show examples of poor generalization for the GP3 in the puzzle\\n3:41\\nand DP tasks.\\n3:43\\nHere in figure five and six, the authors are showing how the ratio of correct nodes, meaning\\nPoor generalization and partial solutions\\n3:48\\ncorrect partial solutions provided by the models, decreases as the problem size increases\\n3:54\\nin terms of the number of graph layers, which is one metric they used to quantify complexity.\\n3:59\\nFinally, the authors present a mathematical argument showcasing why is the case that as\\nRatio of correct nodes and decreasing correct answers\\n4:03\\nthe problem size increases, the chance of the models to produce a correct answer decreases\\n4:08\\nexponentially.\\n4:09\\nThe authors conclude that transformers often solve multi-step compositional problems by collapsing\\nMathematical argument for reducing correct answers\\n4:13\\nthe depth of compositional operations via analogical pattern matching.\\n4:17\\nNow there are a lot of things that I liked about this paper, framing problem solving\\n4:20\\nas computation graphs, quantifying complexity to these graph metrics and showing strong\\n4:25\\nevidence of the limitations of transformers for compositional reasoning.\\n4:28\\nIf you like this video, don't forget to like and subscribe and see you next time.\\n4:32\\nCheers.\"}, {'url': 'https://www.youtube.com/watch?v=j9CHA3hgA10&t=615s', 'chapters_section': \"📚 Chapters:\\n00:00 - Introduction and Update\\n00:16 - O'Reilly Training Course Experience\\n00:48 - Research on Building LLM Agents\\n01:29 - Overview of Autonomous AI Agents Paper\\n02:10 - Examples of LLM-based Agents in Research\\n03:01 - Components of LLM Agents: Memory and Planning\\n04:40 - Action Space and Strategies in Planning\\n05:40 - Challenges in Benchmarking LLM Agents\\n06:31 - Upcoming Projects and Researcher Agents\\n06:54 - Interest in Lamma 2 and Open Source Models\\n07:22 - New Workspace and Design Plans\\n08:45 - Using AI for Rendering Floor Plans\\n09:51 - Future of AI in Architecture and Design\\n10:36 - Channel Goals and Future Content\\n11:18 - Upcoming Training Courses and Conclusion\", 'transcription': \"Introduction and Update\\n0:00\\nwhat's up guys so in this video I want\\n0:02\\nto do a quick update on some stuff that\\n0:04\\nI've been doing over the past two weeks\\n0:05\\nso I took a break from YouTube for a\\n0:07\\ncouple weeks to work on some personal\\n0:09\\nprojects and I want to share some of\\n0:11\\nthese projects with you specifically\\n0:13\\nI've done I did the like training course\\nO'Reilly Training Course Experience\\n0:16\\nfor O'Reilly my first course doing my\\n0:19\\nfirst live training course for them and\\n0:21\\nit went amazing the light training was\\n0:24\\nabout building text-based applications\\n0:26\\nwith the judge TPI and link chain and I\\n0:29\\nmean we had over 275 attendees live and\\n0:34\\nI mean I got hundreds of questions great\\n0:36\\nfeedback uh so it was just an amazing\\n0:39\\nexperience and we're gonna do another\\n0:41\\none in October on the 20th of October\\n0:45\\nand I'm Gonna Leave a link in the\\n0:46\\ndescription if you want to join that you\\nResearch on Building LLM Agents\\n0:48\\nget and besides that I've been getting\\n0:50\\nin building LOL agents I've been\\n0:53\\nresearching a lot about how to build llm\\n0:55\\nagents but not only that researching a\\n0:57\\nlot about the framers researching a lot\\n1:00\\nabout how to formalize building agents\\n1:04\\nand understanding what the problem is\\n1:07\\nwhen we're kind of like attaching large\\n1:10\\nlanguage models to tools and asking the\\n1:15\\nquestion okay how can I make this thing\\n1:18\\ndo useful things reliably right this is\\n1:21\\na really tough\\n1:22\\nit's a really tough thing to do and\\n1:27\\num recently during my vacation I read\\nOverview of Autonomous AI Agents Paper\\n1:29\\nthis 32 Pages paper on a survey for\\n1:32\\nautonomous AI agents again I'll leave a\\n1:34\\nlink in the description essentially this\\n1:37\\npaper was really cool because it kind of\\n1:39\\ngave me like a perfect overview of\\n1:42\\neverything that's going on in research\\n1:44\\nregarding building llm-based agents and\\n1:47\\nI want to share a few of these insights\\n1:50\\nwith you guys because essentially it's a\\n1:51\\nsurvey that just covers a bunch of stuff\\n1:54\\nfrom you know how to think about these\\n1:56\\nagents what research is happening right\\n1:59\\nnow or all these types of problems types\\n2:02\\nof applications of agents and stuff that\\n2:04\\npeople have been doing for example they\\n2:06\\ngive one example inside the paper of a\\nExamples of LLM-based Agents in Research\\n2:10\\nchemistry researcher agent that\\n2:12\\nessentially was able to synthesize a new\\n2:14\\nmolecule just using you know LMS plus\\n2:18\\nchemistry specific tools and I thought\\n2:21\\nthat was one of the coolest Keepers I\\n2:23\\nthink it's called Kim Crow I'll put a\\n2:25\\nlink in the description for both the\\n2:28\\nsurvey paper and this chemistry agent\\n2:30\\npaper I thought it was amazing right in\\n2:32\\nthis paper they talk about how you can\\n2:34\\nthink about are the lens in terms of\\n2:35\\nthese four basic components like a\\n2:38\\nprofile so that's essentially how do you\\n2:40\\nkind of label what the agent is it can\\n2:43\\nbe a researcher like a content creator\\n2:45\\nwhatever you want\\n2:47\\num and then they talk about okay so then\\n2:49\\nyou can manually describe what the agent\\n2:52\\nis give it a profile like you're a\\n2:54\\nresearcher your mathematician Etc or you\\n2:57\\ncan have vanilla live the fine with the\\n2:59\\nagent is or you can do something called\\nComponents of LLM Agents: Memory and Planning\\n3:01\\ndata set alignment so the second\\n3:03\\ncomponent it's is memory and you know\\n3:05\\nwhen you're interacting with llm agents\\n3:08\\nyou needed to have some sort of memory\\n3:10\\nsome sort of ability to hold information\\n3:12\\nfrom past experiences and they talk\\n3:16\\nabout what are the challenges there you\\n3:18\\nknow they talk about something like\\n3:19\\nunify and hybrid memory and the types of\\n3:22\\nformats of memory and the types of\\n3:24\\noperations that you have to do and they\\n3:27\\ngo into detail about that so that was\\n3:28\\ncool so then they go into planning so\\n3:32\\nthese agents need to plan out their\\n3:34\\nactions before right remember that most\\n3:36\\nof the like really high performing\\n3:38\\nagents to date are essentially based off\\n3:40\\nthis paper called react which is I'm\\n3:43\\ngoing to leave a link in the description\\n3:44\\nfor this paper as well and it's a\\n3:46\\nessentially it's a paper about you know\\n3:48\\nhow an llm uh how to like get the best\\n3:52\\nout of vanilla them by just having it\\n3:56\\num interpolated like just put\\n3:58\\nplanning in actions kind of like one\\n4:01\\nafter each other so that it gets the\\n4:04\\nbest performance by just having it\\n4:06\\nhaving it like okay so this is what I'm\\n4:07\\ngonna do and then it does and then it\\n4:09\\nrethinks and then it goes and does again\\n4:11\\nI mean this is a really cool fundamental\\n4:14\\npaper so planning is a key component of\\n4:16\\nthese agents and uh they they talk a lot\\n4:20\\nabout strategies in planning and types\\n4:22\\nof feedback that you can get during the\\n4:24\\nplanning phase like you can have just\\n4:26\\nthe model plan out its own tasks and\\n4:28\\nthen execute them or without any\\n4:31\\nfeedback or you can have feedback for\\n4:33\\nthe planning and the quality of the\\n4:34\\nplanning that the model will Define for\\n4:36\\nitself\\n4:37\\nand finally you have the action space\\nAction Space and Strategies in Planning\\n4:40\\nwhich is actually they go into like\\n4:43\\nbrilliant detail like this hierarchical\\n4:45\\nstructure almost of a lot of stuff\\n4:48\\nrelated to how agents take actions in\\n4:52\\nthe real world and essentially you know\\n4:54\\nyou can have actions based off the tools\\n4:56\\nthat the agents have right like\\n4:57\\nsearching the internet uh searching\\n5:00\\nGoogle running code Etc but you can also\\n5:03\\nhave actions based off the\\n5:05\\nself-knowledge of the agent\\n5:06\\nuh they go into a bunch of details about\\n5:09\\nlike different things to think about for\\n5:11\\nexample the action influence something\\n5:14\\nthey call Action influence which is like\\n5:16\\nto analyze how a nation of an agent uh\\n5:19\\nhas certain consequences certain you\\n5:21\\nknow from when I do this what happens\\n5:24\\nwhat are the consequences to my action\\n5:26\\nfor example if it does something on the\\n5:28\\nInternet what happens from there\\n5:31\\nstuff in that sense so that was kind of\\n5:33\\ninteresting and I mean this paper is\\n5:36\\nfull of cool stuff like for example\\n5:37\\nhaving tables for the benchmarks for\\nChallenges in Benchmarking LLM Agents\\n5:40\\nthese agents because essentially\\n5:41\\nbenchmarking llm agents today is like a\\n5:44\\nbig problem because you know in essence\\n5:47\\nyou have\\n5:49\\num you don't have any kind of structure\\n5:51\\nright now any kind of framework or\\n5:52\\nprotocol or how to evaluate the quality\\n5:55\\nof these agents and it's also very hard\\n5:57\\nbecause the tasks that these agents are\\n5:58\\nsupposed to do are very general so it's\\n6:00\\nvery subjective in a specific sense\\n6:03\\nhowever they do mention papers like\\n6:05\\nagent bench which is a paper that just\\n6:08\\ntry to defy this protocol and framework\\n6:11\\nfor how to properly evaluate llm agents\\n6:14\\nand it's kind of nice because they have\\n6:15\\na bunch of tasks and they're within\\n6:17\\nthose tasks you can see you know and you\\n6:20\\nhave many different ways of evaluating\\n6:22\\nuh Carl Black's things like reasoning\\n6:25\\nand problem solving capabilities\\n6:27\\nso this paper is quite interesting and\\n6:29\\nit got me thinking about doing a few\\nUpcoming Projects and Researcher Agents\\n6:31\\nprojects with these agents I already\\n6:33\\nstarted some and you guys are probably\\n6:35\\ngoing to see that in the very near\\n6:36\\nfuture essentially I want to build my\\n6:39\\nown researcher agents and you know the\\n6:41\\ndifferent types of Agents related to\\n6:43\\ndifferent things\\n6:44\\nand I think that would be a really cool\\n6:47\\nproject to have in the channel\\n6:49\\nand the last thing was I've been getting\\n6:51\\ninterested in La Machu which is the\\nInterest in Lamma 2 and Open Source Models\\n6:54\\nrecently released open source model by\\n6:57\\nmeta where essentially you have a model\\n7:00\\nthat you know Rivals Church B team\\n7:02\\nperformance but it's completely open\\n7:04\\nsource and you can download it to your\\n7:07\\ncomputer and essentially you have a\\n7:09\\nlarge language model that can do\\n7:10\\nwhatever set up a server in my new\\n7:12\\nworkspace which is actually a great\\n7:15\\nsegue to my next topic of discussion\\n7:18\\nwhich is I've been over the last month\\nNew Workspace and Design Plans\\n7:22\\nand a half designing my new workspace I\\n7:26\\nkind of like I'm gonna assure you guys a\\n7:28\\nphoto of what the space looks like right\\n7:30\\nnow but it's actually just I have a\\n7:31\\nsmall office where I do the videos and\\n7:34\\nwhere I work and I want to change the\\n7:36\\nspace to make it like not only better\\n7:39\\nfor making YouTube videos but better to\\n7:41\\nwork better to build stuff I've been\\n7:43\\ngetting into electronics and you know\\n7:45\\ndoing some kind of like you know maker\\n7:48\\ntype projects so when I have a little\\n7:49\\nworkshop inside my workspace\\n7:51\\nand this project has been really\\n7:54\\ninteresting because it got me kind of\\n7:56\\nsketching a lot I've been trying to\\n7:57\\nlearn how to draw for a while now and\\n8:00\\nI've been drawing kind of like I drew\\n8:02\\nout some designs for like the floor plan\\n8:04\\nof the workspace and it was really cool\\n8:07\\nbecause I've learned a lot about how\\n8:10\\npeople in architecture designed their uh\\n8:14\\nyou know do sketches for different\\n8:15\\narchitectural designs and from that I\\n8:20\\nkind of like evolved to like more you\\n8:22\\nknow slightly more formal floor Blends I\\n8:26\\nmean not really not really formal\\n8:28\\nbecause they they don't have like\\n8:30\\nspecific measurements and stuff like\\n8:32\\nthat but they're a little bit more like\\n8:34\\nstructured and from that what I did was\\n8:37\\nI used a tool called relative Fusion\\n8:39\\nwhich I'll leave a link in the\\n8:41\\ndescription they're not sponsoring this\\n8:42\\nvideo by the way\\n8:43\\nand I really liked the video I really\\nUsing AI for Rendering Floor Plans\\n8:45\\nlike the tool because essentially it\\n8:47\\nallowed me to render\\n8:49\\nmy sketches so I have these floor plans\\n8:53\\nand I rendered them with AI using they\\n8:56\\nuse control net they had these they have\\n8:59\\nthis really cool radio\\n9:01\\n2 if you guys don't know what radio is\\n9:03\\nit's a framework for like quickly\\n9:05\\nbuilding a machine learning apps and\\n9:07\\nit's kind of like streamlit but it's\\n9:09\\nlike you know uh simpler and um you know\\n9:12\\nruns in the browser just like\\n9:13\\nstreamlined it's pretty cool\\n9:16\\num and it was quite I had a great\\n9:19\\nexperience doing these uh renderings of\\n9:21\\nthe sketches because they gave me kind\\n9:23\\nof like a visual\\n9:24\\neven not 3D but kind of 3D visual of\\n9:28\\nwhat the workspace would look like\\n9:29\\naccording to different settings for my\\n9:32\\nfloor plan designs\\n9:34\\nand I think that was really cool I would\\n9:37\\nlike to do a video on like how to use AI\\n9:40\\nlike how to use open source AI\\n9:43\\nto do projects like that I think that's\\n9:45\\na really cool kind of Avenue that we're\\n9:48\\ngoing with these things now in mid\\n9:50\\nJourney For example now you can do\\nFuture of AI in Architecture and Design\\n9:51\\nsitting in the journey which is amazing\\n9:53\\nbecause you can you know generate\\n9:56\\nan image of like let's say uh like a an\\n9:59\\noffice or a couch or whatever and then\\n10:02\\nyou can select a small part within that\\n10:04\\nimage of an office or a couch or\\n10:06\\nwhatever and you can ask the model to\\n10:09\\niterate on variations just for that\\n10:12\\nspecific region that you selected\\n10:14\\nso there are a lot of tools for you know\\n10:17\\narchitecture and design that I think are\\n10:19\\ninteresting and I would love to see how\\n10:22\\nwe can have you know free open source\\n10:25\\nyou know available tools for the public\\n10:27\\nfor you guys to interact with and use it\\n10:29\\nin your own workflows I mean that's just\\n10:32\\nmy experience and I thought it was\\n10:33\\nreally cool like I like this idea of\\nChannel Goals and Future Content\\n10:36\\nlooking at how people learn and how\\n10:38\\nmachines learn and just try to find the\\n10:41\\nintersections when we can build tools\\n10:44\\nthat you know improve and augment Our\\n10:46\\nLives I know it sounds kind of like\\n10:48\\ncliche and stuff but this is really the\\n10:50\\ngoal of the Shadow and I want to keep\\n10:53\\ndoing videos where I you know maybe in\\n10:55\\nthe future I won't even do podcasts I'll\\n10:57\\ntalk to people because my goal is to\\n11:00\\nhave a place where you can go and it has\\n11:02\\nlike the latest tools that you can use\\n11:04\\nto build something that helps you do\\n11:07\\nresearch it helps you do learning\\n11:10\\nand that's pretty much it this pretty\\n11:12\\nmuch covers what I've been doing lately\\n11:14\\nuh I'm working on some proposals for\\nUpcoming Training Courses and Conclusion\\n11:18\\nsome new like training courses that I\\n11:20\\nwant to do in the future so I'll keep\\n11:22\\nyou guys updated all that all right\\n11:24\\nthank you guys for watching don't forget\\n11:26\\nto like And subscribe and see you next\\n11:28\\ntime cheers\"}, {'url': 'https://www.youtube.com/watch?v=W8QwNXbO5dk', 'chapters_section': '📚 Chapters:\\n\\n00:00 - Intro\\n00:10 - Pope is coming to Lisbon!\\n00:35 - The idea\\n00:50 - Explaining the automation\\n01:09 - A walkthrough the code\\n01:36 - The final product\\n02:26 - Like and subscribe! ', 'transcription': \"Intro\\n0:00\\nall right so I live in Lisbon so what I\\n0:03\\ndecided to do was I live in Lisbon which\\n0:06\\nis not exactly a huge town\\nPope is coming to Lisbon!\\n0:10\\nokay so I live in Lisbon which is not\\n0:12\\nexactly a super big city it has\\n0:14\\napproximately three million inhabitants\\n0:16\\nand the pope is coming to visit on this\\n0:18\\nfrom the 2nd of August to the 6th and\\n0:21\\nthat enlisted an increase in about a\\n0:24\\nmillion people who are going to be\\n0:26\\ncoming to Lisbon to see the pope which\\n0:30\\nfor me kind of ignited a bell of oh no\\n0:33\\nthere's going to be a lot of people\\nThe idea\\n0:35\\neverywhere so the programmer in me\\n0:37\\nlooked at that and thought okay so how\\n0:40\\ncan we automate staying away from the\\n0:44\\nhot spots where you know the pope might\\n0:46\\nvisit so there might be like a bunch of\\n0:48\\npeople there I decided to do a little\\nExplaining the automation\\n0:51\\npython automation that essentially helps\\n0:54\\nme keep track of\\n0:56\\nwhere the book is going to be and split\\n0:59\\nit by days so that I know the you know\\n1:02\\nareas to avoid when I'm walking around\\n1:05\\nLisbon or you know going to town so what\\n1:07\\nI did was I copied all the itinerary\\nA walkthrough the code\\n1:10\\nfrom the pope from the official press\\n1:11\\nrelease and then I threw that into\\n1:14\\ncharge BT to get some code to structure\\n1:17\\nthat into a panda's data frame I didn't\\n1:19\\ndid a little bit of cleaning and I\\n1:21\\ncertified that all the locations that I\\n1:23\\nobtained for all the dates and the\\n1:25\\ntimestamps were correct and associated\\n1:28\\nwith the correct date and then what I\\n1:30\\ndid is I used the geopy package to get\\n1:34\\nthe coordinates the latitude and\\nThe final product\\n1:36\\nlongitude coordinates for each of those\\n1:37\\nlocations separated by the bay and the\\n1:40\\ntime when the pope is going to be\\n1:42\\nvisiting those locations and then I\\n1:44\\ngenerated this really cool interactive\\n1:46\\nheat map of the city of Lisbon where you\\n1:49\\ncan see the spots where the pope is\\n1:51\\ngoing to be and I have this very simple\\n1:53\\ninteractive interface where I can click\\n1:55\\non a date and then see the spots that\\n1:57\\nthe pope is going to visit that day as\\n2:00\\nwell as you know little lines connecting\\n2:02\\nthose spots so that I have enough like a\\n2:04\\nsmall awareness of the region you know a\\n2:07\\ntriangulation of the region where the\\n2:09\\npope is going to be so I know which\\n2:11\\nplaces to avoid you know whatever which\\n2:14\\nday of the week when I'm going around\\n2:16\\ntown I wanted to have this kind of like\\n2:18\\nholistic view so this ended up being a\\n2:21\\nvery cool project to do and that's it if\\n2:23\\nyou like the video don't forget to like\\n2:25\\nAnd subscribe and see you next time\\nLike and subscribe! :)\"}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"yt_chapters_dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Print the data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have somewhat organized the data we'll use for the fine tunning, we now have to put everything in the neceessary format so that we can actually train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that all we have to do is generate a .json file that holds the information we want to fine tune the model with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our system prompt will be:\n",
    "\n",
    "- You are a helpful assistant\n",
    "\n",
    "The user prompt will always have the following structure:\n",
    "\n",
    "'''\n",
    "Given this Youtube video transcript: <transcript> I want you to create a chapters section for this Youtube video with the following format:\n",
    "\n",
    "\"\"\"\n",
    "<📚 Chapters:\\n>\n",
    "<double digit:time stamp> - <Concise phrase of a major part of the video>\n",
    "\n",
    "For example:\n",
    "\n",
    "\"\"\"📚 Chapters:\n",
    "00:05 - Introducing the concept of Neural Networks\n",
    "02:30 - Discussing activation functions\n",
    "etc...\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "\n",
    "The assistant output will always be the corresponding chapters section for that particular video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to read the original JSON file\n",
    "def read_original_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Function to generate the new JSON structure\n",
    "def generate_new_json_structure(original_data):\n",
    "    new_data = {\"messages\": []}\n",
    "    \n",
    "    # Add a system message\n",
    "    system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "    new_data[\"messages\"].append(system_message)\n",
    "    \n",
    "    for entry in original_data:\n",
    "        url = entry[\"url\"]\n",
    "        chapters = entry[\"chapters_section\"]\n",
    "        transcription = entry[\"transcription\"]\n",
    "        \n",
    "        # Add user message\n",
    "        user_message_content = f\"Given this Youtube video transcript: {transcription} I want you to create a chapters section for this Youtube video with the following format:\\n\"\n",
    "        user_message_content += \"📚 Chapters:\\n\"\n",
    "        user_message_content += \"<double digit:time stamp> - <Concise phrase of a major part of the video>\"\n",
    "        user_message = {\"role\": \"user\", \"content\": user_message_content}\n",
    "        new_data[\"messages\"].append(user_message)\n",
    "        \n",
    "        # Add assistant message\n",
    "        assistant_message_content = chapters  # Assuming chapters are already formatted as desired\n",
    "        assistant_message = {\"role\": \"assistant\", \"content\": assistant_message_content}\n",
    "        new_data[\"messages\"].append(assistant_message)\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "# Read the original JSON file\n",
    "original_data = read_original_json(\"./yt_chapters_dataset.json\")\n",
    "\n",
    "# Generate the new JSON structure\n",
    "new_data = generate_new_json_structure(original_data)\n",
    "\n",
    "# Write the new JSON structure to a file\n",
    "with open(\"./new_combined_data.json\", \"w\") as f:\n",
    "    json.dump(new_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to read the original JSON file\n",
    "def read_original_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Function to generate the new JSON structure\n",
    "def generate_new_json_structure(original_data):\n",
    "    new_data_list = []\n",
    "    \n",
    "    for entry in original_data:\n",
    "        new_data = {\"messages\": []}\n",
    "        \n",
    "        # Add a system message\n",
    "        system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "        new_data[\"messages\"].append(system_message)\n",
    "        \n",
    "        url = entry[\"url\"]\n",
    "        chapters = entry[\"chapters_section\"]\n",
    "        transcription = entry[\"transcription\"]\n",
    "        \n",
    "        # Add user message\n",
    "        user_message_content = f\"Given this Youtube video transcript: {transcription} I want you to create a chapters section for this Youtube video with the following format:\\n\"\n",
    "        user_message_content += \"📚 Chapters:\\n\"\n",
    "        user_message_content += \"<double digit:time stamp> - <Concise phrase of a major part of the video>\"\n",
    "        user_message = {\"role\": \"user\", \"content\": user_message_content}\n",
    "        new_data[\"messages\"].append(user_message)\n",
    "        \n",
    "        # Add assistant message\n",
    "        assistant_message_content = chapters  # Assuming chapters are already formatted as desired\n",
    "        assistant_message = {\"role\": \"assistant\", \"content\": assistant_message_content}\n",
    "        new_data[\"messages\"].append(assistant_message)\n",
    "        \n",
    "        new_data_list.append(new_data)\n",
    "        \n",
    "    return new_data_list\n",
    "\n",
    "# Read the original JSON file\n",
    "original_data = read_original_json(\"./yt_chapters_dataset.json\")\n",
    "\n",
    "# Generate the new JSON structure\n",
    "new_data_list = generate_new_json_structure(original_data)\n",
    "\n",
    "# Write the new JSON structure to a .jsonl file\n",
    "with open(\"./dataset_fine_tunning.jsonl\", \"w\") as f:\n",
    "    for item in new_data_list:\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, let's check the potential costs of this fine tunning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://cookbook.openai.com/examples/chat_finetuning_data_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 11\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a helpful assistant.'}\n",
      "{'role': 'user', 'content': \"Given this Youtube video transcript: 0:00\\nIn this video, you're going to learn how to create Anki flashcards using ChatGPT.\\n0:05\\nOkay, so we're going to start by importing our dependencies. So essentially, I'm going to import\\n0:09\\nOS to access the OpenAI API key in my environment as my environment variable. I'm going to import\\n0:15\\nOpenAI clipboard to get the content from my clipboard. And then Json to save stuff as a Json.\\n0:24\\nHere's what we're going to do. First, we're going to set up an OpenAI API key.\\n0:29\\nThen we're going to get the content from my clipboard. So here I'm getting the content\\n0:35\\nfrom my clipboard. And the idea is I'm going to copy some text from whatever from a paper,\\n0:41\\nfrom a website, etc. And I'm going to run the script. And that's it. I'm going to copy some text,\\n0:47\\nrun the script. And when I run the script, it's going to access the content on my clipboard.\\n0:51\\nAnd from there is going to create the Enki flashcards by sending a request to the ChatGPT\\n0:57\\nAPI. So let's create a conversation with the ChatGPT. I'm going to set up the role for the\\n1:06\\nsystem as a helpful assistant. Before I had set up this as your helpful assistant that creates\\n1:12\\nAnki flashcards. But I found that it had just because of this little change in the system prompt,\\n1:18\\nit had problem creating Enki flashcards in the format that I wanted. And then I'm going to set\\n1:25\\nup the message for the user. And the message for the user is going to be something like create\\n1:29\\nEnki flashcards with the following text using a format like question answer, which is the format\\n1:34\\nthat's more easily importable to Enki. And I'm going to give it the clipboard content. And that's\\n1:42\\nit. That's the message. And now I can just set up the request to the API. So I'm just sending a\\n1:49\\nrequest to the check completion class, access the create method, and then saying, I'm specifying the\\n1:55\\nmodel to GPT for I'm specifying the messages that I just defined in this variable here, that are\\n2:01\\nhave the system message and the user message, the system message describing the behavior that I\\n2:06\\nexpect, and the user message describing what I want specifically, right, that's my actual prompt.\\n2:12\\nAnd then I'm going to set up the temperature to 0.7, which is like traditionally is a good number\\n2:17\\nto set for these kinds of things. I'm going to set up the max tokens. And I'm going to create,\\n2:24\\nI'm going to save the answer from the model to a variable called generate flashcards.\\n2:30\\nAnd from there, I'm going to save them to a file. And that's pretty much it. I actually\\n2:37\\ndidn't use Jason. So let's just remove this. And now let's test this out. So what we're going to do\\n2:43\\nis I'm going to have I have a paper here. And I'm just going to copy the contents from let's say,\\n2:53\\nthe beginning of this paper, let's say this was a paper that I was studying and I already read. And\\n2:58\\nnow I want to kind of like, okay, so I want the some information that's here, I want to save it.\\n3:03\\nBut I don't want to be creating each Enki flashcard, right? So I copied. So now let's check that I\\n3:09\\ncopied. Perfect. It's in my clipboard. So now I can just come here and run the script. So I'm\\n3:15\\ngoing to run flashcards, check it to be perfect. And now let's see the magic happen. So I run the\\n3:23\\nscript, it takes like a few seconds to finish sending the request and getting the response.\\n3:30\\nAll right, perfect. So now I finished running. So I can access that file. And as you can see,\\n3:37\\nit created the flashcards. So what is camel camel stands for communicative agents for mind\\n3:43\\nexploration. What is the goal of camel? What is role playing framework? What is inception prompting?\\n3:50\\nAll useful information that was present in the text. Now I can go to Enki.\\n3:55\\nSo I can go to Enki right here. And I can import those flashcards directly. You see, it created\\n4:03\\nthe flashcards front, back, everything is correct. And I can just click import. And now I have the\\n4:09\\nflashcards to train to access and etc. So yeah, this approach is really cool. I really like this.\\n4:16\\nI like I really like doing this because you know, you can also do it directly with the\\n4:22\\ncharge PT demo or the charge PT website, which also works really well. But having it as a script\\n4:29\\nthat copies, I can just, you know, stop not think about it too much and just have it on my clipboard\\n4:36\\nand then just run the script. And I can automate the running of the script. So I can just do it\\n4:41\\nwith a few commands. And I have a bunch of Enki flashcards that usually would have taken me\\n4:46\\nquite a few minutes to create. So if you like this video, don't forget to like and subscribe\\n4:52\\nand see you next time. Cheers. I want you to create a chapters section for this Youtube video with the following format:\\n📚 Chapters:\\n<double digit:time stamp> - <Concise phrase of a major part of the video>\"}\n",
      "{'role': 'assistant', 'content': '📚 Chapters:\\n00:00 - Introduction \\n00:01 - Creating Anki Flashcards with ChatGPT\\n00:06 - Importing Dependencies\\n00:14 - Setting Up OpenAI API Key\\n00:29 - Getting Content from Clipboard\\n00:46 - Running the Script\\n00:57 - Creating Conversation with ChatGPT\\n01:05 - Setting Up System Role\\n01:24 - Setting Up User Message\\n01:38 - Formatting for Anki Import\\n01:46 - Sending Request to API\\n02:35 - Writing the flashcards to file\\n02:50 - Creating Anki flashcards\\n03:58 - Importing flashcards to Anki'}\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./dataset_fine_tunning.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now lets estimate some token counts and costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be checkinf for this validation checklist as described in the [tutorial suggested by the OpenAI docs](https://cookbook.openai.com/examples/chat_finetuning_data_prep#:~:text=Format%20validation,for%20easier%20debugging.).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helpful utilities\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data warnings and token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 946, 3979\n",
      "mean / median: 2247.5454545454545, 1926.0\n",
      "p5 / p95: 1480.0, 3098.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 63, 282\n",
      "mean / median: 143.1818181818182, 130.0\n",
      "p5 / p95: 91.0, 240.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output obtained, all of our examples are under the context length, which means they won't have to be truncated which is great news! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost estimation\n",
    "\n",
    "Let's estimate the cost of this fine tunning based on the number of tokens in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~24723 tokens that will be charged for during training\n",
      "By default, you'll train for 9 epochs on this dataset\n",
      "By default, you'll be charged for ~222507 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, as we can see it seems like this fine tunning is going to cost a bit, given that if I check on the [pricing page](https://openai.com/pricing) for the cost associated with this amount of tokens:\n",
    "\n",
    "![](2023-10-16-17-22-32.png)\n",
    "\n",
    "\n",
    "Let's write a little function to calculate these costs automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_for_fine_tunning(token_count):\n",
    "    return 0.0080*token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.780056"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_cost_for_fine_tunning(222.507)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so it seems that the amount will be 1.78 dollars which seems reasonable, although we should take into account the fact that we'll be charged more for using this fine tuned model as well, and we only used 10 examples for this use case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets upload our newly generated dataset file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that part, lets write a simply Python script to upload the file, we'll use this snippet from the [OpenAI API docs](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset#:~:text=import%20os%20import%20openai%20openai.api_key%20%3D%20os.getenv(%22openai_api_key%22)%20openai.file.create(%20file%3Dopen(%22mydata.jsonl%22%2C%20%22rb%22)%2C%20purpose%3D'fine-tune'%20))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<File file id=file-kXlvspUQRKFUWcrW5pgzy0PW at 0x16b9e68b0> JSON: {\n",
       "  \"object\": \"file\",\n",
       "  \"id\": \"file-kXlvspUQRKFUWcrW5pgzy0PW\",\n",
       "  \"purpose\": \"fine-tune\",\n",
       "  \"filename\": \"file\",\n",
       "  \"bytes\": 91665,\n",
       "  \"created_at\": 1697473593,\n",
       "  \"status\": \"uploaded\",\n",
       "  \"status_details\": null\n",
       "}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.File.create(\n",
    "  file=open(\"./dataset_fine_tunning.jsonl\", \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, we create our fine-tuned model by running this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-0G57koxcbLk7c6Z4maGRVPEd at 0x2bc475630> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-0G57koxcbLk7c6Z4maGRVPEd\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1697473689,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-gpLJbCQWtORw077QTyeX1IVP\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"validating_files\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-kXlvspUQRKFUWcrW5pgzy0PW\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": \"auto\"\n",
       "  },\n",
       "  \"trained_tokens\": null,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.FineTuningJob.create(training_file=\"file-kXlvspUQRKFUWcrW5pgzy0PW\", model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the `training_file` parameter correspond to the file id generated when running the previous snippet. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wait for an email confirmation that will let us know when the training is done. Which given the size of this job, should be pretty quckly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat thing is that you can promatically query the status of your jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-0G57koxcbLk7c6Z4maGRVPEd at 0x16aad3db0> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-0G57koxcbLk7c6Z4maGRVPEd\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1697473689,\n",
       "  \"finished_at\": 1697474050,\n",
       "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:personal::8AKpfptp\",\n",
       "  \"organization_id\": \"org-gpLJbCQWtORw077QTyeX1IVP\",\n",
       "  \"result_files\": [\n",
       "    \"file-DZhopqkDrs5xSzQsrzA4zbjN\"\n",
       "  ],\n",
       "  \"status\": \"succeeded\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-kXlvspUQRKFUWcrW5pgzy0PW\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 9\n",
       "  },\n",
       "  \"trained_tokens\": 222309,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List 10 fine-tuning jobs\n",
    "openai.FineTuningJob.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "openai.FineTuningJob.retrieve(\"ftjob-0G57koxcbLk7c6Z4maGRVPEd\")\n",
    "\n",
    "# Cancel a job\n",
    "#openai.FineTuningJob.cancel(\"file-kXlvspUQRKFUWcrW5pgzy0PW\")\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "#openai.FineTuningJob.list_events(id=\"file-kXlvspUQRKFUWcrW5pgzy0PW\", limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-10-16-17-39-30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go to the platform to check out our fine tuned model or run inference from it using the API! Let's look at how to run our fine tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN the platform we see:\n",
    "\n",
    "![](2023-10-16-17-40-30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can choose that model and see how it performs for a different new video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying the model:\n",
    "\n",
    "![](2023-10-16-17-49-09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response:\n",
    "\n",
    "![](2023-10-16-17-49-24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks pretty good! :) Let's compare with the output of regular ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-10-16-17-50-32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine tuned model is definitely more thourough than the regular ChatGPT model so I would call this a nice success! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run inference with our fine tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": \"Chapters:\\n\\n00:00 - Introduction and setting up the OCR search project\\n00:25 - Initial project setup requirements and Python package installations\\n00:38 - Importing necessary libraries\\n00:45 - Setting up the functions to scan images for text and loop over images in a folder\\n01:04 - Setting up EasyOCR reader for text detection\\n01:16 - Coding the OCR scan function\\n02:05 - Setting up the function to process recognized text\\n02:25 - Introduction of a template for the search images function\\n02:37 - Initial testing of the OCR scan function\\n03:22 - Starting the development of the search images function\\n04:13 - Explanation of how the search images function works\\n06:05 - Testing the search images function\\n07:32 - Transitioning the code to an application callable from terminal\\n08:09 - Developing a CLI tool to enable OCR search from terminal\\n10:00 - Parsing the arguments for the CLI tool\\n11:36 - Coding the logic to handle directory search and keyword recognition\\n12:31 - Handling single image OCR search in the CLI tool\\n14:33 - Running and testing the application in the terminal\\n15:07 - Conclusion and review of the project.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Given this Youtube video transcript:\n",
    "'''Introduction and setting up the OCR search project\n",
    "0:00\n",
    "that just put this right here there yep what's  up guys welcome back to the channel in this video  \n",
    "0:05\n",
    "we're gonna learn how to do OCR search over  text inside images in your local files in a  \n",
    "0:11\n",
    "local folder so I already have my Jupiter notebook  setup over here and essentially what you're going  \n",
    "0:16\n",
    "to need is OCR search ask your foot python equal  3.10 and you're gonna You're Gonna Wanna install  \n",
    "Initial project setup requirements and Python package installations\n",
    "0:25\n",
    "Ezio CR and torch and that's what you need to get  set up I'm already set up so I'm already ready to  \n",
    "0:31\n",
    "go I'm gonna import OS typing I'm gonna import  a list from the type checking so I will do type  \n",
    "Importing necessary libraries\n",
    "0:38\n",
    "checking on the functions and p0shar to do to have  a lifting of the user then we're going to set up  \n",
    "Setting up the functions to scan images for text and loop over images in a folder\n",
    "0:45\n",
    "two functions one to scan the um images for text  and the second one to Loop over the image inside  \n",
    "0:52\n",
    "of a folder so and before we do that of course we  gotta set up the redial CR so we're gonna set up  \n",
    "0:58\n",
    "reader equal to easusar.reader we're going to give  it the English language and here I'm setting up  \n",
    "Setting up EasyOCR reader for text detection\n",
    "1:04\n",
    "GPU to true because I have a GPU machine but if  you don't just set this to post and now I'm going  \n",
    "1:10\n",
    "to run this and now what we need is those are  those two functions so we're gonna need the OCR  \n",
    "Coding the OCR scan function\n",
    "1:16\n",
    "scan function that runs the OCR in an image and  another one that Loops over images in a folder  \n",
    "1:23\n",
    "so we're gonna set up Dual Star scan it's gonna  take in the image path as the keyword argument as  \n",
    "1:29\n",
    "a as an argument that's going to be a string and  we're going to return a string and the string is  \n",
    "1:34\n",
    "going to be obviously the text that was detected  inside the image and now we're gonna set up  \n",
    "1:41\n",
    "the function so essentially here we're calling  the reader and the read text method inside the  \n",
    "1:47\n",
    "reader that we instantiated over there the reader  object and we're going to give it the image pass  \n",
    "1:55\n",
    "and now we're gonna say the recognized text is  just a join of all the element texts inside the  \n",
    "Setting up the function to process recognized text\n",
    "2:05\n",
    "image that are stored in the result variable and  now that we've done that we're going to return  \n",
    "2:11\n",
    "the regular guys text and we can do from here  is we're going to set up right now just a toy  \n",
    "2:19\n",
    "just a template boilerplate of the search images  so that we have it on our the skeleton over app  \n",
    "Introduction of a template for the search images function\n",
    "2:25\n",
    "and we're going to go back to this function later  to finish it off but now I want to test the OCR  \n",
    "2:30\n",
    "scan function that we just read that we just wrote  so I'm gonna come here I'm gonna set up the image  \n",
    "Initial testing of the OCR scan function\n",
    "2:37\n",
    "path and I have an image path I have an image  inside as a test there you guys can check out here  \n",
    "2:43\n",
    "it's just just a copy paste of some text that I  have in my machine and I'm gonna run the OCR scan  \n",
    "2:51\n",
    "on that image for you guys to have an idea I'm  going to just run it here and there you go super  \n",
    "Execution of the OCR scan on an image for demonstration\n",
    "3:00\n",
    "quick because I'm using the GPU and it detected  all the text inside of that image so that's  \n",
    "3:05\n",
    "perfect and now I'm gonna move this search images  function over here and now we can start setting  \n",
    "3:15\n",
    "this image up setting this function up so search  images essentially is going to take in a directory  \n",
    "Starting the development of the search images function\n",
    "3:22\n",
    "okay and that's going to be a strain and  a keyword which is going to be the thing  \n",
    "3:28\n",
    "that we're searching for when we are doing our  OCR search because remember the goal of this  \n",
    "3:33\n",
    "is not just to do Char over the uh text inside  images in local files in the local folder but  \n",
    "3:41\n",
    "to find a specific keyword that we're looking for  within the text that we find in those images so  \n",
    "3:47\n",
    "that's that can be a really useful tool and it's  really simple to set up with python and easier  \n",
    "3:52\n",
    "so this keyword is going to also be a string  and we're going to Output the output of this  \n",
    "3:59\n",
    "function is going to be a list of image paths that  contain the keyword that we're looking for okay so  \n",
    "4:08\n",
    "that's that's essentially that's essentially  our search images function so now we're going  \n",
    "Explanation of how the search images function works\n",
    "4:13\n",
    "to say okay so we're going to define a matching  images list right and let's get a store all the  \n",
    "4:18\n",
    "images that match our requirements meaning  contains the keyword that we're looking for  \n",
    "4:23\n",
    "and we're going to say update so forth root and  then directory and then files in OST I'll walk  \n",
    "4:31\n",
    "directory okay so inside of the stain so for  file in files what we're gonna do is we're gonna  \n",
    "4:41\n",
    "say if the file ends with uh that ends with and  now here I'm going to give a tuple with all the  \n",
    "4:51\n",
    "possible image extensions so wpng.jpg and Dot  J back with the knee and that's good for now  \n",
    "5:02\n",
    "and we're going to set up image best so join  gluten file perfect that's Gelco pilot already  \n",
    "5:08\n",
    "recognize that that's what I want then we're going  to give the text so let's say detect the text  \n",
    "5:15\n",
    "detective text is equal to a shark scan image  path and then if the keyword dot lower in record  \n",
    "5:26\n",
    "in the detected text exactly detected text the  lower so now I want to make everything lower so  \n",
    "5:33\n",
    "it's easier to search for we don't want to have  issues where there's some Capital um some some  \n",
    "5:40\n",
    "letter characters that are capitalized and then  from that reason we don't find keyword that we're  \n",
    "5:46\n",
    "looking for and now I can say all right so if that  happens if we find our keyword we're gonna pin  \n",
    "5:54\n",
    "that path to our matching images list finally at  the end we're going to return the matching images  \n",
    "6:05\n",
    "and that's going to be what we're looking  for so now to test the search images  \n",
    "Testing the search images function\n",
    "6:10\n",
    "what we're going to do is I'm gonna give the  current folder as uh the reference and the keyword  \n",
    "6:19\n",
    "I'm going to set up keyword let's take a look at  this one so this one image test if we came here  \n",
    "6:30\n",
    "we'll see no one for Windows please the official  instructions here so this is some random text that  \n",
    "6:35\n",
    "I just copy pasted from my screen somewhere I  don't even know where so now let's look for the  \n",
    "6:40\n",
    "word note okay so that's the one that's written  inside this thing here just to see if it works  \n",
    "6:45\n",
    "so I'm going to say keyword equal note and what  I expect to happen is it should return a list  \n",
    "6:54\n",
    "containing the name image test.png as the response  so that's if that works if we run it and no it  \n",
    "7:01\n",
    "doesn't it says type list cannot be instantiated  use list instead and that's where where where list  \n",
    "7:10\n",
    "string that should be correct ah yeah  because yeah oh sorry that's my mistake  \n",
    "7:19\n",
    "I always my mistake I should have done this and  there we go we found it and we return it so this  \n",
    "7:27\n",
    "thing is working so what we're going to do is  we're going to transfer all of this code through  \n",
    "Transitioning the code to an application callable from terminal\n",
    "7:32\n",
    "an application that we can call from the terminal  so I'm going to open up another file here on JS  \n",
    "7:37\n",
    "code I'm going to say Lucia search dot pi and now  I'm going to put that on my right and I'm going  \n",
    "7:44\n",
    "to come back here and I'm going to come here to  my notebook I'm going to copy these two functions  \n",
    "7:50\n",
    "to my right uh yeah perfect we're going to come  here we're also going to copy this one yep and  \n",
    "7:59\n",
    "we're going to copy the setting up so this thing  and now we're going to copy the Imports obviously\n",
    "8:09\n",
    "and all we need now let's remove this and let's  move that and all we need now is a main function  \n",
    "Developing a CLI tool to enable OCR search from terminal\n",
    "8:20\n",
    "that's going to contain the CLI tool that we're  going to use so that we can call all this from  \n",
    "8:26\n",
    "the terminal okay so that's what we're going  to be doing so this one is defined a CLI tool  \n",
    "8:34\n",
    "that allows for OCR search for HC word over  uh images in a local folder or a single image  \n",
    "8:50\n",
    "that's what our main function is going to be  doing let's set up this thing here I'm going  \n",
    "8:56\n",
    "to set up this thing here so now with our main  function we're going to be doing I already have  \n",
    "9:00\n",
    "it here so I'm just going to copy One Lie by  line so that you guys can take a look and see  \n",
    "9:06\n",
    "what what was going to be happening so now we're  going to need arc bars so that we set up our cli2  \n",
    "9:13\n",
    "and let's write it up so we don't need this  part of the Jupiter notebook anymore okay  \n",
    "9:20\n",
    "so our main function is going to be so we're  going to set up our parser okay so we're gonna  \n",
    "9:25\n",
    "set up uh ARG parse dot argument parser and then  that's we're going to give it a description the  \n",
    "9:32\n",
    "description is going to be uh OCR search over  local images add an argument and then we're  \n",
    "9:42\n",
    "gonna say this is going to be our directory so  where we set up the directory so it's going to  \n",
    "9:47\n",
    "be called directory and the type obviously is  going to be a string and this is the directory\n",
    "10:00\n",
    "the images\n",
    "10:04\n",
    "perfect and now we're going to say  parser.add argument the first one  \n",
    "10:11\n",
    "here is going to be the keyword so KW and  then here we're gonna say minus keyword  \n",
    "10:22\n",
    "and we're gonna say type equals string  as well and we're gonna say help equal to\n",
    "Parsing the arguments for the CLI tool\n",
    "10:31\n",
    "uh this is the three word text we'll\n",
    "10:38\n",
    "look for perfect and now that we have that  we can add uh the option of a single image  \n",
    "10:48\n",
    "so we can say parser dot add argument minus I so  let me put it like this minus I and then we're  \n",
    "10:59\n",
    "gonna say image and then we're gonna say type  equal Str and then we're gonna say help equal\n",
    "11:09\n",
    "d image to scan the single image case to Scag  not the single image the scan is better yeah  \n",
    "11:22\n",
    "perfect now that we have these things we  can just get our arguments so we can say  \n",
    "11:27\n",
    "R is equal to parser dot uh parse args right so  now we can you know leverage RMS and then if the  \n",
    "Coding the logic to handle directory search and keyword recognition\n",
    "11:36\n",
    "directory is given then we're going to say well  matching images is equal to search homages uh  \n",
    "11:45\n",
    "and we're going to fit in the directory so we're  going to say uh args.directory R and rs.q word  \n",
    "11:55\n",
    "and we're gonna print so we're gonna say images  that console the keyword and we're gonna Loop over\n",
    "12:09\n",
    "the matching images and we're gonna print  that's called instead of calling image we're  \n",
    "12:16\n",
    "going to call it image name image path because  those are not actual loaded images but they're  \n",
    "12:24\n",
    "paths two images now in the case that we don't  Define the directory that means that we want to  \n",
    "Handling single image OCR search in the CLI tool\n",
    "12:31\n",
    "scan a single image so we say okay and what is the  detected text for that single image that's perfect  \n",
    "12:38\n",
    "and then that's correct our image contains the  image exactly so we say okay uh if that is inside  \n",
    "12:46\n",
    "we say detected image in the image and then we're  gonna say this this this and we're going to give d  \n",
    "12:54\n",
    "image and we're gonna say all right scan the image  and then we're not going to have a Passover the  \n",
    "13:02\n",
    "image so we're gonna say just the detected text  so see where the Titan the image so we're going  \n",
    "13:12\n",
    "to say yes and then we're gonna print the tactic  texts detected texts and I'm gonna just here  \n",
    "13:24\n",
    "and I'm gonna say this and this else keyword  not attacking the image and that is it okay  \n",
    "13:31\n",
    "now that we have that all we need to do is we're  going to set up our main function here and we're  \n",
    "13:36\n",
    "going to call it so now I can open near my  terminal and I can come here and say okay  \n",
    "13:42\n",
    "uh I'm going to run this function so it's  going to crawl let's try search we're going  \n",
    "13:47\n",
    "to do it over one image so let's say minus I it's  gonna be the image that we just tested over here  \n",
    "13:57\n",
    "and the keyword is going to be nopes and now we  run it let's see if it works keyword not detected  \n",
    "14:08\n",
    "in the image that is kind of weird so let's  see English test the PNG because it's note in  \n",
    "14:14\n",
    "our notes so that was my mistake so I'm going to  run that again with Note so now we run it keyword  \n",
    "14:22\n",
    "detected and then it prints the detected text as  expected perfect so now just to finish things off  \n",
    "14:28\n",
    "instead of the single image we're gonna give the  directory the director is going to be the current  \n",
    "Running and testing the application in the terminal\n",
    "14:33\n",
    "directory so that you know it's easier for us so  just we just test a simple example and instead  \n",
    "14:39\n",
    "of using the word node let's look for the word  windows so I'm just gonna I'm gonna say windows  \n",
    "14:47\n",
    "and let's see what happens we should return a list  with the path to image test.png and that's exactly  \n",
    "14:55\n",
    "what happens images that contain the keyword  and it gives me the path so all right guys so  \n",
    "15:01\n",
    "that's how you set up a simple OCR search app to  look for text over images in your local folder  \n",
    "Conclusion and review of the project.\n",
    "15:07\n",
    "if you guys like this video don't forget to  like And subscribe and see you next time cheers'''\n",
    "\n",
    "I want you to create a chapters section for this Youtube video with the following format: Chapters:\\n<double digit:time stamp> - <Concise phrase of a major part of the video>\n",
    "\"\"\"\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0613:personal::8AKpfptp\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4264\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "def get_num_tokens(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Calculates the number of tokens in a text prompt\"\"\"    \n",
    "\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    return len(enc.encode(prompt))\n",
    "\n",
    "\n",
    "# Open the file and read its contents\n",
    "with open('./chatgpt_fine_tunning_content.txt', 'r') as file:\n",
    "    contents = file.read()\n",
    "\n",
    "# Call the get_num_tokens() function on the contents\n",
    "num_tokens = get_num_tokens(contents)\n",
    "\n",
    "# Print the number of tokens\n",
    "print(num_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly_env",
   "language": "python",
   "name": "oreilly_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
