{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# loading from a .env file\n",
    "# load_dotenv(dotenv_path=\"/full/path/to/your/.env\")\n",
    "\n",
    "# or \n",
    "# if you're on google colab just uncomment below and replace with your openai api key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<your-openai-api-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's first understand how [OpenAI](https://openai.com/) the company behind ChatGPT, allows for these function call implementations in its API.\n",
    "\n",
    "OpenAI implemented a [function calling API](https://platform.openai.com/docs/guides/function-calling) which is a standard way to connect their models to outside tools like in the very simple example we did above.\n",
    "\n",
    "According to their [official documentation](https://platform.openai.com/docs/guides/function-calling#:~:text=The%20basic%20sequence,to%20the%20user.) the sequence of steps for function calling is as follows:\n",
    "\n",
    "<img src=\"./assets-resources/openai-function-calling-diagram.png\" width=300px;>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example taken from their official documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how our previous model with those three simple functions: `create_directory()`, `create_file()`, and `list_files()` would be implemented using OpenAI's function calling approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "\n",
    "def create_directory(directory_name):\n",
    "    \"\"\"Function that creates a directory given a directory name.\"\"\"\"\"\n",
    "    subprocess.run([\"mkdir\", directory_name])\n",
    "    return json.dumps({\"directory_name\": directory_name})\n",
    "\n",
    "\n",
    "tool_create_directory = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"create_directory\",\n",
    "        \"description\": \"Create a directory given a directory name.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"directory_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The name of the directory to create.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"directory_name\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "tools = [tool_create_directory]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-A3P5r1KyUO4wwGS92Jecu0m1YC6py', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The folder 'lucas-the-agent-master' has been successfully created.\", refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1725374807, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f33667828e', usage=CompletionUsage(completion_tokens=14, prompt_tokens=56, total_tokens=70))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def run_terminal_task():\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Create a folder called 'lucas-the-agent-master'.\"}]\n",
    "    tools = [tool_create_directory]  \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    # Step 2: check if the model wanted to call a function\n",
    "    \n",
    "    if tool_calls:\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"create_directory\": create_directory,\n",
    "        }\n",
    "        messages.append(response_message)\n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(\n",
    "                directory_name=function_args.get(\"directory_name\"),\n",
    "            )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        return second_response\n",
    "\n",
    "output = run_terminal_task()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The folder 'lucas-the-agent-master' has been successfully created.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36massets-resources/\u001b[m\u001b[m       \u001b[1m\u001b[36mimages/\u001b[m\u001b[m                 \u001b[1m\u001b[36mmy-app/\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mdocs/\u001b[m\u001b[m                   \u001b[1m\u001b[36mlucas-the-agent-master/\u001b[m\u001b[m \u001b[1m\u001b[36mpages/\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -d */"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We implemented openai function calling for creating directories! We could evolve this approach but let's stop for now.\n",
    "\n",
    "See more info on these examples from OpenAI's [official cookbook](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-chatgpt-apps",
   "language": "python",
   "name": "oreilly-chatgpt-apps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
