{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-chatgpt-course-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Building a Large Language Model (LLM) app involves several steps, from understanding your requirements to deploying the app. Here's a structured approach to guide you through the process:\\n\\n### 1. Define the Purpose and Scope\\n- **Identify the Use Case**: Determine what problem your app will solve. Examples include chatbots, content generation, sentiment analysis, etc.\\n- **Target Audience**: Understand who will use the app and tailor the features accordingly.\\n\\n### 2. Choose the Right LLM\\n- **Select a Pre-trained Model**: Options include OpenAI's GPT series, Google's BERT, Meta's LLaMA, etc. Consider the model's capabilities, licensing, and cost.\\n- **Model Size and Complexity**: Balance between model capabilities and computational resources.\\n\\n### 3. Set Up the Development Environment\\n- **Hardware Requirements**: Ensure you have adequate computational resources (CPU/GPU) depending on the model size.\\n- **Software Requirements**: Install necessary libraries and frameworks like TensorFlow, PyTorch, or Hugging Face Transformers.\\n\\n### 4. Fine-tune the Model (if necessary)\\n- **Data Collection**: Gather domain-specific data for fine-tuning.\\n- **Training**: Use machine learning frameworks to fine-tune the model on your dataset, adjusting hyperparameters as needed.\\n\\n### 5. Develop the Application\\n- **Backend**: Create APIs to interact with the LLM. Use frameworks like Flask, FastAPI, or Django.\\n- **Frontend**: Develop the user interface using web technologies (React, Angular, Vue) or mobile frameworks (Flutter, React Native).\\n\\n### 6. Integrate the LLM\\n- **API Integration**: If using a third-party LLM service, integrate their API.\\n- **Model Hosting**: Deploy your model on a server if you are hosting it yourself.\\n\\n### 7. Optimize for Performance\\n- **Latency Reduction**: Use techniques like model quantization or distillation to reduce response time.\\n- **Scalability**: Implement load balancing and consider cloud services like AWS, Google Cloud, or Azure for scaling.\\n\\n### 8. Testing\\n- **Functional Testing**: Ensure the app meets all functional requirements.\\n- **Performance Testing**: Evaluate the app's speed, scalability, and reliability.\\n- **User Testing**: Gather feedback from real users to identify and fix UX issues.\\n\\n### 9. Deployment\\n- **Deployment Platform**: Choose between cloud platforms, on-premises servers, or hybrid solutions.\\n- **Continuous Integration/Continuous Deployment (CI/CD)**: Set up pipelines for automatic testing and deployment.\\n\\n### 10. Monitor and Maintain\\n- **Logging and Monitoring**: Implement tools to track app performance and errors.\\n- **Regular Updates**: Update the model and app to incorporate new features and improvements.\\n\\n### 11. Address Ethical and Privacy Concerns\\n- **Data Privacy**: Ensure compliance with data protection regulations (e.g., GDPR).\\n- **Bias Mitigation**: Regularly audit your model for biases and take corrective actions.\\n\\n### Additional Tips\\n- **Community and Support**: Engage with developer communities for support and updates.\\n- **Documentation**: Maintain clear documentation for both users and developers.\\n\\nBy following these steps, you can systematically develop an LLM-based application that meets the needs of your users while leveraging the powerful capabilities of language models.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 686, 'prompt_tokens': 15, 'total_tokens': 701, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'stop', 'logprobs': None}, id='run-09482dd1-04a8-4320-a936-f5688ac6c4ec-0', usage_metadata={'input_tokens': 15, 'output_tokens': 686, 'total_tokens': 701, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "llm.invoke(\"How to build an LLM app?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The classic answer is: \"To get to the other side.\" It\\'s a well-known joke that plays on the simplicity of the setup, leading to an unexpectedly straightforward punchline.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 15, 'total_tokens': 51, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_703d4ff298', 'finish_reason': 'stop', 'logprobs': None}, id='run-df95c87b-f181-431d-baa7-2719a53e5777-0', usage_metadata={'input_tokens': 15, 'output_tokens': 36, 'total_tokens': 51, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Why did the chicken cross the street?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-chatgpt-apps",
   "language": "python",
   "name": "oreilly-chatgpt-apps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
