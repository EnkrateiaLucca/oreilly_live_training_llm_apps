{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-chatgpt-course-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Building an application that utilizes a Large Language Model (LLM) involves several steps. Here’s a high-level guide to help you get started:\\n\\n### 1. Define Your Use Case\\n- **Objective:** Clearly define the problem you aim to solve with the LLM. This could be anything from chatbots, content generation, translation, sentiment analysis, etc.\\n- **Audience:** Understand who will use the app and tailor the features to their needs.\\n\\n### 2. Choose the Right Model\\n- **Pre-Trained Models:** Decide whether to use a pre-trained model or fine-tune an existing one. Common options include OpenAI's GPT series, Google's BERT, Meta's LLaMA, etc.\\n- **Customization:** Consider if you need to fine-tune the model on domain-specific data for better performance.\\n\\n### 3. Set Up Environment\\n- **Cloud Providers:** Use services like AWS, Google Cloud, or Azure which offer infrastructure and possibly pre-trained models.\\n- **Local Setup:** Alternatively, set up a local environment with necessary libraries like TensorFlow or PyTorch.\\n\\n### 4. Integrate the Model\\n- **APIs:** Use available APIs for easy integration if you are using models like GPT-3 or GPT-4.\\n- **Libraries:** Utilize libraries like Hugging Face Transformers to load and use models programmatically.\\n\\n### 5. Build the Application\\n- **Frontend:** Develop a user interface using frameworks like React, Angular, or Vue.js.\\n- **Backend:** Implement the server-side logic using Node.js, Python (Flask, Django), or any preferred backend technology.\\n- **Model Interaction:** Create endpoints to handle requests and interact with the LLM for generating responses.\\n\\n### 6. Optimize and Test\\n- **Performance:** Optimize your model’s performance by adjusting parameters, or using smaller, faster models if necessary.\\n- **Testing:** Rigorously test the application to ensure functionality, accuracy, and user satisfaction.\\n\\n### 7. Deploy\\n- **Deployment Platforms:** Deploy your app using platforms like Heroku, AWS Elastic Beanstalk, or Docker containers.\\n- **Scaling:** Ensure your architecture can scale to handle increasing loads as your user base grows.\\n\\n### 8. Monitor and Iterate\\n- **Feedback:** Gather user feedback for improvements.\\n- **Monitoring:** Use tools to monitor app performance and logs for any issues.\\n- **Updates:** Regularly update the model and features based on feedback and technological advancements.\\n\\n### Additional Considerations\\n- **Ethics and Bias:** Be mindful of ethical considerations and biases inherent in LLMs and implement safeguards.\\n- **Security:** Ensure your app is secure, especially if handling sensitive data.\\n- **Cost Management:** Keep track of costs, especially if using cloud-based models and services.\\n\\nBy following these steps, you can create an LLM-powered application that is both functional and valuable to your users.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 584, 'prompt_tokens': 15, 'total_tokens': 599, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'stop', 'logprobs': None}, id='run-346017b9-00e1-4ec2-b122-434406292c68-0', usage_metadata={'input_tokens': 15, 'output_tokens': 584, 'total_tokens': 599, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "llm.invoke(\"How to build an LLM app?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-chatgpt-apps",
   "language": "python",
   "name": "oreilly-chatgpt-apps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
