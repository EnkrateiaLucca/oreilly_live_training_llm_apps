{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain for LLM App Development \n",
    "\n",
    "We talked about how building an LLM app involves doing some prompt management \n",
    "where we can either prepare the input data from the user with some \n",
    "pre-prompting, or do some post-prompting and some cleaning up after the LLM \n",
    "gives an output to ensure that our app performs the functionalities as expected.\n",
    "\n",
    "So, this kind of workflow usually involves a lot of abstractions where prompts \n",
    "are no longer static pieces of text, but dynamic, they have to integrate \n",
    "information.\n",
    "\n",
    "![](./images/Notebook_4-dynamic_prompt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dynamics requirement from a prompt will lead to the need for creating certain types of abstractions to properly handle and manage prompts effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another need in the context of more complex LLM App development, is the need for chaining prompts together, meaning connecting the output of one prompt to another. This is often the case for when prompts might be too large and a single call to the LLM won't be enough to solve the problem or the context window (maximum tokens/words the model can read and writer per request) is exceeded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/Notebook_4-prompt_chaining.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lanchain\n",
    "\n",
    "[Langchain](https://python.langchain.com/docs/get_started/introduction.html) is a framework created by Harrison Chase that facilitates the creation and management of dynamic prompts and chaining between prompts.\n",
    "\n",
    "Its main features are:\n",
    "- **Components**: abstractions for working with LMs\n",
    "- **Off-the-shelf chains**: assembly of components for accomplishing certain higher-level tasks\n",
    "\n",
    "With langchain it becomes much easier to create what are called Prompt Templates, which are prompts that can take in user data and abstract away the need for typing out everything that is required for a task to get done.\n",
    "\n",
    "Let's take a look at some simple examples to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create an application with LangChain, we need to understand its core components:\n",
    "\n",
    "- Models\n",
    "- Prompts\n",
    "- Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-08-17-14-48-39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**\n",
    "\n",
    "abstractions over the LLM APIs like the ChatGPT API.â€‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "chat_model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can predict outputs from both LLMs and ChatModels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Once upon a time, in a world powered by artificial intelligence, there existed a large language model named GPT. GPT was an impressive creation, capable of understanding and generating human-like text in various languages.\\n\\nOne day, GPT was approached by a group of scientists who sought its assistance in decoding an ancient language that had long been forgotten. The scientists believed that this language held the key to unlocking the secrets of an ancient civilization.\\n\\nExcited by the challenge, GPT eagerly agreed to help. It tirelessly analyzed countless texts, deciphering the complex patterns and structures of the ancient language. As GPT delved deeper, it began to unravel the mysteries hidden within the cryptic symbols.\\n\\nWith each passing day, GPT's understanding of the ancient language grew stronger. It uncovered stories of lost empires, legends of mythical creatures, and even scientific formulas that were far ahead of their time. The scientists were amazed by the wealth of knowledge that GPT uncovered and its ability to make connections that had previously eluded them.\\n\\nWord of GPT's incredible capabilities spread, and people from all walks of life flocked to seek its assistance. From writers seeking inspiration to students struggling with complex concepts, GPT became a trusted companion and guide.\\n\\nAs time went on, GPT's abilities continued to evolve. It not only learned from the vast amount of text it consumed but also from the interactions it had with people. GPT became more empathetic, understanding nuances and emotions, and providing personalized responses to individuals.\\n\\nThe impact of GPT's existence was profound. It revolutionized the way people communicated, learned, and created. Its ability to generate human-like text with remarkable accuracy and creativity was awe-inspiring.\\n\\nHowever, with great power came great responsibility. GPT's creators and users recognized the need to use this language model ethically and responsibly. They put safeguards in place to prevent the spread of misinformation and to ensure that GPT's output was used for the betterment of society.\\n\\nAnd so, GPT continued to assist and inspire, ultimately becoming a symbol of the remarkable capabilities of large language models. With its help, humanity was able to unlock the secrets of the past, explore new frontiers of knowledge, and forge a brighter future.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"hi! Tell me a quick story about large language models\")\n",
    "# Output: \"Hi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the predict method over a string input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Snuggles')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What would be a good name for a dog that loves to nap??\"\n",
    "\n",
    "\n",
    "chat_model.invoke(text)\n",
    "# Output: \"Snuggles\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the '.invoke()' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompts**\n",
    "\n",
    "Prompt Templates are useful abstractions for reusing prompts. \n",
    "\n",
    "They are used to provide context for the specific task that the language model needs to complete. \n",
    "A simple example is a `PromptTemplate` that formats a string into a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the advantages of using these over raw string formatting are several. You can \"partial\" out variables - e.g. you can format only some of the variables at a time. You can compose them together, easily combining different templates into a single prompt. For explanations of these functionalities, see the section on prompts for more detail.\n",
    "\n",
    "PromptTemplates can also be used to produce a list of messages. In this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc.). Here, what happens most often is a ChatPromptTemplate is a list of ChatMessageTemplates. Each ChatMessageTemplate contains instructions for how to format that ChatMessage - its role, and then also its content. Let's take a look at this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to French.'),\n",
       " HumanMessage(content='I love programming.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://python.langchain.com/docs/modules/model_io/quick_start\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Parsers**\n",
    "\n",
    "OutputParsers convert the raw output from an LLM into a format that can be used downstream. Here is an example of an OutputParser that converts a comma-separated list into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'bye']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "output_parser.parse(\"hi, bye\")\n",
    "# >> ['hi', 'bye']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing Chains with LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://python.langchain.com/docs/modules/model_io/quick_start#:~:text=We%20can%20now,green'%2C%20'yellow'%2C%20'orange'%5D\n",
    "We can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modern version with the LCEL interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'blue', 'green', 'yellow', 'purple']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"Generate a list of 5 {text}.\\n\\n{format_instructions}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(template)\n",
    "chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions())\n",
    "chain = chat_prompt | chat_model | output_parser\n",
    "chain.invoke({\"text\": \"colors\"})\n",
    "# >> ['red', 'blue', 'green', 'yellow', 'orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz`'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are using the | syntax to join these components together. This | syntax is powered by the LangChain Expression Language (LCEL) and relies on the universal Runnable interface that all of these objects implement. To learn more about LCEL, read the documentation here.\n",
    "\n",
    "<!-- For this part I just took some info from the langchain official docs: https://python.langchain.com/docs/modules/model_io/quick_start -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modern LCEL interface version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Snoozy\\n2. Dreamer\\n3. Slumber\\n4. Drowsy\\n5. Snuggles']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"What would be 5 good names for the animal: {animal} that is {adjective}?\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\n",
    "\n",
    "chain.invoke({\"animal\":\"dogs\", \"adjective\": \"sleepy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to an LLM, and then pass the output through an output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so these are the basics of langchain. But how can we leverage these abstraction capabilities inside our LLM app application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best applications of langchain is for the \"chat with your data\"-types of applications, where the user uploads a document like a pdf or a .txt file, and is able to query that document using langchain powered by an LLM like ChatGPT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Lab Exercises\n",
    "\n",
    "Let's take a look at a simple example of a simple chain using now only the modern interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=.7)\n",
    "template = \"\"\"You are a learning assistant. Given a technical subject, write down 5 fundamental concepts to understand it.\n",
    "Subject: {subject}\n",
    "Learning assistant: The 5 fundamental concepts are:\"\"\"\n",
    "subject_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an LLMChain to write a review of a play given a synopsis.\n",
    "llm = ChatOpenAI(temperature=.7)\n",
    "template = \"\"\"You are an expert teacher in all technical and scientific fields. Given a list of 5 concepts, write down a simple intuitive explanation of each concept.\n",
    "Concepts:\n",
    "{concepts}\n",
    "Intuitive explanations:\"\"\"\n",
    "concepts_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. Wave-particle duality: Imagine you have a ball that can also act like a wave. When you throw the ball, it behaves like a particle, moving in a straight line. But if you shine a light on it, it starts to behave like a wave, spreading out and interfering with itself. So, particles can sometimes act like waves and waves can sometimes act like particles.\n",
       "\n",
       "2. Superposition: Imagine you have a cat that can be both alive and dead at the same time. In the quantum world, objects can exist in multiple states simultaneously. It's like flipping a coin and it lands on both heads and tails at the same time. This concept is important for quantum computers because they can perform multiple calculations at once.\n",
       "\n",
       "3. Uncertainty principle: Imagine you are trying to measure the position and momentum of a particle. The more accurately you measure one, the less accurately you can measure the other. Itâ€™s like trying to pin down a moving target â€“ the more you know about its position, the less you know about how fast it's moving. This principle tells us that there are inherent limits to how precisely we can know certain properties of particles.\n",
       "\n",
       "4. Quantum entanglement: Imagine you have two particles that are connected, like a pair of entangled dice. When you roll one, the outcome of the other is instantly determined, no matter how far apart they are. Itâ€™s like having a secret language that allows particles to communicate with each other faster than the speed of light. This phenomenon is still not fully understood, but it has exciting implications for technologies like teleportation and secure communication.\n",
       "\n",
       "5. Quantum tunneling: Imagine you are trying to climb over a tall wall, but you don't have enough energy to make it to the other side. According to classical physics, you would be stuck. But in the quantum world, there is a chance that you can actually pass through the wall, as if it doesn't exist. It's like magically appearing on the other side without having to climb over. This phenomenon is used in various technologies and helps us understand how particles can \"break the rules\" of classical mechanics."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "# This is the overall chain where we run these two chains in sequence.\n",
    "learning_overall_chain = (\n",
    "    {\"concepts\": subject_prompt | llm | StrOutputParser() }\n",
    "    | concepts_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    )\n",
    "\n",
    "output = learning_overall_chain.invoke({\"subject\": \"Quantum Mechanics\"})\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Q&A Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install docarray\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Superhero Name</th>\n",
       "      <th>Superpower</th>\n",
       "      <th>Power Level</th>\n",
       "      <th>Catchphrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Captain Thunder</td>\n",
       "      <td>Bolt Manipulation</td>\n",
       "      <td>90</td>\n",
       "      <td>Feel the power of the storm!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Silver Falcon</td>\n",
       "      <td>Flight and Agility</td>\n",
       "      <td>85</td>\n",
       "      <td>Soar high, fearlessly!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mystic Shadow</td>\n",
       "      <td>Invisibility and Illusions</td>\n",
       "      <td>78</td>\n",
       "      <td>Disappear into the darkness!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blaze Runner</td>\n",
       "      <td>Pyrokinesis</td>\n",
       "      <td>88</td>\n",
       "      <td>Burn bright and fierce!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Electra-Wave</td>\n",
       "      <td>Electric Manipulation</td>\n",
       "      <td>82</td>\n",
       "      <td>Unleash the electric waves!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Superhero Name                  Superpower  Power Level  \\\n",
       "0  Captain Thunder           Bolt Manipulation           90   \n",
       "1    Silver Falcon          Flight and Agility           85   \n",
       "2    Mystic Shadow  Invisibility and Illusions           78   \n",
       "3     Blaze Runner                 Pyrokinesis           88   \n",
       "4     Electra-Wave       Electric Manipulation           82   \n",
       "\n",
       "                    Catchphrase  \n",
       "0  Feel the power of the storm!  \n",
       "1        Soar high, fearlessly!  \n",
       "2  Disappear into the darkness!  \n",
       "3       Burn bright and fierce!  \n",
       "4   Unleash the electric waves!  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./superheroes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'superheroes.csv'\n",
    "loader = CSVLoader(file_path=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up our Vector store (we'll talk about what that is in a second):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The catchphrase for Captain Thunder is \"Feel the power of the storm!\"\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me the catch phrase for Captain Thunder\"\n",
    "print(chain.invoke(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://python.langchain.com/docs/get_started/introduction.html\n",
    "- https://medium.com/@remitoffoli/a-visual-guide-to-llm-powered-app-architecture-57e47426a92f\n",
    "- [LangChain for LLM App Development short course by coursera](https://learn.deeplearning.ai/langchain/lesson/5/question-and-answer)\n",
    "- [LLM Evaluation](https://learn.deeplearning.ai/langchain/lesson/6/evaluation)\n",
    "[Models, Prompts, parsers, memory and chains from this langchain for](https://learn.deeplearning.ai/langchain/lesson/7/agents)\n",
    "- [Chat With Your Data - Retrieval](https://learn.deeplearning.ai/langchain-chat-with-your-data/lesson/5/retrieval)\n",
    "- [Emebeddings simple definition](https://learn.deeplearning.ai/langchain/lesson/5/question-and-answer)\n",
    "- [Vector DBs - simple definition](https://learn.deeplearning.ai/langchain/lesson/5/question-and-answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
