{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain for LLM App Development \n",
    "\n",
    "We talked about how building an LLM app involves doing some prompt management \n",
    "where we can either prepare the input data from the user with some \n",
    "pre-prompting, or do some post-prompting and some cleaning up after the LLM \n",
    "gives an output to ensure that our app performs the functionalities as expected.\n",
    "\n",
    "So, this kind of workflow usually involves a lot of abstractions where prompts \n",
    "are no longer static pieces of text, but dynamic, they have to integrate \n",
    "information.\n",
    "\n",
    "![](./images/Notebook_4-dynamic_prompt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dynamics requirement from a prompt will lead to the need for creating certain types of abstractions to properly handle and manage prompts effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another need in the context of more complex LLM App development, is the need for chaining prompts together, meaning connecting the output of one prompt to another. This is often the case for when prompts might be too large and a single call to the LLM won't be enough to solve the problem or the context window (maximum tokens/words the model can read and writer per request) is exceeded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/Notebook_4-prompt_chaining.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lanchain\n",
    "\n",
    "[Langchain](https://python.langchain.com/docs/get_started/introduction.html) is a framework created by Harrison Chase that facilitates the creation and management of dynamic prompts and chaining between prompts.\n",
    "\n",
    "Its main features are:\n",
    "- **Components**: abstractions for working with LMs\n",
    "- **Off-the-shelf chains**: assembly of components for accomplishing certain higher-level tasks\n",
    "\n",
    "With langchain it becomes much easier to create what are called Prompt Templates, which are prompts that can take in user data and abstract away the need for typing out everything that is required for a task to get done.\n",
    "\n",
    "Let's take a look at some simple examples to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create an application with LangChain, we need to understand its core components:\n",
    "\n",
    "- Models\n",
    "- Prompts\n",
    "- Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-08-17-14-48-39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**\n",
    "\n",
    "abstractions over the LLM APIs like the ChatGPT API.â€‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can predict outputs from both LLMs and ChatModels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.predict(\"hi!\")\n",
    "# Output: \"Hi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the predict method over a string input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Snuggles'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What would be a good name for a dog that loves to nap??\"\n",
    "\n",
    "\n",
    "chat_model.predict(text)\n",
    "# Output: \"Snuggles\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the '.invoke()' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use the `predict_messages` method over a list of messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='For a dog that loves to nap, a good name could be \"Snooze\" or \"Dreamer.\" These names capture the essence of their favorite activity.\\n\\nFor a cat that loves to nap, a good name could be \"Snuggles\" or \"Purrfect.\" These names reflect their love for cozy and peaceful moments.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "text = \"What would be a good dog name for a dog that loves to nap?\"\n",
    "messages = [HumanMessage(content=text), \n",
    "            HumanMessage(content=\"What would be a good cat name for a cat that loves to nap?\")]\n",
    "chat_model.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompts**\n",
    "\n",
    "Prompt Templates are useful abstractions for reusing prompts. \n",
    "\n",
    "They are used to provide context for the specific task that the language model needs to complete. \n",
    "A simple example is a `PromptTemplate` that formats a string into a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good dog name for a dog that loves to nap?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good dog name for a dog that loves to {activity}?\")\n",
    "prompt.format(activity=\"nap\")\n",
    "# Output: \"What is a good dog name for a dog that loves to nap?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good dog name for a dog that loves to nap?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"What is a good dog name for a {animal} that loves to {activity}?\")\n",
    "prompt.format(activity=\"nap\", animal='dog')\n",
    "# Output: \"What is a good dog name for a dog that loves to nap?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The legacy version of chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A good dog name for a dog that loves to nap could be \"Snooze\" or \"Dozer\".'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=prompt,\n",
    ")\n",
    "chain.run({\"activity\": \"nap\", \"animal\": \"dog\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modern version with the LCEL interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Snooze')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | ChatOpenAI()\n",
    "\n",
    "chain.invoke({\"activity\": \"nap\", \"animal\": \"dog\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create more complex ChatPromptTemplates that contains a list of ChatMessageTemplates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to French.'),\n",
       " HumanMessage(content='I love programming.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Parsers**\n",
    "\n",
    "OutputParsers convert the raw output from an LLM into a format that can be used downstream. Here is an example of an OutputParser that converts a comma-separated list into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'bye']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "CommaSeparatedListOutputParser().parse(\"hi, bye\")\n",
    "# Output: ['hi', 'bye']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLMChain**\n",
    "\n",
    "Finally, you can combine all these components into an LLMChain, the legacy version would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Golden Retriever',\n",
       " 'German Shepherd',\n",
       " 'Labrador Retriever',\n",
       " 'Bulldog',\n",
       " 'Poodle']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generated 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{animal}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=chat_prompt,\n",
    "    output_parser=CommaSeparatedListOutputParser()\n",
    ")\n",
    "chain.run(\"dogs\")\n",
    "# Output: ['Golden Retriever','Labrador Retriever','German Shepherd','Bulldog','Poodle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modern LCEL interface version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Golden Retriever',\n",
       " 'Labrador Retriever',\n",
       " 'German Shepherd',\n",
       " 'Bulldog',\n",
       " 'Poodle']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\n",
    "\n",
    "chain.invoke({\"animal\":\"dogs\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to an LLM, and then pass the output through an output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so these are the basics of langchain. But how can we leverage these abstraction capabilities inside our LLM app application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best applications of langchain is for the \"chat with your data\"-types of applications, where the user uploads a document like a pdf or a .txt file, and is able to query that document using langchain powered by an LLM like ChatGPT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Lab Exercises\n",
    "\n",
    "Let's take a look at a simple example of a simple chain using now only the modern interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=.7)\n",
    "template = \"\"\"You are a learning assistant. Given a technical subject, write down 5 fundamental concepts to understand it.\n",
    "Subject: {subject}\n",
    "Learning assistant: The 5 fundamental concepts are:\"\"\"\n",
    "subject_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an LLMChain to write a review of a play given a synopsis.\n",
    "llm = ChatOpenAI(temperature=.7)\n",
    "template = \"\"\"You are an expert teacher in all technical and scientific fields. Given a list of 5 concepts, write down a simple intuitive explanation of each concept.\n",
    "Concepts:\n",
    "{concepts}\n",
    "Intuitive explanations:\"\"\"\n",
    "concepts_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Wave-particle duality: Imagine a tennis ball being thrown towards a wall. Usually, we expect the ball to bounce back or be absorbed by the wall. But in quantum mechanics, the ball can also behave like a wave and pass through the wall, like a ghostly mist. This means that particles can sometimes act like waves and exhibit strange behaviors that we don\\'t usually see in our everyday world.\\n\\n2. Uncertainty principle: Imagine trying to measure the position and momentum of a moving car at the same time. The more accurately you try to measure its position (by using a GPS device, for example), the less accurately you can measure its momentum (how fast it\\'s going). This is because by measuring one property, you disturb the other property, making it impossible to know both with perfect precision.\\n\\n3. Superposition: Think of a spinning top that can spin clockwise or counterclockwise. In quantum mechanics, the top can be in a superposition, where it is simultaneously spinning both clockwise and counterclockwise until someone looks at it and \"collapses\" it into one state. It\\'s like the top is in two different states at the same time, until we observe it and force it to choose one.\\n\\n4. Quantum entanglement: Imagine two entangled particles, like two coins that are magically connected. If you flip one coin and it lands on heads, the other coin will always land on tails, no matter how far apart they are. It\\'s like the coins are communicating with each other instantly, even though they are separated by a great distance. This spooky connection between particles is called quantum entanglement.\\n\\n5. Quantum tunneling: Imagine a person trying to climb over a high wall, but they don\\'t have enough energy to do so. In classical physics, they would be stuck and unable to pass through the wall. But in quantum mechanics, there\\'s a small chance that the person can \"tunnel\" through the wall, as if they magically appear on the other side without actually climbing over it. This phenomenon allows particles to overcome barriers that they shouldn\\'t be able to cross, defying our normal understanding of physics.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "learning_overall_chain = (\n",
    "    {\"concepts\": subject_prompt | llm | StrOutputParser() }\n",
    "    | concepts_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    )\n",
    "\n",
    "output = learning_overall_chain.invoke({\"subject\": \"Quantum Mechanics\"})\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Q&A Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install docarray\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Superhero Name</th>\n",
       "      <th>Superpower</th>\n",
       "      <th>Power Level</th>\n",
       "      <th>Catchphrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Captain Thunder</td>\n",
       "      <td>Bolt Manipulation</td>\n",
       "      <td>90</td>\n",
       "      <td>Feel the power of the storm!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Silver Falcon</td>\n",
       "      <td>Flight and Agility</td>\n",
       "      <td>85</td>\n",
       "      <td>Soar high, fearlessly!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mystic Shadow</td>\n",
       "      <td>Invisibility and Illusions</td>\n",
       "      <td>78</td>\n",
       "      <td>Disappear into the darkness!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blaze Runner</td>\n",
       "      <td>Pyrokinesis</td>\n",
       "      <td>88</td>\n",
       "      <td>Burn bright and fierce!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Electra-Wave</td>\n",
       "      <td>Electric Manipulation</td>\n",
       "      <td>82</td>\n",
       "      <td>Unleash the electric waves!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Superhero Name                  Superpower  Power Level  \\\n",
       "0  Captain Thunder           Bolt Manipulation           90   \n",
       "1    Silver Falcon          Flight and Agility           85   \n",
       "2    Mystic Shadow  Invisibility and Illusions           78   \n",
       "3     Blaze Runner                 Pyrokinesis           88   \n",
       "4     Electra-Wave       Electric Manipulation           82   \n",
       "\n",
       "                    Catchphrase  \n",
       "0  Feel the power of the storm!  \n",
       "1        Soar high, fearlessly!  \n",
       "2  Disappear into the darkness!  \n",
       "3       Burn bright and fierce!  \n",
       "4   Unleash the electric waves!  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./superheroes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'superheroes.csv'\n",
    "loader = CSVLoader(file_path=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up our Vector store (we'll talk about what that is in a second):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(vectorstore_cls=DocArrayInMemorySearch).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me the catch phrase for Captain Thunder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Feel the power of the storm!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://python.langchain.com/docs/get_started/introduction.html\n",
    "- https://medium.com/@remitoffoli/a-visual-guide-to-llm-powered-app-architecture-57e47426a92f\n",
    "- [LangChain for LLM App Development short course by coursera](https://learn.deeplearning.ai/langchain/lesson/5/question-and-answer)\n",
    "- [LLM Evaluation](https://learn.deeplearning.ai/langchain/lesson/6/evaluation)\n",
    "[Models, Prompts, parsers, memory and chains from this langchain for](https://learn.deeplearning.ai/langchain/lesson/7/agents)\n",
    "- [Chat With Your Data - Retrieval](https://learn.deeplearning.ai/langchain-chat-with-your-data/lesson/5/retrieval)\n",
    "- [Emebeddings simple definition](https://learn.deeplearning.ai/langchain/lesson/5/question-and-answer)\n",
    "- [Vector DBs - simple definition](https://learn.deeplearning.ai/langchain/lesson/5/question-and-answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly_env",
   "language": "python",
   "name": "oreilly_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
