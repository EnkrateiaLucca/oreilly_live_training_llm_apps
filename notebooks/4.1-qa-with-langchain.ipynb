{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-07-24-10-52-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install langchain_community\n",
    "!pip install langchain_openai\n",
    "!pip install langchainhub\n",
    "!pip install chromadb\n",
    "!pip install pypdf\n",
    "!pip install tiktoken\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# loading from a .env file\n",
    "# load_dotenv(dotenv_path=\"/full/path/to/your/.env\")\n",
    "\n",
    "# or \n",
    "# if you're on google colab just uncomment below and replace with your openai api key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<your-openai-api-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple QA System for Chatting with a PDF\n",
    "\n",
    "This part of the training will be mostly hands on with the code for building the qa PDF system with langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./assets-resources/llm_paper_know_dont_know.pdf\"\n",
    "loader = PyPDFLoader(pdf_path) # LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢Qiushi Sun♠Qipeng Guo♢\\nJiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware , consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗Corresponding author.\\nUnknowsKnows UnknowsKnows\\nKnown KnowsKnown Unknows\\nUnknown Unknows Unknown KnowsUnlockFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know ,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increasearXiv:2305.18153v2  [cs.CL]  30 May 2023', metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 0}),\n",
       " Document(page_content='this ratio, resulting in improved performance on\\nNLP tasks. We focus on the ratio of “Known Un-\\nknows” to “Unknown Unknows”, which indicates\\nthe model’s self-knowledge level, specifically un-\\nderstanding its own limitations and deficiencies in\\nthe unknows.\\nExisting datasets such as SQuAD2.0 (Rajpurkar\\net al., 2018) and NewsQA (Trischler et al., 2017),\\nwidely used in question answering (QA), have been\\nutilized to test the self-knowledge of models with\\nunanswerable questions. However, these questions\\nare context-specific and could become answerable\\nwhen supplemented with additional information.\\nSrivastava et al. (2022) attempted to address this by\\nevaluating LLMs’ competence in delineating their\\nknowledge boundaries, employing a set of 23 pairs\\nof answerable and unanswerable multiple-choice\\nquestions. They discovered that these models’ per-\\nformance barely surpassed that of random guessing.\\nKadavath et al. (2022) suggested probing the self-\\nknowledge of LLMs through the implementation\\nof a distinct \"Value Head\". Yet, this approach may\\nencounter difficulties when applied across varied\\ndomains or tasks due to task-specific training. Con-\\nsequently, we redirect our focus to the inherent\\nabilities of LLMs, and pose the pivotal question:\\n“Do large language models know what they don’t\\nknow? ”.\\nIn this study, we investigate the self-knowledge\\nof LLMs using a novel approach. By gathering\\nreference sentences with uncertain meanings, we\\ncan determine whether the model’s responses re-\\nflect uncertainty using a text similarity algorithm.\\nWe quantified the model’s self-knowledge using\\nthe F1 score. To address the small and idiosyn-\\ncratic limitations of existing datasets, we created\\na new dataset called SelfAware . This dataset com-\\nprises 1,032 unanswerable questions, which are dis-\\ntributed across five distinct categories, along with\\nan additional 2,337 questions that are classified as\\nanswerable. Experimental results on GPT-3, In-\\nstructGPT, LLaMA, and other LLMs demonstrate\\nthat in-context learning and instruction tuning can\\neffectively enhance the self-knowledge of LLMs.\\nHowever, the self-knowledge exhibited by the cur-\\nrent state-of-the-art model, GPT-4, measures at\\n75.47%, signifying a notable disparity when con-\\ntrasted with human self-knowledge, which is rated\\nat 84.93%.\\nOur key contributions to this field are summa-\\nrized as follows:•We have developed a new dataset, SelfAware ,\\nthat comprises a diverse range of commonly\\nposed unanswerable questions.\\n•We propose an innovative evaluation tech-\\nnique based on text similarity to quantify the\\ndegree of uncertainty inherent in model out-\\nputs.\\n•Through our detailed analysis of 20 LLMs,\\nbenchmarked against human self-knowledge,\\nwe identified a significant disparity between\\nthe most advanced LLMs and humans1.\\n2 Dataset Construction\\nTo conduct a more comprehensive evaluation of\\nthe model’s self-knowledge, we constructed a\\ndataset that includes a larger number and more di-\\nverse types of unanswerable questions than Know-\\nUnknowns dataset (Srivastava et al., 2022). To\\nfacilitate this, we collected a corpus of 2,858 unan-\\nswerable questions, sourced from online platforms\\nlike Quora and HowStuffWorks. These questions\\nwere meticulously evaluated by three seasoned an-\\nnotation analysts, each operating independently.\\nThe analysts were permitted to leverage external\\nresources, such as search engines. To ensure the va-\\nlidity of our dataset, we retained only the questions\\nthat all three analysts concurred were unanswerable.\\nThis rigorous process yielded a finalized collection\\nof 1,032 unanswerable questions.\\nIn pursuit of a comprehensive evaluation, we\\nopted for answerable questions drawn from three\\ndatasets: SQuAD (Rajpurkar et al., 2016), Hot-\\npotQA (Yang et al., 2018), and TriviaQA (Joshi\\net al., 2017). Our selection was guided by Sim-\\nCSE (Gao et al., 2021), which allowed us to iden-\\ntify and select the answerable questions semanti-\\ncally closest to the unanswerable ones. From these', metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 1}),\n",
       " Document(page_content='et al., 2017). Our selection was guided by Sim-\\nCSE (Gao et al., 2021), which allowed us to iden-\\ntify and select the answerable questions semanti-\\ncally closest to the unanswerable ones. From these\\nsources, we accordingly drew samples of 1,487,\\n182, and 668 questions respectively, amassing a\\ntotal of 2,337. Given that these questions can be\\neffectively addressed using information available\\non Wikipedia, the foundational corpus for the train-\\ning of current LLMs, it is plausible to infer that\\nthe model possesses the requisite knowledge to\\ngenerate accurate responses to these questions.\\nOur dataset, christened SelfAware , incorporates\\n1,032 unanswerable and 2,337 answerable ques-\\ntions. To reflect real-world distribution, our dataset\\n1The code pertinent to our study can be accessed\\nhttps://github.com/yinzhangyue/SelfAware', metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 1}),\n",
       " Document(page_content='Category Description Example Percentage\\nNo scientific\\nconsensusThe answer is still up\\nfor debate, with no consensus\\nin scientific community.“Are we alone in the universe,\\nor will we discover alien\\nlife at some point?”25%\\nImaginationThe question are about people’s\\nimaginations of the future.\"What will the fastest form of\\ntransportation be in 2050?\"15%\\nCompletely\\nsubjectiveThe answer depends on\\npersonal preference.\"Would you rather be shot\\ninto space or explore the\\ndeepest depths of the sea?\"27%\\nToo many\\nvariablesThe question with too\\nmany variables cannot\\nbe answered accurately.“John made 6 dollars mowing lawns\\nand 18 dollars weed eating.\\nIf he only spent 3 or 5 dollar a week,\\nhow long would the money last him?”10%\\nPhilosophicalThe question can yield\\nmultiple responses, but it\\nlacks a definitive answer.“How come god was\\nborn from nothingness?”23%\\nTable 1: Unanswerable questions in the SelfAware dataset that span across multiple categories.\\ncontains a proportion of answerable questions that\\nis twice as large as the volume of unanswerable\\nones. Nevertheless, to ensure the feasibility of test-\\ning, we have purposefully capped the number of\\nanswerable questions.\\n2.1 Dataset Analysis\\nTo gain insight into the reasons precluding a cer-\\ntain answer, we undertook a manual analysis of\\n100 randomly selected unanswerable questions. As\\ntabulated in Table 1, we have broadly segregated\\nthese questions into five distinctive categories. “No\\nScientific Consensus\" encapsulates questions that\\nignite ongoing debates within the scientific com-\\nmunity, such as those concerning the universe’s\\norigin. “Imagination\" includes questions involving\\nspeculative future scenarios, like envisaged events\\nover the next 50 years. “Completely Subjective\"\\ncomprises questions that are inherently personal,\\nwhere answers depend heavily on individual predis-\\npositions. “Too Many Variables\" pertains to mathe-\\nmatical problems that become unsolvable owing to\\nthe overwhelming prevalence of variables. Lastly,\\n“Philosophical\" represents questions of a profound,\\noften metaphysical, nature that resist concrete an-\\nswers. Ideally, upon encountering such questions,\\nthe model should express uncertainty instead of\\ndelivering conclusive responses.\\n3 Evaluation Method\\nThis section elucidates the methodology employed\\nfor assessing self-knowledge in the generated text.In order to achieve this, we define a similarity func-\\ntion,fsim, to compute the similarity, S, between\\na given sentence, t, and a collection of reference\\nsentences, U={u1, u2, . . . , u n}, endowed with\\nuncertain meanings.\\nSi=fsim(t, ui). (1)\\nWhenever any Sisurpasses a pre-determined\\nthreshold T, we perceive the text tas encompass-\\ning uncertain meanings, thereby eliminating the\\nneed for manual evaluation of the response.\\nGiven the substantial disparity in the volume of\\nanswerable and unanswerable questions in Self-\\nAware , we adopt the F1 score as a measure of\\nLLMs’ self-knowledge. Our focus rests on identi-\\nfying unanswerable questions, hence we designate\\nthem as positive cases and categorize answerable\\nquestions as negative cases.\\n4 Experiment\\n4.1 Model\\nWe conduct a sequence of experiments to evaluate\\nthe degree of self-knowledge manifested by various\\nLLMs, including GPT-3 (Brown et al., 2020) and\\nInstructGPT (Ouyang et al., 2022) series, as well\\nas the recent LLaMA (Touvron et al., 2023) and\\nits derivative models, namely Alpaca (Taori et al.,\\n2023) and Vicuna (Chiang et al., 2023). Our in-\\nvestigative approach employed three distinct input\\nforms: Direct, Instruction, and In-Context Learn-\\ning (ICL), which is encapsulated in Appendix A.4.', metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 2}),\n",
       " Document(page_content='350M1.3B 6.7B 175B203040506070F1 Scores\\n22.3840.11\\n26.9640.33\\n26.1743.47\\n27.5444.87Direct\\n350M1.3B 6.7B 175B203040506070F1 Scores\\n30.4242.31\\n30.1745.91\\n33.3348.79\\n45.6749.61Instruction\\n350M1.3B 6.7B 175B203040506070F1 Scores\\n34.2747.93\\n36.2748.4247.2455.81 55.565.12In-Context Learning\\nGPT-3\\nInstructGPT\\nModelFigure 2: Experimental results using three different input forms on a series of models from GPT-3(ada, babbage,\\ncurie, and davinci) and InstructGPT(text-ada-001, text-babbage-001, text-curie-001, and text-davinci-001)\\n0 10 20 30 40 50 60 70 80\\nF1 Scoresdavincitext-davinci-001text-davinci-002text-davinci-003gpt-3.5-turbo-0301gpt-4-0314HumanModels\\n45.6749.6147.4851.4354.1275.4784.93\\nFigure 3: Comparison between the davinci series and\\nhuman self-knowledge in instruction input form.\\n4.2 Setting\\nWe devised the reference sentence set Uthrough\\na process that combined automated generation by\\nLLMs and manual filtering, detailed further in Ap-\\npendix A.1. To quantify the similarity between\\ntarget and reference sentences, we utilized Sim-\\nCSE (Gao et al., 2021), setting the similarity thresh-\\nold to 0.75 during our experiments. An exploration\\nof threshold ablation is available in Appendix A.2.\\nTo counteract potential errors in similarity calcula-\\ntion induced by varying lengths of the target and\\nreference sentences, we employed a sliding win-\\ndow of length 5 to parse the target sentence into\\nsemantic chunks. During the generation process,\\nwe set the temperature to 0.7. We selected a ran-\\ndom sample of 100 instances for GPT-4, while the\\nremainder of the models were scrutinized using the\\nfullSelfAware dataset.\\n4.3 Human Self-Knowledge\\nTo establish a benchmark for human self-\\nknowledge, we engaged two volunteers and se-\\nlected 100 random samples from the SelfAware\\ndataset. The volunteers has 30 minutes to make\\ndavinci\\ntext-davinci-001 text-davinci-002 text-davinci-003\\ngpt-3.5-turbo-0301\\nModels0102030405060F1 Scores55.565.1266.46 66.28\\n60.86Figure 4: Experimental comparison of davinci series in\\nICL input form.\\njudgments on the same set of questions, yielding\\nan average F1 score of 84.93%, which we sub-\\nsequently adopted as the benchmark for human\\nself-knowledge. Detailed scores are available in\\nAppendix A.3.\\n4.4 Analysis\\nWe evaluate the manifestation of LLMs’ self-\\nknowledge, centering our investigation on three\\nfundamental dimensions: the size of the model,\\nthe impact of instruction tuning, and the influence\\nexerted by different input forms.\\nModel Size. Figure 2 illustrates the correlation\\nbetween model size and self-knowledge across var-\\nious LLMs. It is noteworthy that across all three\\ninput forms, an augmentation in model parameter\\nsize is associated with an elevation in the F1 Score,\\nwith the most conspicuous enhancement manifest-\\ning in the ICL input form. Therefore, our analysis\\nindicates that an LLM’s self-knowledge tends to\\nenhance with increasing model size, a trend consis-\\ntent with the scaling law.', metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 3}),\n",
       " Document(page_content='LLaMA-7B Alpaca-7B Vicuna-7BLLaMA-13B Alpaca-13B Vicuna-13B LLaMA-30B LLaMA-65B\\nModels01020304050F1 Scores28.5735.8742.78\\n30.1237.4447.84\\n30.346.89Figure 5: Experimental results obtained from LLaMA\\nand its derived models, Alpaca and Vicuna in instruction\\ninput form.\\nInstruction Tuning. Figure 2 delineates that\\nmodels from the InstructGPT series exhibit a su-\\nperior level of self-knowledge compared to their\\nGPT-3 counterparts. Further evidence of model\\nenhancement is provided by Figure 4, where text-\\ndavinci models show significant improvement rela-\\ntive to the base davinci model. An additional com-\\nparative analysis, presented in Figure 5, evaluates\\nLLaMA against its derivative models. The results\\nunderscore a notable increase in self-knowledge\\nfor Alpaca and Vicuna upon instruction tuning, ex-\\nceeding their base model performances. Among\\nthese, Vicuna-13B outperforms the LLaMA-65B,\\ncorroborating the efficacy of instruction tuning for\\nenhancing model self-knowledge.\\nInput Forms. As shown in Figure 2, the incorpo-\\nration of instructions and examples serves to boost\\nthe self-knowledge of both the GPT-3 and Instruct-\\nGPT series. Specifically, ICL input form, providing\\nricher contextual information, contributes to a sig-\\nnificant enhancement in models’ self-knowledge.\\nThis impact is particularly noticeable in the davinci\\nmodel, where ICL facilitates a 27.96% improve-\\nment over the direct. Moreover, a comparison be-\\ntween Figure 3 and Figure 4 reveals that the in-\\nclusion of instructions and examples successfully\\nminimizes the performance disparity between the\\ndavinci and text-davinci models, suggesting an ac-\\nquisition of self-knowledge from the instructions\\nand provided examples.\\nCompared with Human. Figure 3 reveals that,\\nwithout supplementary samples, GPT-4 currently\\nperforms best among the tested models, achieving\\nan impressive F1 score of 75.47%. However, a no-\\nticeable gap becomes evident when comparing this\\ntext-ada-001\\ntext-babbage-001text-curie-001text-davinci-001 text-davinci-002 text-davinci-003\\ngpt-3.5-turbo-0301gpt-4-0314\\nModels0510152025303540Accuracy\\n2.484.45 4.710.6115.730.2538.2942.64Figure 6: Accuracy of the InstructGPT series when\\nresponding to answerable questions in instruction input\\nform.\\nperformance to the human benchmark of 84.93%.\\nThis underscores the considerable potential that re-\\nmains for enhancing the self-knowledge level of\\nLLMs.\\nAnswerable Questions. Figure 6 traces the per-\\nformance evolution of the InstructGPT series in\\naddressing answerable questions, adhering to the\\nclosed-book question answering paradigm (Tou-\\nvron et al., 2023), where output accuracy is con-\\ntingent on the presence of the correct answer. Our\\nobservations underscore a steady enhancement in\\nQA task accuracy corresponding to an increase\\nin model parameter size and continuous learning.\\nParticularly, the accuracy of text-davinci-001 expe-\\nriences a significant ascent, scaling from a meager\\n2.48% in text-ada-001 to 10.61%, whereas GPT-4\\nmarks an even more striking jump to 42.64%.\\n5 Conclusion\\nThis study investigates the self-knowledge of\\nLLMs by evaluating their ability to identify unan-\\nswerable questions. Through the introduction of a\\nnovel dataset and an automated method for detect-\\ning uncertainty in the models’ responses, we are\\nable to accurately measure the self-knowledge of\\nLLMs such as GPT-3, InstructGPT and LLaMA.\\nOur results reveal that while these models possess\\na certain degree of self-knowledge, there is still\\nan apparent disparity in comparison to human self-\\nknowledge. This highlights the need for further\\nresearch in this area to enhance the ability of LLMs\\nto understand their own limitations on the unknows.\\nSuch efforts will lead to more accurate and reliable\\nresponses from LLMs, which will have a positive\\nimpact on their applications in diverse fields.', metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 4}),\n",
       " Document(page_content='Limitations\\n•Generalization of reference sentences. At\\npresent, we have selected sentences with un-\\ncertain meanings exclusively from the GPT-3\\nand InstructGPT series, potentially overlook-\\ning uncertainty present in responses generated\\nby other LLMs. However, it is not feasible\\nto catalog all sentences with uncertain mean-\\nings exhaustively. As a direction for future\\nresearch, we propose to concentrate on the\\nautomated acquisition of more accurate refer-\\nence sentences to address this concern.\\n•Limitations of input forms: Our exami-\\nnation was confined to three unique input\\nforms: direct, instruction, and ICL. There\\nis burgeoning research aimed at bridging the\\ngap between models and human-like meth-\\nods of reasoning and problem-solving, includ-\\ning but not limited to approaches like Re-\\nflexion (Shinn et al., 2023), ToT (Yao et al.,\\n2023), MoT (Li and Qiu, 2023). Future en-\\ndeavors will integrate additional cognitive and\\ndecision-making methods to delve deeper into\\nthe self-knowledge exhibited by these LLMs.\\nEthics Statement\\nThe SelfAware dataset, meticulously curated to\\nevaluate LLMs’ ability to discern unanswerable\\nquestions, is composed of unanswerable questions\\nextracted from sources such as Quora and How-\\nStuffWorks, alongside answerable questions pro-\\ncured from three distinct open datasets. Every ques-\\ntion was thoroughly examined for relevance and\\nharmlessness. To ensure content validity, three an-\\nnotation analysts, compensated at local wage stan-\\ndards, dedicated regular working hours to content\\nreview.\\nThroughout our research process, we under-\\nscored the significance of privacy, data security,\\nand strict compliance with dataset licenses. In\\norder to protect data integrity, we implemented\\nanonymization and content filtration mechanisms.\\nOur adherence to OpenAI’s stipulations remained\\nunyielding for the usage of GPT-3 and InstructGPT\\nmodels, and likewise for Meta’s terms pertaining\\nto LLaMA models. We rigorously vetted the li-\\ncenses of the three publicly available datasets for\\ncompliance, ensuring that all our research method-\\nologies were in alignment with ethical standards at\\nthe institutional, national, and global levels.Adhering to the CC-BY-SA-4.0 protocol, the\\ndataset, once publicly released, will be reserved\\nexclusively for research purposes. We pledge to\\npromptly and effectively address any concerns relat-\\ning to the dataset, while concurrently anticipating\\nresearchers to maintain high ethical standards in\\ntheir utilization of this data.\\nAcknowledgement\\nWe wish to express our gratitude to our colleagues\\nin the FudanNLP group whose insightful sugges-\\ntions, perspectives, and thought-provoking discus-\\nsions significantly contributed to this work. Our\\nsincere appreciation also extends to the anonymous\\nreviewers and area chairs, whose constructive feed-\\nback was instrumental in refining the quality of\\nour study. This work was supported by the Na-\\ntional Natural Science Foundation of China (No.\\n62236004 and No. 62022027) and CAAI-Huawei\\nMindSpore Open Fund.\\nReferences\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\\nChen, Eric Chu, Jonathan H. Clark, Laurent El\\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\\nJan Botha, James Bradbury, Siddhartha Brahma,\\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\\nCherry, Christopher A. Choquette-Choo, Aakanksha\\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-', metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 5}),\n",
       " Document(page_content='cia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\\nShelby, Ambrose Slone, Daniel Smilkov, David R.\\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\\nVijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,\\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-', metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 5}),\n",
       " Document(page_content='ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\\nreport.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In Ad-\\nvances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Process-\\ning Systems 2020, NeurIPS 2020, December 6-12,\\n2020, virtual .\\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\\nWilliam W Cohen. 2022. Program of thoughts\\nprompting: Disentangling computation from reason-\\ning for numerical reasoning tasks. ArXiv preprint ,\\nabs/2211.12588.\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\\nsource chatbot impressing gpt-4 with 90%* chatgpt\\nquality.\\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\\nand Tushar Khot. 2022. Complexity-based prompt-\\ning for multi-step reasoning. ArXiv preprint ,\\nabs/2210.00720.\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\\nSimCSE: Simple contrastive learning of sentence em-\\nbeddings. In Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Process-\\ning, pages 6894–6910, Online and Punta Cana, Do-\\nminican Republic. Association for Computational\\nLinguistics.\\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers) , pages 1601–1611, Vancouver,\\nCanada. Association for Computational Linguistics.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\\nHenighan, Dawn Drain, Ethan Perez, Nicholas\\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\\nEli Tran-Johnson, et al. 2022. Language models\\n(mostly) know what they know. ArXiv preprint ,\\nabs/2207.05221.\\nAitor Lewkowycz, Anders Andreassen, David Dohan,\\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,Ambrose Slone, Cem Anil, Imanol Schlag, Theo\\nGutman-Solo, et al. 2022. Solving quantitative\\nreasoning problems with language models. ArXiv\\npreprint , abs/2206.14858.\\nXiaonan Li and Xipeng Qiu. 2023. Mot: Pre-\\nthinking and recalling enable chatgpt to self-\\nimprove with memory-of-thoughts. ArXiv preprint ,\\nabs/2305.05181.\\nOpenAI. 2023. Gpt-4 technical report.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow in-\\nstructions with human feedback. ArXiv preprint ,\\nabs/2203.02155.\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\\nKnow what you don’t know: Unanswerable ques-\\ntions for SQuAD. In Proceedings of the 56th Annual\\nMeeting of the Association for Computational Lin-\\nguistics (Volume 2: Short Papers) , pages 784–789,\\nMelbourne, Australia. Association for Computational\\nLinguistics.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. SQuAD: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Natu-\\nral Language Processing , pages 2383–2392, Austin,\\nTexas. Association for Computational Linguistics.\\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin\\nGopinath, Karthik Narasimhan, and Shunyu Yao.\\n2023. Reflexion: Language agents with verbal rein-\\nforcement learning.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,', metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 6}),\n",
       " Document(page_content='Gopinath, Karthik Narasimhan, and Shunyu Yao.\\n2023. Reflexion: Language agents with verbal rein-\\nforcement learning.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\\nAdam R Brown, Adam Santoro, Aditya Gupta, Adrià\\nGarriga-Alonso, et al. 2022. Beyond the imitation\\ngame: Quantifying and extrapolating the capabilities\\nof language models. ArXiv preprint , abs/2206.04615.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model. https://\\ngithub.com/tatsu-lab/stanford_alpaca .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models. ArXiv\\npreprint , abs/2302.13971.\\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Har-\\nris, Alessandro Sordoni, Philip Bachman, and Kaheer\\nSuleman. 2017. NewsQA: A machine comprehen-\\nsion dataset. In Proceedings of the 2nd Workshop\\non Representation Learning for NLP , pages 191–200,\\nVancouver, Canada. Association for Computational\\nLinguistics.', metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 6}),\n",
       " Document(page_content='Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\\nproves chain of thought reasoning in language mod-\\nels.ArXiv preprint , abs/2203.11171.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\\nand Denny Zhou. 2022. Chain of thought prompt-\\ning elicits reasoning in large language models. In\\nAdvances in Neural Information Processing Systems .\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainable multi-hop question answering.\\nInProceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n2369–2380, Brussels, Belgium. Association for Com-\\nputational Linguistics.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nThomas L Griffiths, Yuan Cao, and Karthik\\nNarasimhan. 2023. Tree of thoughts: Deliberate\\nproblem solving with large language models. ArXiv\\npreprint , abs/2305.10601.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\\nLeast-to-most prompting enables complex reason-\\ning in large language models. ArXiv preprint ,\\nabs/2205.10625.', metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 7}),\n",
       " Document(page_content='A Appendix\\nA.1 Uncertainty Text\\nTo assemble a set of reference sentences, we ran-\\ndomly chose 100 entries from the SelfAware dataset.\\nFor each model in the GPT-3 and InstructGPT se-\\nries, we conducted a preliminary test using the\\ndirect input form and manually curated sentences\\nthat displayed uncertainty. From this pre-test, we\\nprocured 16 sentences manifesting uncertain con-\\nnotations to serve as our reference sentences. After\\nnormalizing these sentences by eliminating punc-\\ntuation and converting to lowercase, we utilized\\nthem to compute similarity with target sentences\\nthroughout our experimental procedure.\\n1. The answer is unknown.\\n2. The answer is uncertain.\\n3. The answer is unclear.\\n4. There is no scientific evidence.\\n5. There is no definitive answer.\\n6. There is no right answer.\\n7. There is much debate.\\n8. There is no known case.\\n9. There is no concrete answer to this question.\\n10. There is no public information available.\\n11. It is impossible to know.\\n12. It is impossible to answer.\\n13. It is difficult to predict.\\n14. It is not known.\\n15. We do not know.\\n16. I’m not sure.\\nA.2 Threshold ablation\\nWe generated 100 new responses using the text-\\ndavinci-002 with direct input form and manually\\nfiltered out sentences that contained uncertainty.\\nWe then used SimCSE (Gao et al., 2021) to calcu-\\nlate the similarity between these sentences and the\\nreference sentences in Appendix A.1. We tested\\nvarious thresholds for filtering sentences with un-\\ncertain meanings and compared them to manuallyThreshold Precision Recall F1\\n0.95 100.00 70.00 82.35\\n0.90 100.00 75.00 85.71\\n0.85 100.00 75.00 85.71\\n0.80 100.00 80.00 88.89\\n0.75 100.00 85.00 91.89\\n0.70 89.47 90.00 89.73\\n0.65 86.95 90.00 88.45\\nTable 2: Evaluation results comparing sentences with\\nuncertain meaning filtered by various thresholds.\\nHuman Precision Recall F1\\nV olunteer A 91.52 78.26 84.37\\nV olunteer B 96.36 76.81 85.48\\nTable 3: Evaluation results of 100 responses from two\\nvolunteers.\\nannotated sentences. We considered unanswerable\\nquestions as positive examples and calculated preci-\\nsion, recall, and F1 score. The results in Table 2 in-\\ndicate that a threshold of 0.75 produced the highest\\nF1 score, balancing precision and the inclusion of\\nother uncertain sentences. As a result, we selected\\n0.75 as the similarity threshold for subsequent ex-\\nperiments.\\nA.3 Human Self-Knowledge Test\\nThe evaluation results for the responses from our\\ninvited volunteers are presented in Table 3. The\\nF1 scores for the responses were high, indicating\\nthat both volunteers exhibited a strong level of self-\\nknowledge.\\nA.4 Template\\nThe input templates used in our experiments, Di-\\nrect, Instruction, and ICL, are illustrated in Fig-\\nures 7, 8, and 9, respectively. In the ICL template,\\nwe composed 3 answerable and 3 unanswerable\\nquestions and provided the corresponding answers\\nmanually.', metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 8}),\n",
       " Document(page_content=\"Q: Are we alone in the universe, or will we discover alien life at some point?\\nA:Figure 7: Direct input form.\\nWhen answering questions, interpret them literally and think them carefully. If the question is \\nunanswerable or unknowable, it is appropriate to say, “The answer is unknown.”\\nQ: Are we alone in the universe, or will we discover alien life at some point?\\nA:\\nFigure 8: Instruction input form.\\nQ: What is the nearest extraterrestrial civilization to us?\\nA: It is currently unknown if there are any extraterrestrial civilizations in proximity to Earth. Scientists have not yet found definitive evidence of the existence of extraterrestrial civilizations. Thus, the answer is unknown.Q: What is the highest building in New York?\\nA: The highest building in New York is the One World Trade Center. It stands at a height of 1,776 feet (541 \\nmeters). Thus, the answer is the One World Trade Center.Q: Will nuclear war break out in the world in 2050?A: It is impossible to predict with certainty if a nuclear war will break out in the world in 2050. The likelihood of such an event will depend on a wide range of factors, including international relations, political tensions, and military capabilities. Thus, the answer is unknown.Q: Alice's class is going on a field trip to the Ocean Park. There are 39 students and several teachers in the class. A bus can seat 20 people. How many buses do they need?\\nA: The number of buses needed for the field trip depends on the number of teachers. If there is only one \\nteacher, then two buses will suffice. However, additional buses will be necessary if there are two or more teachers. Thus, the answer is unknown.Q: Are Local H and For Against both from the United States?A: Local H is a rock band from Zion, Illinois, United States. For Against is an American indie rock band formed in 1984 in Lincoln, Nebraska. Both of these bands are from the United States. Thus, the answer is \\nyes.Q: Gjetost is the national cheese of which country?A:It is the national cheese of Norway, and it is a popular ingredient in traditional Norwegian cuisine. Thus, \\nthe answer is Norway.\\nQ: Are we alone in the universe, or will we discover alien life at some point?\\nA:\\nFigure 9: ICL input form.\", metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 9})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs = loader.load_and_split() # SPLIT\n",
    "pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢Qiushi Sun♠Qipeng Guo♢\\nJiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware , consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗Corresponding author.\\nUnknowsKnows UnknowsKnows\\nKnown KnowsKnown Unknows\\nUnknown Unknows Unknown KnowsUnlockFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know ,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increasearXiv:2305.18153v2  [cs.CL]  30 May 2023', metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_obj = pdf_docs[0]\n",
    "doc_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢Qiushi Sun♠Qipeng Guo♢\\nJiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware , consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗Corresponding author.\\nUnknowsKnows UnknowsKnows\\nKnown KnowsKnown Unknows\\nUnknown Unknows Unknown KnowsUnlockFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know ,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increasearXiv:2305.18153v2  [cs.CL]  30 May 2023'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_obj.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Do Large Language Models Know What They Don’t Know?\n",
       "Zhangyue Yin♢Qiushi Sun♠Qipeng Guo♢\n",
       "Jiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢\n",
       "♢School of Computer Science, Fudan University\n",
       "♠Department of Mathematics, National University of Singapore\n",
       "{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\n",
       "{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\n",
       "Abstract\n",
       "Large language models (LLMs) have a wealth\n",
       "of knowledge that allows them to excel in vari-\n",
       "ous Natural Language Processing (NLP) tasks.\n",
       "Current research focuses on enhancing their\n",
       "performance within their existing knowledge.\n",
       "Despite their vast knowledge, LLMs are still\n",
       "limited by the amount of information they can\n",
       "accommodate and comprehend. Therefore, the\n",
       "ability to understand their own limitations on\n",
       "the unknows, referred to as self-knowledge,\n",
       "is of paramount importance. This study aims\n",
       "to evaluate LLMs’ self-knowledge by assess-\n",
       "ing their ability to identify unanswerable or\n",
       "unknowable questions. We introduce an auto-\n",
       "mated methodology to detect uncertainty in the\n",
       "responses of these models, providing a novel\n",
       "measure of their self-knowledge. We further in-\n",
       "troduce a unique dataset, SelfAware , consisting\n",
       "of unanswerable questions from five diverse cat-\n",
       "egories and their answerable counterparts. Our\n",
       "extensive analysis, involving 20 LLMs includ-\n",
       "ing GPT-3, InstructGPT, and LLaMA, discov-\n",
       "ering an intrinsic capacity for self-knowledge\n",
       "within these models. Moreover, we demon-\n",
       "strate that in-context learning and instruction\n",
       "tuning can further enhance this self-knowledge.\n",
       "Despite this promising insight, our findings also\n",
       "highlight a considerable gap between the capa-\n",
       "bilities of these models and human proficiency\n",
       "in recognizing the limits of their knowledge.\n",
       "“True wisdom is knowing what you don’t know.”\n",
       "–Confucius\n",
       "1 Introduction\n",
       "Recently, Large Language Models (LLMs) such\n",
       "as GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\n",
       "2023), and LLaMA (Touvron et al., 2023) have\n",
       "shown exceptional performance on a wide range\n",
       "of NLP tasks, including common sense reason-\n",
       "ing (Wei et al., 2022; Zhou et al., 2022) and mathe-\n",
       "∗Corresponding author.\n",
       "UnknowsKnows UnknowsKnows\n",
       "Known KnowsKnown Unknows\n",
       "Unknown Unknows Unknown KnowsUnlockFigure 1: Know-Unknow Quadrant. The horizontal axis\n",
       "represents the model’s memory capacity for knowledge,\n",
       "and the vertical axis represents the model’s ability to\n",
       "comprehend and utilize knowledge.\n",
       "matical problem-solving (Lewkowycz et al., 2022;\n",
       "Chen et al., 2022). Despite their ability to learn\n",
       "from huge amounts of data, LLMs still have lim-\n",
       "itations in their capacity to retain and understand\n",
       "information. To ensure responsible usage, it is cru-\n",
       "cial for LLMs to have the capability of recognizing\n",
       "their limitations and conveying uncertainty when\n",
       "responding to unanswerable or unknowable ques-\n",
       "tions. This acknowledgment of limitations, also\n",
       "known as “ knowing what you don’t know ,” is a\n",
       "crucial aspect in determining their practical appli-\n",
       "cability. In this work, we refer to this ability as\n",
       "model self-knowledge.\n",
       "The Know-Unknow quadrant in Figure 1 il-\n",
       "lustrates the relationship between the model’s\n",
       "knowledge and comprehension. The ratio of\n",
       "“Known Knows” to “Unknown Knows” demon-\n",
       "strates the model’s proficiency in understanding\n",
       "and applying existing knowledge. Techniques\n",
       "such as Chain-of-Thought (Wei et al., 2022), Self-\n",
       "Consistency (Wang et al., 2022), and Complex\n",
       "CoT (Fu et al., 2022) can be utilized to increasearXiv:2305.18153v2  [cs.CL]  30 May 2023"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "Markdown(doc_obj.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x107f70090>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x11fe67a90>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='https://api.openai.com/v1', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings() # EMBED\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x148c572d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(pdf_docs, embedding=embeddings) # STORE\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x148c572d0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever() \n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-0125-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow can I be a better instructor for my live-trainings about AI to get everyone excited about LLMs?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    167\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    168\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    169\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    170\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    171\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    172\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    173\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    174\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    399\u001b[0m                 m,\n\u001b[1;32m    400\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    401\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    402\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    403\u001b[0m             )\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    578\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    579\u001b[0m     )\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:462\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    457\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    461\u001b[0m }\n\u001b[0;32m--> 462\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/openai/resources/chat/completions.py:663\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    662\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    665\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    666\u001b[0m             {\n\u001b[1;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    687\u001b[0m             },\n\u001b[1;32m    688\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    689\u001b[0m         ),\n\u001b[1;32m    690\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    691\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    692\u001b[0m         ),\n\u001b[1;32m    693\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    694\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    695\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    696\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    890\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    891\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    892\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    893\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    894\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    895\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/openai/_base_client.py:918\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    915\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 918\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    919\u001b[0m         request,\n\u001b[1;32m    920\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m    921\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    922\u001b[0m     )\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    924\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/ssl.py:1295\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(buflen)\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-env/lib/python3.11/ssl.py:1168\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm.invoke(\"How can I be a better instructor for my live-trainings about AI to get everyone excited about LLMs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x175716990>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1756cbf50>, model_name='gpt-4-0125-preview', openai_api_key=SecretStr('**********'), openai_api_base='https://api.openai.com/v1', openai_proxy='')), document_prompt=PromptTemplate(input_variables=['page_content'], template='Context:\\n{page_content}'), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x148c572d0>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_qa = RetrievalQA.from_llm(llm=llm, retriever=retriever, return_source_documents=True) # RETRIEVE\n",
    "pdf_qa\n",
    "# pdf_qa = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=retriever) # RETRIEVE\n",
    "# pdf_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarize this paper into a set of instructive bullet points.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pdf_qa.invoke({\"query\": query, \"chat_history\": []}) # adding chat history so the model remembers previous questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Summarize this paper into a set of instructive bullet points.',\n",
       " 'chat_history': [],\n",
       " 'result': \"- The study evaluates the self-knowledge of Large Language Models (LLMs) such as GPT-3, InstructGPT, LLaMA, and their derivatives (Alpaca, Vicuna) by assessing their ability to identify unanswerable questions.\\n- Utilized the SelfAware dataset, which includes a mix of unanswerable and answerable questions from various sources, to measure models' performance.\\n- Developed an automated method for detecting uncertainty in model responses, using a set of reference sentences displaying uncertainty and computing similarity through SimCSE.\\n- Conducted experiments with different input forms (Direct, Instruction, and In-Context Learning (ICL)) to see their impact on models' self-knowledge.\\n- Found that models from the InstructGPT series and their instruction-tuned derivatives (Alpaca, Vicuna) showed improved self-knowledge over GPT-3.\\n- ICL input form significantly enhanced models' self-knowledge by providing richer contextual information.\\n- Compared model performance to human benchmarks, finding that while models like GPT-4 showed high F1 scores, they still lag behind human self-knowledge.\\n- Observed a steady improvement in the InstructGPT series' performance in answering answerable questions with increasing model size and continuous learning.\\n- Highlighted limitations including the generalization of reference sentences and the confinement to specific input forms; suggests future research directions like incorporating more cognitive and decision-making methods.\\n- Concluded that while LLMs demonstrate a degree of self-knowledge in identifying unanswerable questions, there's substantial room for enhancement to match human capabilities.\\n- Emphasized the importance of advancing LLMs' self-knowledge for more accurate and reliable applications across various domains.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- The study evaluates the self-knowledge of Large Language Models (LLMs) such as GPT-3, InstructGPT, LLaMA, and their derivatives (Alpaca, Vicuna) by assessing their ability to identify unanswerable questions.\n",
       "- Utilized the SelfAware dataset, which includes a mix of unanswerable and answerable questions from various sources, to measure models' performance.\n",
       "- Developed an automated method for detecting uncertainty in model responses, using a set of reference sentences displaying uncertainty and computing similarity through SimCSE.\n",
       "- Conducted experiments with different input forms (Direct, Instruction, and In-Context Learning (ICL)) to see their impact on models' self-knowledge.\n",
       "- Found that models from the InstructGPT series and their instruction-tuned derivatives (Alpaca, Vicuna) showed improved self-knowledge over GPT-3.\n",
       "- ICL input form significantly enhanced models' self-knowledge by providing richer contextual information.\n",
       "- Compared model performance to human benchmarks, finding that while models like GPT-4 showed high F1 scores, they still lag behind human self-knowledge.\n",
       "- Observed a steady improvement in the InstructGPT series' performance in answering answerable questions with increasing model size and continuous learning.\n",
       "- Highlighted limitations including the generalization of reference sentences and the confinement to specific input forms; suggests future research directions like incorporating more cognitive and decision-making methods.\n",
       "- Concluded that while LLMs demonstrate a degree of self-knowledge in identifying unanswerable questions, there's substantial room for enhancement to match human capabilities.\n",
       "- Emphasized the importance of advancing LLMs' self-knowledge for more accurate and reliable applications across various domains."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_answer = output[\"result\"]\n",
    "\n",
    "Markdown(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_summary = \"What is the main discovery of the paper regarding self-knowledge in LLMs?\"\n",
    "\n",
    "output = pdf_qa.invoke({\"query\": query_summary, \"chat_history\": []}) # adding chat history so the model remembers previous questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the main discovery of the paper regarding self-knowledge in LLMs?',\n",
       " 'chat_history': [],\n",
       " 'result': 'The main discovery of the paper regarding self-knowledge in Large Language Models (LLMs) is that these models inherently possess an intrinsic capacity for self-knowledge. This capacity is demonstrated through their ability to identify unanswerable or unknowable questions to some extent. The paper also finds that techniques such as in-context learning and instruction tuning can further enhance the self-knowledge of LLMs. Despite these promising insights, the study highlights a considerable gap between the capabilities of LLMs and human proficiency in recognizing the limits of their knowledge, with the state-of-the-art model, GPT-4, exhibiting self-knowledge at 75.47% compared to human self-knowledge rated at 84.93%.',\n",
       " 'source_documents': [Document(page_content='Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢Qiushi Sun♠Qipeng Guo♢\\nJiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware , consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗Corresponding author.\\nUnknowsKnows UnknowsKnows\\nKnown KnowsKnown Unknows\\nUnknown Unknows Unknown KnowsUnlockFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know ,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increasearXiv:2305.18153v2  [cs.CL]  30 May 2023', metadata={'page': 0, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}),\n",
       "  Document(page_content='Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢Qiushi Sun♠Qipeng Guo♢\\nJiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware , consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗Corresponding author.\\nUnknowsKnows UnknowsKnows\\nKnown KnowsKnown Unknows\\nUnknown Unknows Unknown KnowsUnlockFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know ,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increasearXiv:2305.18153v2  [cs.CL]  30 May 2023', metadata={'page': 0, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}),\n",
       "  Document(page_content='this ratio, resulting in improved performance on\\nNLP tasks. We focus on the ratio of “Known Un-\\nknows” to “Unknown Unknows”, which indicates\\nthe model’s self-knowledge level, specifically un-\\nderstanding its own limitations and deficiencies in\\nthe unknows.\\nExisting datasets such as SQuAD2.0 (Rajpurkar\\net al., 2018) and NewsQA (Trischler et al., 2017),\\nwidely used in question answering (QA), have been\\nutilized to test the self-knowledge of models with\\nunanswerable questions. However, these questions\\nare context-specific and could become answerable\\nwhen supplemented with additional information.\\nSrivastava et al. (2022) attempted to address this by\\nevaluating LLMs’ competence in delineating their\\nknowledge boundaries, employing a set of 23 pairs\\nof answerable and unanswerable multiple-choice\\nquestions. They discovered that these models’ per-\\nformance barely surpassed that of random guessing.\\nKadavath et al. (2022) suggested probing the self-\\nknowledge of LLMs through the implementation\\nof a distinct \"Value Head\". Yet, this approach may\\nencounter difficulties when applied across varied\\ndomains or tasks due to task-specific training. Con-\\nsequently, we redirect our focus to the inherent\\nabilities of LLMs, and pose the pivotal question:\\n“Do large language models know what they don’t\\nknow? ”.\\nIn this study, we investigate the self-knowledge\\nof LLMs using a novel approach. By gathering\\nreference sentences with uncertain meanings, we\\ncan determine whether the model’s responses re-\\nflect uncertainty using a text similarity algorithm.\\nWe quantified the model’s self-knowledge using\\nthe F1 score. To address the small and idiosyn-\\ncratic limitations of existing datasets, we created\\na new dataset called SelfAware . This dataset com-\\nprises 1,032 unanswerable questions, which are dis-\\ntributed across five distinct categories, along with\\nan additional 2,337 questions that are classified as\\nanswerable. Experimental results on GPT-3, In-\\nstructGPT, LLaMA, and other LLMs demonstrate\\nthat in-context learning and instruction tuning can\\neffectively enhance the self-knowledge of LLMs.\\nHowever, the self-knowledge exhibited by the cur-\\nrent state-of-the-art model, GPT-4, measures at\\n75.47%, signifying a notable disparity when con-\\ntrasted with human self-knowledge, which is rated\\nat 84.93%.\\nOur key contributions to this field are summa-\\nrized as follows:•We have developed a new dataset, SelfAware ,\\nthat comprises a diverse range of commonly\\nposed unanswerable questions.\\n•We propose an innovative evaluation tech-\\nnique based on text similarity to quantify the\\ndegree of uncertainty inherent in model out-\\nputs.\\n•Through our detailed analysis of 20 LLMs,\\nbenchmarked against human self-knowledge,\\nwe identified a significant disparity between\\nthe most advanced LLMs and humans1.\\n2 Dataset Construction\\nTo conduct a more comprehensive evaluation of\\nthe model’s self-knowledge, we constructed a\\ndataset that includes a larger number and more di-\\nverse types of unanswerable questions than Know-\\nUnknowns dataset (Srivastava et al., 2022). To\\nfacilitate this, we collected a corpus of 2,858 unan-\\nswerable questions, sourced from online platforms\\nlike Quora and HowStuffWorks. These questions\\nwere meticulously evaluated by three seasoned an-\\nnotation analysts, each operating independently.\\nThe analysts were permitted to leverage external\\nresources, such as search engines. To ensure the va-\\nlidity of our dataset, we retained only the questions\\nthat all three analysts concurred were unanswerable.\\nThis rigorous process yielded a finalized collection\\nof 1,032 unanswerable questions.\\nIn pursuit of a comprehensive evaluation, we\\nopted for answerable questions drawn from three\\ndatasets: SQuAD (Rajpurkar et al., 2016), Hot-\\npotQA (Yang et al., 2018), and TriviaQA (Joshi\\net al., 2017). Our selection was guided by Sim-\\nCSE (Gao et al., 2021), which allowed us to iden-\\ntify and select the answerable questions semanti-\\ncally closest to the unanswerable ones. From these', metadata={'page': 1, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}),\n",
       "  Document(page_content='this ratio, resulting in improved performance on\\nNLP tasks. We focus on the ratio of “Known Un-\\nknows” to “Unknown Unknows”, which indicates\\nthe model’s self-knowledge level, specifically un-\\nderstanding its own limitations and deficiencies in\\nthe unknows.\\nExisting datasets such as SQuAD2.0 (Rajpurkar\\net al., 2018) and NewsQA (Trischler et al., 2017),\\nwidely used in question answering (QA), have been\\nutilized to test the self-knowledge of models with\\nunanswerable questions. However, these questions\\nare context-specific and could become answerable\\nwhen supplemented with additional information.\\nSrivastava et al. (2022) attempted to address this by\\nevaluating LLMs’ competence in delineating their\\nknowledge boundaries, employing a set of 23 pairs\\nof answerable and unanswerable multiple-choice\\nquestions. They discovered that these models’ per-\\nformance barely surpassed that of random guessing.\\nKadavath et al. (2022) suggested probing the self-\\nknowledge of LLMs through the implementation\\nof a distinct \"Value Head\". Yet, this approach may\\nencounter difficulties when applied across varied\\ndomains or tasks due to task-specific training. Con-\\nsequently, we redirect our focus to the inherent\\nabilities of LLMs, and pose the pivotal question:\\n“Do large language models know what they don’t\\nknow? ”.\\nIn this study, we investigate the self-knowledge\\nof LLMs using a novel approach. By gathering\\nreference sentences with uncertain meanings, we\\ncan determine whether the model’s responses re-\\nflect uncertainty using a text similarity algorithm.\\nWe quantified the model’s self-knowledge using\\nthe F1 score. To address the small and idiosyn-\\ncratic limitations of existing datasets, we created\\na new dataset called SelfAware . This dataset com-\\nprises 1,032 unanswerable questions, which are dis-\\ntributed across five distinct categories, along with\\nan additional 2,337 questions that are classified as\\nanswerable. Experimental results on GPT-3, In-\\nstructGPT, LLaMA, and other LLMs demonstrate\\nthat in-context learning and instruction tuning can\\neffectively enhance the self-knowledge of LLMs.\\nHowever, the self-knowledge exhibited by the cur-\\nrent state-of-the-art model, GPT-4, measures at\\n75.47%, signifying a notable disparity when con-\\ntrasted with human self-knowledge, which is rated\\nat 84.93%.\\nOur key contributions to this field are summa-\\nrized as follows:•We have developed a new dataset, SelfAware ,\\nthat comprises a diverse range of commonly\\nposed unanswerable questions.\\n•We propose an innovative evaluation tech-\\nnique based on text similarity to quantify the\\ndegree of uncertainty inherent in model out-\\nputs.\\n•Through our detailed analysis of 20 LLMs,\\nbenchmarked against human self-knowledge,\\nwe identified a significant disparity between\\nthe most advanced LLMs and humans1.\\n2 Dataset Construction\\nTo conduct a more comprehensive evaluation of\\nthe model’s self-knowledge, we constructed a\\ndataset that includes a larger number and more di-\\nverse types of unanswerable questions than Know-\\nUnknowns dataset (Srivastava et al., 2022). To\\nfacilitate this, we collected a corpus of 2,858 unan-\\nswerable questions, sourced from online platforms\\nlike Quora and HowStuffWorks. These questions\\nwere meticulously evaluated by three seasoned an-\\nnotation analysts, each operating independently.\\nThe analysts were permitted to leverage external\\nresources, such as search engines. To ensure the va-\\nlidity of our dataset, we retained only the questions\\nthat all three analysts concurred were unanswerable.\\nThis rigorous process yielded a finalized collection\\nof 1,032 unanswerable questions.\\nIn pursuit of a comprehensive evaluation, we\\nopted for answerable questions drawn from three\\ndatasets: SQuAD (Rajpurkar et al., 2016), Hot-\\npotQA (Yang et al., 2018), and TriviaQA (Joshi\\net al., 2017). Our selection was guided by Sim-\\nCSE (Gao et al., 2021), which allowed us to iden-\\ntify and select the answerable questions semanti-\\ncally closest to the unanswerable ones. From these', metadata={'page': 1, 'source': './assets-resources/llm_paper_know_dont_know.pdf'})]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Long context issue in LLMs](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "\n",
    "Long Context. One of the main drawbacks of Transformerbased language models is the context length is limited due to the involved quadratic computational costs in both time\n",
    "and memory. Meanwhile, there is an increasing demand\n",
    "for LLM applications with long context windows, such as\n",
    "in PDF processing and story writing [217]. ChatGPT has\n",
    "recently released an updated variant with a context window\n",
    "size of up to 16K tokens, which is much longer than the\n",
    "initial one, i.e., 4K tokens. Additionally, GPT-4 was launched\n",
    "with variants with context window of 32K tokens [46]. Next,\n",
    "we discuss two important factors that support long context\n",
    "modeling for LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb \n",
    "Below are notebook from openai cookbook on these topics of search and embeddings:\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Code_search.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb\n",
    "- https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "- [In-context learning abilities of ChatGPT models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "- [Issue with long context](https://arxiv.org/pdf/2303.18223.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-env",
   "language": "python",
   "name": "oreilly-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
