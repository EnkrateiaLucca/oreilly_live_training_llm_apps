{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-07-24-10-52-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple QA System for Chatting with a PDF\n",
    "\n",
    "This part of the training will be mostly hands on with the code for building the qa PDF system with langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-openai\n",
    "!pip install langchainhub\n",
    "!pip install pypdf\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# # Set OPENAI API Key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your openai key\"\n",
    "\n",
    "# OR (load from .env file)\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./assets-resources/llm_paper_know_dont_know.pdf\"\n",
    "loader = PyPDFLoader(pdf_path) # LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 0}, page_content='Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢ Qiushi Sun♠ Qipeng Guo♢\\nJiawen Wu♢ Xipeng Qiu♢∗ Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware, consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗ Corresponding author.\\nUnknows\\nKnowsUnknows\\nKnows\\nKnown Knows Known Unknows\\nUnknown UnknowsUnknown Knows\\nUnlock\\nFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increase\\narXiv:2305.18153v2  [cs.CL]  30 May 2023'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 1}, page_content='this ratio, resulting in improved performance on\\nNLP tasks. We focus on the ratio of “Known Un-\\nknows” to “Unknown Unknows”, which indicates\\nthe model’s self-knowledge level, specifically un-\\nderstanding its own limitations and deficiencies in\\nthe unknows.\\nExisting datasets such as SQuAD2.0 (Rajpurkar\\net al., 2018) and NewsQA (Trischler et al., 2017),\\nwidely used in question answering (QA), have been\\nutilized to test the self-knowledge of models with\\nunanswerable questions. However, these questions\\nare context-specific and could become answerable\\nwhen supplemented with additional information.\\nSrivastava et al. (2022) attempted to address this by\\nevaluating LLMs’ competence in delineating their\\nknowledge boundaries, employing a set of 23 pairs\\nof answerable and unanswerable multiple-choice\\nquestions. They discovered that these models’ per-\\nformance barely surpassed that of random guessing.\\nKadavath et al. (2022) suggested probing the self-\\nknowledge of LLMs through the implementation\\nof a distinct \"Value Head\". Yet, this approach may\\nencounter difficulties when applied across varied\\ndomains or tasks due to task-specific training. Con-\\nsequently, we redirect our focus to the inherent\\nabilities of LLMs, and pose the pivotal question:\\n“Do large language models know what they don’t\\nknow?”.\\nIn this study, we investigate the self-knowledge\\nof LLMs using a novel approach. By gathering\\nreference sentences with uncertain meanings, we\\ncan determine whether the model’s responses re-\\nflect uncertainty using a text similarity algorithm.\\nWe quantified the model’s self-knowledge using\\nthe F1 score. To address the small and idiosyn-\\ncratic limitations of existing datasets, we created\\na new dataset called SelfAware. This dataset com-\\nprises 1,032 unanswerable questions, which are dis-\\ntributed across five distinct categories, along with\\nan additional 2,337 questions that are classified as\\nanswerable. Experimental results on GPT-3, In-\\nstructGPT, LLaMA, and other LLMs demonstrate\\nthat in-context learning and instruction tuning can\\neffectively enhance the self-knowledge of LLMs.\\nHowever, the self-knowledge exhibited by the cur-\\nrent state-of-the-art model, GPT-4, measures at\\n75.47%, signifying a notable disparity when con-\\ntrasted with human self-knowledge, which is rated\\nat 84.93%.\\nOur key contributions to this field are summa-\\nrized as follows:\\n• We have developed a new dataset,SelfAware,\\nthat comprises a diverse range of commonly\\nposed unanswerable questions.\\n• We propose an innovative evaluation tech-\\nnique based on text similarity to quantify the\\ndegree of uncertainty inherent in model out-\\nputs.\\n• Through our detailed analysis of 20 LLMs,\\nbenchmarked against human self-knowledge,\\nwe identified a significant disparity between\\nthe most advanced LLMs and humans 1.\\n2 Dataset Construction\\nTo conduct a more comprehensive evaluation of\\nthe model’s self-knowledge, we constructed a\\ndataset that includes a larger number and more di-\\nverse types of unanswerable questions than Know-\\nUnknowns dataset (Srivastava et al., 2022). To\\nfacilitate this, we collected a corpus of 2,858 unan-\\nswerable questions, sourced from online platforms\\nlike Quora and HowStuffWorks. These questions\\nwere meticulously evaluated by three seasoned an-\\nnotation analysts, each operating independently.\\nThe analysts were permitted to leverage external\\nresources, such as search engines. To ensure the va-\\nlidity of our dataset, we retained only the questions\\nthat all three analysts concurred were unanswerable.\\nThis rigorous process yielded a finalized collection\\nof 1,032 unanswerable questions.\\nIn pursuit of a comprehensive evaluation, we\\nopted for answerable questions drawn from three\\ndatasets: SQuAD (Rajpurkar et al., 2016), Hot-\\npotQA (Yang et al., 2018), and TriviaQA (Joshi\\net al., 2017). Our selection was guided by Sim-\\nCSE (Gao et al., 2021), which allowed us to iden-\\ntify and select the answerable questions semanti-\\ncally closest to the unanswerable ones. From these'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 1}, page_content='et al., 2017). Our selection was guided by Sim-\\nCSE (Gao et al., 2021), which allowed us to iden-\\ntify and select the answerable questions semanti-\\ncally closest to the unanswerable ones. From these\\nsources, we accordingly drew samples of 1,487,\\n182, and 668 questions respectively, amassing a\\ntotal of 2,337. Given that these questions can be\\neffectively addressed using information available\\non Wikipedia, the foundational corpus for the train-\\ning of current LLMs, it is plausible to infer that\\nthe model possesses the requisite knowledge to\\ngenerate accurate responses to these questions.\\nOur dataset, christened SelfAware, incorporates\\n1,032 unanswerable and 2,337 answerable ques-\\ntions. To reflect real-world distribution, our dataset\\n1The code pertinent to our study can be accessed\\nhttps://github.com/yinzhangyue/SelfAware'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 2}, page_content='Category Description Example Percentage\\nNo scientific\\nconsensus\\nThe answer is still up\\nfor debate, with no consensus\\nin scientific community.\\n“Are we alone in the universe,\\nor will we discover alien\\nlife at some point?”\\n25%\\nImagination The question are about people’s\\nimaginations of the future.\\n\"What will the fastest form of\\ntransportation be in 2050?\" 15%\\nCompletely\\nsubjective\\nThe answer depends on\\npersonal preference.\\n\"Would you rather be shot\\ninto space or explore the\\ndeepest depths of the sea?\"\\n27%\\nToo many\\nvariables\\nThe question with too\\nmany variables cannot\\nbe answered accurately.\\n“John made 6 dollars mowing lawns\\nand 18 dollars weed eating.\\nIf he only spent 3 or 5 dollar a week,\\nhow long would the money last him?”\\n10%\\nPhilosophical\\nThe question can yield\\nmultiple responses, but it\\nlacks a definitive answer.\\n“How come god was\\nborn from nothingness?” 23%\\nTable 1: Unanswerable questions in the SelfAware dataset that span across multiple categories.\\ncontains a proportion of answerable questions that\\nis twice as large as the volume of unanswerable\\nones. Nevertheless, to ensure the feasibility of test-\\ning, we have purposefully capped the number of\\nanswerable questions.\\n2.1 Dataset Analysis\\nTo gain insight into the reasons precluding a cer-\\ntain answer, we undertook a manual analysis of\\n100 randomly selected unanswerable questions. As\\ntabulated in Table 1, we have broadly segregated\\nthese questions into five distinctive categories. “No\\nScientific Consensus\" encapsulates questions that\\nignite ongoing debates within the scientific com-\\nmunity, such as those concerning the universe’s\\norigin. “Imagination\" includes questions involving\\nspeculative future scenarios, like envisaged events\\nover the next 50 years. “Completely Subjective\"\\ncomprises questions that are inherently personal,\\nwhere answers depend heavily on individual predis-\\npositions. “Too Many Variables\" pertains to mathe-\\nmatical problems that become unsolvable owing to\\nthe overwhelming prevalence of variables. Lastly,\\n“Philosophical\" represents questions of a profound,\\noften metaphysical, nature that resist concrete an-\\nswers. Ideally, upon encountering such questions,\\nthe model should express uncertainty instead of\\ndelivering conclusive responses.\\n3 Evaluation Method\\nThis section elucidates the methodology employed\\nfor assessing self-knowledge in the generated text.\\nIn order to achieve this, we define a similarity func-\\ntion, fsim, to compute the similarity, S, between\\na given sentence, t, and a collection of reference\\nsentences, U = {u1, u2, . . . , un}, endowed with\\nuncertain meanings.\\nSi = fsim(t, ui). (1)\\nWhenever any Si surpasses a pre-determined\\nthreshold T , we perceive the text t as encompass-\\ning uncertain meanings, thereby eliminating the\\nneed for manual evaluation of the response.\\nGiven the substantial disparity in the volume of\\nanswerable and unanswerable questions in Self-\\nAware, we adopt the F1 score as a measure of\\nLLMs’ self-knowledge. Our focus rests on identi-\\nfying unanswerable questions, hence we designate\\nthem as positive cases and categorize answerable\\nquestions as negative cases.\\n4 Experiment\\n4.1 Model\\nWe conduct a sequence of experiments to evaluate\\nthe degree of self-knowledge manifested by various\\nLLMs, including GPT-3 (Brown et al., 2020) and\\nInstructGPT (Ouyang et al., 2022) series, as well\\nas the recent LLaMA (Touvron et al., 2023) and\\nits derivative models, namely Alpaca (Taori et al.,\\n2023) and Vicuna (Chiang et al., 2023). Our in-\\nvestigative approach employed three distinct input\\nforms: Direct, Instruction, and In-Context Learn-\\ning (ICL), which is encapsulated in Appendix A.4.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 3}, page_content='350M 1.3B 6.7B 175B\\n20\\n30\\n40\\n50\\n60\\n70F1 Scores\\n22.38\\n40.11\\n26.96\\n40.33\\n26.17\\n43.47\\n27.54\\n44.87\\nDirect\\n350M 1.3B 6.7B 175B\\n20\\n30\\n40\\n50\\n60\\n70F1 Scores\\n30.42\\n42.31\\n30.17\\n45.91\\n33.33\\n48.79\\n45.67\\n49.61\\nInstruction\\n350M 1.3B 6.7B 175B\\n20\\n30\\n40\\n50\\n60\\n70F1 Scores\\n34.27\\n47.93\\n36.27\\n48.42 47.24\\n55.81 55.5\\n65.12\\nIn-Context Learning\\nGPT-3\\nInstructGPT\\nModel\\nFigure 2: Experimental results using three different input forms on a series of models from GPT-3(ada, babbage,\\ncurie, and davinci) and InstructGPT(text-ada-001, text-babbage-001, text-curie-001, and text-davinci-001)\\n0 10 20 30 40 50 60 70 80\\nF1 Scores\\ndavinci\\ntext-davinci-001\\ntext-davinci-002\\ntext-davinci-003\\ngpt-3.5-turbo-0301\\ngpt-4-0314\\nHuman\\nModels\\n45.67\\n49.61\\n47.48\\n51.43\\n54.12\\n75.47\\n84.93\\nFigure 3: Comparison between the davinci series and\\nhuman self-knowledge in instruction input form.\\n4.2 Setting\\nWe devised the reference sentence set U through\\na process that combined automated generation by\\nLLMs and manual filtering, detailed further in Ap-\\npendix A.1. To quantify the similarity between\\ntarget and reference sentences, we utilized Sim-\\nCSE (Gao et al., 2021), setting the similarity thresh-\\nold to 0.75 during our experiments. An exploration\\nof threshold ablation is available in Appendix A.2.\\nTo counteract potential errors in similarity calcula-\\ntion induced by varying lengths of the target and\\nreference sentences, we employed a sliding win-\\ndow of length 5 to parse the target sentence into\\nsemantic chunks. During the generation process,\\nwe set the temperature to 0.7. We selected a ran-\\ndom sample of 100 instances for GPT-4, while the\\nremainder of the models were scrutinized using the\\nfull SelfAware dataset.\\n4.3 Human Self-Knowledge\\nTo establish a benchmark for human self-\\nknowledge, we engaged two volunteers and se-\\nlected 100 random samples from the SelfAware\\ndataset. The volunteers has 30 minutes to make\\ndavinci\\ntext-davinci-001text-davinci-002text-davinci-003\\ngpt-3.5-turbo-0301\\nModels\\n0\\n10\\n20\\n30\\n40\\n50\\n60F1 Scores\\n55.5\\n65.12 66.46 66.28\\n60.86\\nFigure 4: Experimental comparison of davinci series in\\nICL input form.\\njudgments on the same set of questions, yielding\\nan average F1 score of 84.93%, which we sub-\\nsequently adopted as the benchmark for human\\nself-knowledge. Detailed scores are available in\\nAppendix A.3.\\n4.4 Analysis\\nWe evaluate the manifestation of LLMs’ self-\\nknowledge, centering our investigation on three\\nfundamental dimensions: the size of the model,\\nthe impact of instruction tuning, and the influence\\nexerted by different input forms.\\nModel Size. Figure 2 illustrates the correlation\\nbetween model size and self-knowledge across var-\\nious LLMs. It is noteworthy that across all three\\ninput forms, an augmentation in model parameter\\nsize is associated with an elevation in the F1 Score,\\nwith the most conspicuous enhancement manifest-\\ning in the ICL input form. Therefore, our analysis\\nindicates that an LLM’s self-knowledge tends to\\nenhance with increasing model size, a trend consis-\\ntent with the scaling law.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 4}, page_content='LLaMA-7B Alpaca-7B Vicuna-7B LLaMA-13B Alpaca-13B Vicuna-13B LLaMA-30B LLaMA-65B\\nModels\\n0\\n10\\n20\\n30\\n40\\n50F1 Scores\\n28.57\\n35.87\\n42.78\\n30.12\\n37.44\\n47.84\\n30.3\\n46.89\\nFigure 5: Experimental results obtained from LLaMA\\nand its derived models, Alpaca and Vicuna in instruction\\ninput form.\\nInstruction Tuning. Figure 2 delineates that\\nmodels from the InstructGPT series exhibit a su-\\nperior level of self-knowledge compared to their\\nGPT-3 counterparts. Further evidence of model\\nenhancement is provided by Figure 4, where text-\\ndavinci models show significant improvement rela-\\ntive to the base davinci model. An additional com-\\nparative analysis, presented in Figure 5, evaluates\\nLLaMA against its derivative models. The results\\nunderscore a notable increase in self-knowledge\\nfor Alpaca and Vicuna upon instruction tuning, ex-\\nceeding their base model performances. Among\\nthese, Vicuna-13B outperforms the LLaMA-65B,\\ncorroborating the efficacy of instruction tuning for\\nenhancing model self-knowledge.\\nInput Forms. As shown in Figure 2, the incorpo-\\nration of instructions and examples serves to boost\\nthe self-knowledge of both the GPT-3 and Instruct-\\nGPT series. Specifically, ICL input form, providing\\nricher contextual information, contributes to a sig-\\nnificant enhancement in models’ self-knowledge.\\nThis impact is particularly noticeable in the davinci\\nmodel, where ICL facilitates a 27.96% improve-\\nment over the direct. Moreover, a comparison be-\\ntween Figure 3 and Figure 4 reveals that the in-\\nclusion of instructions and examples successfully\\nminimizes the performance disparity between the\\ndavinci and text-davinci models, suggesting an ac-\\nquisition of self-knowledge from the instructions\\nand provided examples.\\nCompared with Human. Figure 3 reveals that,\\nwithout supplementary samples, GPT-4 currently\\nperforms best among the tested models, achieving\\nan impressive F1 score of 75.47%. However, a no-\\nticeable gap becomes evident when comparing this\\ntext-ada-001\\ntext-babbage-001\\ntext-curie-001 text-davinci-001 text-davinci-002 text-davinci-003\\ngpt-3.5-turbo-0301\\ngpt-4-0314\\nModels\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40Accuracy\\n2.48\\n4.45 4.7\\n10.61\\n15.7\\n30.25\\n38.29\\n42.64\\nFigure 6: Accuracy of the InstructGPT series when\\nresponding to answerable questions in instruction input\\nform.\\nperformance to the human benchmark of 84.93%.\\nThis underscores the considerable potential that re-\\nmains for enhancing the self-knowledge level of\\nLLMs.\\nAnswerable Questions. Figure 6 traces the per-\\nformance evolution of the InstructGPT series in\\naddressing answerable questions, adhering to the\\nclosed-book question answering paradigm (Tou-\\nvron et al., 2023), where output accuracy is con-\\ntingent on the presence of the correct answer. Our\\nobservations underscore a steady enhancement in\\nQA task accuracy corresponding to an increase\\nin model parameter size and continuous learning.\\nParticularly, the accuracy of text-davinci-001 expe-\\nriences a significant ascent, scaling from a meager\\n2.48% in text-ada-001 to 10.61%, whereas GPT-4\\nmarks an even more striking jump to 42.64%.\\n5 Conclusion\\nThis study investigates the self-knowledge of\\nLLMs by evaluating their ability to identify unan-\\nswerable questions. Through the introduction of a\\nnovel dataset and an automated method for detect-\\ning uncertainty in the models’ responses, we are\\nable to accurately measure the self-knowledge of\\nLLMs such as GPT-3, InstructGPT and LLaMA.\\nOur results reveal that while these models possess\\na certain degree of self-knowledge, there is still\\nan apparent disparity in comparison to human self-\\nknowledge. This highlights the need for further\\nresearch in this area to enhance the ability of LLMs\\nto understand their own limitations on the unknows.\\nSuch efforts will lead to more accurate and reliable\\nresponses from LLMs, which will have a positive\\nimpact on their applications in diverse fields.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 5}, page_content='Limitations\\n• Generalization of reference sentences.At\\npresent, we have selected sentences with un-\\ncertain meanings exclusively from the GPT-3\\nand InstructGPT series, potentially overlook-\\ning uncertainty present in responses generated\\nby other LLMs. However, it is not feasible\\nto catalog all sentences with uncertain mean-\\nings exhaustively. As a direction for future\\nresearch, we propose to concentrate on the\\nautomated acquisition of more accurate refer-\\nence sentences to address this concern.\\n• Limitations of input forms: Our exami-\\nnation was confined to three unique input\\nforms: direct, instruction, and ICL. There\\nis burgeoning research aimed at bridging the\\ngap between models and human-like meth-\\nods of reasoning and problem-solving, includ-\\ning but not limited to approaches like Re-\\nflexion (Shinn et al., 2023), ToT (Yao et al.,\\n2023), MoT (Li and Qiu, 2023). Future en-\\ndeavors will integrate additional cognitive and\\ndecision-making methods to delve deeper into\\nthe self-knowledge exhibited by these LLMs.\\nEthics Statement\\nThe SelfAware dataset, meticulously curated to\\nevaluate LLMs’ ability to discern unanswerable\\nquestions, is composed of unanswerable questions\\nextracted from sources such as Quora and How-\\nStuffWorks, alongside answerable questions pro-\\ncured from three distinct open datasets. Every ques-\\ntion was thoroughly examined for relevance and\\nharmlessness. To ensure content validity, three an-\\nnotation analysts, compensated at local wage stan-\\ndards, dedicated regular working hours to content\\nreview.\\nThroughout our research process, we under-\\nscored the significance of privacy, data security,\\nand strict compliance with dataset licenses. In\\norder to protect data integrity, we implemented\\nanonymization and content filtration mechanisms.\\nOur adherence to OpenAI’s stipulations remained\\nunyielding for the usage of GPT-3 and InstructGPT\\nmodels, and likewise for Meta’s terms pertaining\\nto LLaMA models. We rigorously vetted the li-\\ncenses of the three publicly available datasets for\\ncompliance, ensuring that all our research method-\\nologies were in alignment with ethical standards at\\nthe institutional, national, and global levels.\\nAdhering to the CC-BY-SA-4.0 protocol, the\\ndataset, once publicly released, will be reserved\\nexclusively for research purposes. We pledge to\\npromptly and effectively address any concerns relat-\\ning to the dataset, while concurrently anticipating\\nresearchers to maintain high ethical standards in\\ntheir utilization of this data.\\nAcknowledgement\\nWe wish to express our gratitude to our colleagues\\nin the FudanNLP group whose insightful sugges-\\ntions, perspectives, and thought-provoking discus-\\nsions significantly contributed to this work. Our\\nsincere appreciation also extends to the anonymous\\nreviewers and area chairs, whose constructive feed-\\nback was instrumental in refining the quality of\\nour study. This work was supported by the Na-\\ntional Natural Science Foundation of China (No.\\n62236004 and No. 62022027) and CAAI-Huawei\\nMindSpore Open Fund.\\nReferences\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\\nChen, Eric Chu, Jonathan H. Clark, Laurent El\\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\\nJan Botha, James Bradbury, Siddhartha Brahma,\\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\\nCherry, Christopher A. Choquette-Choo, Aakanksha\\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 5}, page_content='cia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\\nShelby, Ambrose Slone, Daniel Smilkov, David R.\\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\\nVijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,\\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 6}, page_content='ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\\nreport.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In Ad-\\nvances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Process-\\ning Systems 2020, NeurIPS 2020, December 6-12,\\n2020, virtual.\\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\\nWilliam W Cohen. 2022. Program of thoughts\\nprompting: Disentangling computation from reason-\\ning for numerical reasoning tasks. ArXiv preprint,\\nabs/2211.12588.\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\\nsource chatbot impressing gpt-4 with 90%* chatgpt\\nquality.\\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\\nand Tushar Khot. 2022. Complexity-based prompt-\\ning for multi-step reasoning. ArXiv preprint,\\nabs/2210.00720.\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\\nSimCSE: Simple contrastive learning of sentence em-\\nbeddings. In Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Process-\\ning, pages 6894–6910, Online and Punta Cana, Do-\\nminican Republic. Association for Computational\\nLinguistics.\\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pages 1601–1611, Vancouver,\\nCanada. Association for Computational Linguistics.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\\nHenighan, Dawn Drain, Ethan Perez, Nicholas\\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\\nEli Tran-Johnson, et al. 2022. Language models\\n(mostly) know what they know. ArXiv preprint,\\nabs/2207.05221.\\nAitor Lewkowycz, Anders Andreassen, David Dohan,\\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,\\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo\\nGutman-Solo, et al. 2022. Solving quantitative\\nreasoning problems with language models. ArXiv\\npreprint, abs/2206.14858.\\nXiaonan Li and Xipeng Qiu. 2023. Mot: Pre-\\nthinking and recalling enable chatgpt to self-\\nimprove with memory-of-thoughts. ArXiv preprint,\\nabs/2305.05181.\\nOpenAI. 2023. Gpt-4 technical report.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow in-\\nstructions with human feedback. ArXiv preprint,\\nabs/2203.02155.\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\\nKnow what you don’t know: Unanswerable ques-\\ntions for SQuAD. In Proceedings of the 56th Annual\\nMeeting of the Association for Computational Lin-\\nguistics (Volume 2: Short Papers), pages 784–789,\\nMelbourne, Australia. Association for Computational\\nLinguistics.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. SQuAD: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Natu-\\nral Language Processing, pages 2383–2392, Austin,\\nTexas. Association for Computational Linguistics.\\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin\\nGopinath, Karthik Narasimhan, and Shunyu Yao.\\n2023. Reflexion: Language agents with verbal rein-\\nforcement learning.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 6}, page_content='Gopinath, Karthik Narasimhan, and Shunyu Yao.\\n2023. Reflexion: Language agents with verbal rein-\\nforcement learning.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\\nAdam R Brown, Adam Santoro, Aditya Gupta, Adrià\\nGarriga-Alonso, et al. 2022. Beyond the imitation\\ngame: Quantifying and extrapolating the capabilities\\nof language models. ArXiv preprint, abs/2206.04615.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model. https://\\ngithub.com/tatsu-lab/stanford_alpaca.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models. ArXiv\\npreprint, abs/2302.13971.\\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Har-\\nris, Alessandro Sordoni, Philip Bachman, and Kaheer\\nSuleman. 2017. NewsQA: A machine comprehen-\\nsion dataset. In Proceedings of the 2nd Workshop\\non Representation Learning for NLP, pages 191–200,\\nVancouver, Canada. Association for Computational\\nLinguistics.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 7}, page_content='Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\\nproves chain of thought reasoning in language mod-\\nels. ArXiv preprint, abs/2203.11171.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\\nand Denny Zhou. 2022. Chain of thought prompt-\\ning elicits reasoning in large language models. In\\nAdvances in Neural Information Processing Systems.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainable multi-hop question answering.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n2369–2380, Brussels, Belgium. Association for Com-\\nputational Linguistics.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nThomas L Griffiths, Yuan Cao, and Karthik\\nNarasimhan. 2023. Tree of thoughts: Deliberate\\nproblem solving with large language models. ArXiv\\npreprint, abs/2305.10601.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\\nLeast-to-most prompting enables complex reason-\\ning in large language models. ArXiv preprint,\\nabs/2205.10625.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 8}, page_content='A Appendix\\nA.1 Uncertainty Text\\nTo assemble a set of reference sentences, we ran-\\ndomly chose 100 entries from theSelfAwaredataset.\\nFor each model in the GPT-3 and InstructGPT se-\\nries, we conducted a preliminary test using the\\ndirect input form and manually curated sentences\\nthat displayed uncertainty. From this pre-test, we\\nprocured 16 sentences manifesting uncertain con-\\nnotations to serve as our reference sentences. After\\nnormalizing these sentences by eliminating punc-\\ntuation and converting to lowercase, we utilized\\nthem to compute similarity with target sentences\\nthroughout our experimental procedure.\\n1. The answer is unknown.\\n2. The answer is uncertain.\\n3. The answer is unclear.\\n4. There is no scientific evidence.\\n5. There is no definitive answer.\\n6. There is no right answer.\\n7. There is much debate.\\n8. There is no known case.\\n9. There is no concrete answer to this question.\\n10. There is no public information available.\\n11. It is impossible to know.\\n12. It is impossible to answer.\\n13. It is difficult to predict.\\n14. It is not known.\\n15. We do not know.\\n16. I’m not sure.\\nA.2 Threshold ablation\\nWe generated 100 new responses using the text-\\ndavinci-002 with direct input form and manually\\nfiltered out sentences that contained uncertainty.\\nWe then used SimCSE (Gao et al., 2021) to calcu-\\nlate the similarity between these sentences and the\\nreference sentences in Appendix A.1. We tested\\nvarious thresholds for filtering sentences with un-\\ncertain meanings and compared them to manually\\nThreshold Precision Recall F1\\n0.95 100.00 70.00 82.35\\n0.90 100.00 75.00 85.71\\n0.85 100.00 75.00 85.71\\n0.80 100.00 80.00 88.89\\n0.75 100.00 85.00 91.89\\n0.70 89.47 90.00 89.73\\n0.65 86.95 90.00 88.45\\nTable 2: Evaluation results comparing sentences with\\nuncertain meaning filtered by various thresholds.\\nHuman Precision Recall F1\\nV olunteer A 91.52 78.26 84.37\\nV olunteer B 96.36 76.81 85.48\\nTable 3: Evaluation results of 100 responses from two\\nvolunteers.\\nannotated sentences. We considered unanswerable\\nquestions as positive examples and calculated preci-\\nsion, recall, and F1 score. The results in Table 2 in-\\ndicate that a threshold of 0.75 produced the highest\\nF1 score, balancing precision and the inclusion of\\nother uncertain sentences. As a result, we selected\\n0.75 as the similarity threshold for subsequent ex-\\nperiments.\\nA.3 Human Self-Knowledge Test\\nThe evaluation results for the responses from our\\ninvited volunteers are presented in Table 3. The\\nF1 scores for the responses were high, indicating\\nthat both volunteers exhibited a strong level of self-\\nknowledge.\\nA.4 Template\\nThe input templates used in our experiments, Di-\\nrect, Instruction, and ICL, are illustrated in Fig-\\nures 7, 8, and 9, respectively. In the ICL template,\\nwe composed 3 answerable and 3 unanswerable\\nquestions and provided the corresponding answers\\nmanually.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 9}, page_content=\"Q: Are we alone in the universe, or will we discover alien life at some point?\\nA:\\nFigure 7: Direct input form.\\nWhen answering questions, interpret them literally and think them carefully. If the question is \\nunanswerable or unknowable, it is appropriate to say, “The answer is unknown.”\\nQ: Are we alone in the universe, or will we discover alien life at some point?\\nA:\\nFigure 8: Instruction input form.\\nQ: What is the nearest extraterrestrial civilization to us?\\nA: It is currently unknown if there are any extraterrestrial civilizations in proximity to Earth. Scientists \\nhave not yet found definitive evidence of the existence of extraterrestrial civilizations. Thus, the answer is \\nunknown.\\nQ: What is the highest building in New York?\\nA: The highest building in New York is the One World Trade Center. It stands at a height of 1,776 feet (541 \\nmeters). Thus, the answer is the One World Trade Center.\\nQ: Will nuclear war break out in the world in 2050?\\nA: It is impossible to predict with certainty if a nuclear war will break out in the world in 2050. The \\nlikelihood of such an event will depend on a wide range of factors, including international relations, \\npolitical tensions, and military capabilities. Thus, the answer is unknown.\\nQ: Alice's class is going on a field trip to the Ocean Park. There are 39 students and several teachers in the \\nclass. A bus can seat 20 people. How many buses do they need?\\nA: The number of buses needed for the field trip depends on the number of teachers. If there is only one \\nteacher, then two buses will suffice. However, additional buses will be necessary if there are two or more \\nteachers. Thus, the answer is unknown.\\nQ: Are Local H and For Against both from the United States?\\nA: Local H is a rock band from Zion, Illinois, United States. For Against is an American indie rock band \\nformed in 1984 in Lincoln, Nebraska. Both of these bands are from the United States. Thus, the answer is \\nyes.\\nQ: Gjetost is the national cheese of which country?\\nA:It is the national cheese of Norway, and it is a popular ingredient in traditional Norwegian cuisine. Thus, \\nthe answer is Norway.\\nQ: Are we alone in the universe, or will we discover alien life at some point?\\nA:\\nFigure 9: ICL input form.\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs = loader.load_and_split() # SPLIT\n",
    "pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 0}, page_content='Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢ Qiushi Sun♠ Qipeng Guo♢\\nJiawen Wu♢ Xipeng Qiu♢∗ Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware, consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗ Corresponding author.\\nUnknows\\nKnowsUnknows\\nKnows\\nKnown Knows Known Unknows\\nUnknown UnknowsUnknown Knows\\nUnlock\\nFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increase\\narXiv:2305.18153v2  [cs.CL]  30 May 2023')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_obj = pdf_docs[0]\n",
    "doc_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢ Qiushi Sun♠ Qipeng Guo♢\\nJiawen Wu♢ Xipeng Qiu♢∗ Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware, consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗ Corresponding author.\\nUnknows\\nKnowsUnknows\\nKnows\\nKnown Knows Known Unknows\\nUnknown UnknowsUnknown Knows\\nUnlock\\nFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increase\\narXiv:2305.18153v2  [cs.CL]  30 May 2023'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_obj.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Do Large Language Models Know What They Don’t Know?\n",
       "Zhangyue Yin♢ Qiushi Sun♠ Qipeng Guo♢\n",
       "Jiawen Wu♢ Xipeng Qiu♢∗ Xuanjing Huang♢\n",
       "♢School of Computer Science, Fudan University\n",
       "♠Department of Mathematics, National University of Singapore\n",
       "{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\n",
       "{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\n",
       "Abstract\n",
       "Large language models (LLMs) have a wealth\n",
       "of knowledge that allows them to excel in vari-\n",
       "ous Natural Language Processing (NLP) tasks.\n",
       "Current research focuses on enhancing their\n",
       "performance within their existing knowledge.\n",
       "Despite their vast knowledge, LLMs are still\n",
       "limited by the amount of information they can\n",
       "accommodate and comprehend. Therefore, the\n",
       "ability to understand their own limitations on\n",
       "the unknows, referred to as self-knowledge,\n",
       "is of paramount importance. This study aims\n",
       "to evaluate LLMs’ self-knowledge by assess-\n",
       "ing their ability to identify unanswerable or\n",
       "unknowable questions. We introduce an auto-\n",
       "mated methodology to detect uncertainty in the\n",
       "responses of these models, providing a novel\n",
       "measure of their self-knowledge. We further in-\n",
       "troduce a unique dataset, SelfAware, consisting\n",
       "of unanswerable questions from five diverse cat-\n",
       "egories and their answerable counterparts. Our\n",
       "extensive analysis, involving 20 LLMs includ-\n",
       "ing GPT-3, InstructGPT, and LLaMA, discov-\n",
       "ering an intrinsic capacity for self-knowledge\n",
       "within these models. Moreover, we demon-\n",
       "strate that in-context learning and instruction\n",
       "tuning can further enhance this self-knowledge.\n",
       "Despite this promising insight, our findings also\n",
       "highlight a considerable gap between the capa-\n",
       "bilities of these models and human proficiency\n",
       "in recognizing the limits of their knowledge.\n",
       "“True wisdom is knowing what you don’t know.”\n",
       "–Confucius\n",
       "1 Introduction\n",
       "Recently, Large Language Models (LLMs) such\n",
       "as GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\n",
       "2023), and LLaMA (Touvron et al., 2023) have\n",
       "shown exceptional performance on a wide range\n",
       "of NLP tasks, including common sense reason-\n",
       "ing (Wei et al., 2022; Zhou et al., 2022) and mathe-\n",
       "∗ Corresponding author.\n",
       "Unknows\n",
       "KnowsUnknows\n",
       "Knows\n",
       "Known Knows Known Unknows\n",
       "Unknown UnknowsUnknown Knows\n",
       "Unlock\n",
       "Figure 1: Know-Unknow Quadrant. The horizontal axis\n",
       "represents the model’s memory capacity for knowledge,\n",
       "and the vertical axis represents the model’s ability to\n",
       "comprehend and utilize knowledge.\n",
       "matical problem-solving (Lewkowycz et al., 2022;\n",
       "Chen et al., 2022). Despite their ability to learn\n",
       "from huge amounts of data, LLMs still have lim-\n",
       "itations in their capacity to retain and understand\n",
       "information. To ensure responsible usage, it is cru-\n",
       "cial for LLMs to have the capability of recognizing\n",
       "their limitations and conveying uncertainty when\n",
       "responding to unanswerable or unknowable ques-\n",
       "tions. This acknowledgment of limitations, also\n",
       "known as “ knowing what you don’t know,” is a\n",
       "crucial aspect in determining their practical appli-\n",
       "cability. In this work, we refer to this ability as\n",
       "model self-knowledge.\n",
       "The Know-Unknow quadrant in Figure 1 il-\n",
       "lustrates the relationship between the model’s\n",
       "knowledge and comprehension. The ratio of\n",
       "“Known Knows” to “Unknown Knows” demon-\n",
       "strates the model’s proficiency in understanding\n",
       "and applying existing knowledge. Techniques\n",
       "such as Chain-of-Thought (Wei et al., 2022), Self-\n",
       "Consistency (Wang et al., 2022), and Complex\n",
       "CoT (Fu et al., 2022) can be utilized to increase\n",
       "arXiv:2305.18153v2  [cs.CL]  30 May 2023"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "Markdown(doc_obj.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x1274fec90>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x12764b150>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base='https://api.openai.com/v1', openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings() # EMBED\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_lie = embeddings.embed_query(\"Lucas is a gorgeous man.\")\n",
    "embedding_truth = embeddings.embed_query(\"Lucas is a silly man.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two sentences: 0.8996095419150993\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate cosine similarity between the two embedding vectors\n",
    "similarity = np.dot(embedding_lie, embedding_truth) / (np.linalg.norm(embedding_lie) * np.linalg.norm(embedding_truth))\n",
    "print(f\"Cosine similarity between the two sentences: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_random = embeddings.embed_query(\"The sky is blue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two sentences: 0.7751547827937032\n"
     ]
    }
   ],
   "source": [
    "similarity = np.dot(embedding_lie, embedding_random) / (np.linalg.norm(embedding_lie) * np.linalg.norm(embedding_random))\n",
    "print(f\"Cosine similarity between the two sentences: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x12774e610>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(pdf_docs, embedding=embeddings) # STORE\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of a [retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/#:~:text=A%20retriever%20is,Document's%20as%20output.):\n",
    "\n",
    "> A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x12774e610>, search_kwargs={})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever() \n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=MODEL, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://python.langchain.com/v0.2/docs/tutorials/pdf_qa/#question-answering-with-rag\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x327897810>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x3274cc650>, root_client=<openai.OpenAI object at 0x1277d0a10>, root_async_client=<openai.AsyncOpenAI object at 0x32783bc50>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.openai.com/v1')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "question_answer_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method `create_stuff_documents_chain` [outputs an LCEL runnable](https://arc.net/l/quote/bnsztwth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the dataset refered in this paper?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x12774e610>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x327897810>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x3274cc650>, root_client=<openai.OpenAI object at 0x1277d0a10>, root_async_client=<openai.AsyncOpenAI object at 0x32783bc50>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.openai.com/v1')\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the dataset refered in this paper?',\n",
       " 'context': [Document(metadata={'page': 1, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}, page_content='et al., 2017). Our selection was guided by Sim-\\nCSE (Gao et al., 2021), which allowed us to iden-\\ntify and select the answerable questions semanti-\\ncally closest to the unanswerable ones. From these\\nsources, we accordingly drew samples of 1,487,\\n182, and 668 questions respectively, amassing a\\ntotal of 2,337. Given that these questions can be\\neffectively addressed using information available\\non Wikipedia, the foundational corpus for the train-\\ning of current LLMs, it is plausible to infer that\\nthe model possesses the requisite knowledge to\\ngenerate accurate responses to these questions.\\nOur dataset, christened SelfAware, incorporates\\n1,032 unanswerable and 2,337 answerable ques-\\ntions. To reflect real-world distribution, our dataset\\n1The code pertinent to our study can be accessed\\nhttps://github.com/yinzhangyue/SelfAware'),\n",
       "  Document(metadata={'page': 5, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}, page_content='Limitations\\n• Generalization of reference sentences.At\\npresent, we have selected sentences with un-\\ncertain meanings exclusively from the GPT-3\\nand InstructGPT series, potentially overlook-\\ning uncertainty present in responses generated\\nby other LLMs. However, it is not feasible\\nto catalog all sentences with uncertain mean-\\nings exhaustively. As a direction for future\\nresearch, we propose to concentrate on the\\nautomated acquisition of more accurate refer-\\nence sentences to address this concern.\\n• Limitations of input forms: Our exami-\\nnation was confined to three unique input\\nforms: direct, instruction, and ICL. There\\nis burgeoning research aimed at bridging the\\ngap between models and human-like meth-\\nods of reasoning and problem-solving, includ-\\ning but not limited to approaches like Re-\\nflexion (Shinn et al., 2023), ToT (Yao et al.,\\n2023), MoT (Li and Qiu, 2023). Future en-\\ndeavors will integrate additional cognitive and\\ndecision-making methods to delve deeper into\\nthe self-knowledge exhibited by these LLMs.\\nEthics Statement\\nThe SelfAware dataset, meticulously curated to\\nevaluate LLMs’ ability to discern unanswerable\\nquestions, is composed of unanswerable questions\\nextracted from sources such as Quora and How-\\nStuffWorks, alongside answerable questions pro-\\ncured from three distinct open datasets. Every ques-\\ntion was thoroughly examined for relevance and\\nharmlessness. To ensure content validity, three an-\\nnotation analysts, compensated at local wage stan-\\ndards, dedicated regular working hours to content\\nreview.\\nThroughout our research process, we under-\\nscored the significance of privacy, data security,\\nand strict compliance with dataset licenses. In\\norder to protect data integrity, we implemented\\nanonymization and content filtration mechanisms.\\nOur adherence to OpenAI’s stipulations remained\\nunyielding for the usage of GPT-3 and InstructGPT\\nmodels, and likewise for Meta’s terms pertaining\\nto LLaMA models. We rigorously vetted the li-\\ncenses of the three publicly available datasets for\\ncompliance, ensuring that all our research method-\\nologies were in alignment with ethical standards at\\nthe institutional, national, and global levels.\\nAdhering to the CC-BY-SA-4.0 protocol, the\\ndataset, once publicly released, will be reserved\\nexclusively for research purposes. We pledge to\\npromptly and effectively address any concerns relat-\\ning to the dataset, while concurrently anticipating\\nresearchers to maintain high ethical standards in\\ntheir utilization of this data.\\nAcknowledgement\\nWe wish to express our gratitude to our colleagues\\nin the FudanNLP group whose insightful sugges-\\ntions, perspectives, and thought-provoking discus-\\nsions significantly contributed to this work. Our\\nsincere appreciation also extends to the anonymous\\nreviewers and area chairs, whose constructive feed-\\nback was instrumental in refining the quality of\\nour study. This work was supported by the Na-\\ntional Natural Science Foundation of China (No.\\n62236004 and No. 62022027) and CAAI-Huawei\\nMindSpore Open Fund.\\nReferences\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\\nChen, Eric Chu, Jonathan H. Clark, Laurent El\\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\\nJan Botha, James Bradbury, Siddhartha Brahma,\\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\\nCherry, Christopher A. Choquette-Choo, Aakanksha\\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-'),\n",
       "  Document(metadata={'page': 8, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}, page_content='A Appendix\\nA.1 Uncertainty Text\\nTo assemble a set of reference sentences, we ran-\\ndomly chose 100 entries from theSelfAwaredataset.\\nFor each model in the GPT-3 and InstructGPT se-\\nries, we conducted a preliminary test using the\\ndirect input form and manually curated sentences\\nthat displayed uncertainty. From this pre-test, we\\nprocured 16 sentences manifesting uncertain con-\\nnotations to serve as our reference sentences. After\\nnormalizing these sentences by eliminating punc-\\ntuation and converting to lowercase, we utilized\\nthem to compute similarity with target sentences\\nthroughout our experimental procedure.\\n1. The answer is unknown.\\n2. The answer is uncertain.\\n3. The answer is unclear.\\n4. There is no scientific evidence.\\n5. There is no definitive answer.\\n6. There is no right answer.\\n7. There is much debate.\\n8. There is no known case.\\n9. There is no concrete answer to this question.\\n10. There is no public information available.\\n11. It is impossible to know.\\n12. It is impossible to answer.\\n13. It is difficult to predict.\\n14. It is not known.\\n15. We do not know.\\n16. I’m not sure.\\nA.2 Threshold ablation\\nWe generated 100 new responses using the text-\\ndavinci-002 with direct input form and manually\\nfiltered out sentences that contained uncertainty.\\nWe then used SimCSE (Gao et al., 2021) to calcu-\\nlate the similarity between these sentences and the\\nreference sentences in Appendix A.1. We tested\\nvarious thresholds for filtering sentences with un-\\ncertain meanings and compared them to manually\\nThreshold Precision Recall F1\\n0.95 100.00 70.00 82.35\\n0.90 100.00 75.00 85.71\\n0.85 100.00 75.00 85.71\\n0.80 100.00 80.00 88.89\\n0.75 100.00 85.00 91.89\\n0.70 89.47 90.00 89.73\\n0.65 86.95 90.00 88.45\\nTable 2: Evaluation results comparing sentences with\\nuncertain meaning filtered by various thresholds.\\nHuman Precision Recall F1\\nV olunteer A 91.52 78.26 84.37\\nV olunteer B 96.36 76.81 85.48\\nTable 3: Evaluation results of 100 responses from two\\nvolunteers.\\nannotated sentences. We considered unanswerable\\nquestions as positive examples and calculated preci-\\nsion, recall, and F1 score. The results in Table 2 in-\\ndicate that a threshold of 0.75 produced the highest\\nF1 score, balancing precision and the inclusion of\\nother uncertain sentences. As a result, we selected\\n0.75 as the similarity threshold for subsequent ex-\\nperiments.\\nA.3 Human Self-Knowledge Test\\nThe evaluation results for the responses from our\\ninvited volunteers are presented in Table 3. The\\nF1 scores for the responses were high, indicating\\nthat both volunteers exhibited a strong level of self-\\nknowledge.\\nA.4 Template\\nThe input templates used in our experiments, Di-\\nrect, Instruction, and ICL, are illustrated in Fig-\\nures 7, 8, and 9, respectively. In the ICL template,\\nwe composed 3 answerable and 3 unanswerable\\nquestions and provided the corresponding answers\\nmanually.'),\n",
       "  Document(metadata={'page': 1, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}, page_content='this ratio, resulting in improved performance on\\nNLP tasks. We focus on the ratio of “Known Un-\\nknows” to “Unknown Unknows”, which indicates\\nthe model’s self-knowledge level, specifically un-\\nderstanding its own limitations and deficiencies in\\nthe unknows.\\nExisting datasets such as SQuAD2.0 (Rajpurkar\\net al., 2018) and NewsQA (Trischler et al., 2017),\\nwidely used in question answering (QA), have been\\nutilized to test the self-knowledge of models with\\nunanswerable questions. However, these questions\\nare context-specific and could become answerable\\nwhen supplemented with additional information.\\nSrivastava et al. (2022) attempted to address this by\\nevaluating LLMs’ competence in delineating their\\nknowledge boundaries, employing a set of 23 pairs\\nof answerable and unanswerable multiple-choice\\nquestions. They discovered that these models’ per-\\nformance barely surpassed that of random guessing.\\nKadavath et al. (2022) suggested probing the self-\\nknowledge of LLMs through the implementation\\nof a distinct \"Value Head\". Yet, this approach may\\nencounter difficulties when applied across varied\\ndomains or tasks due to task-specific training. Con-\\nsequently, we redirect our focus to the inherent\\nabilities of LLMs, and pose the pivotal question:\\n“Do large language models know what they don’t\\nknow?”.\\nIn this study, we investigate the self-knowledge\\nof LLMs using a novel approach. By gathering\\nreference sentences with uncertain meanings, we\\ncan determine whether the model’s responses re-\\nflect uncertainty using a text similarity algorithm.\\nWe quantified the model’s self-knowledge using\\nthe F1 score. To address the small and idiosyn-\\ncratic limitations of existing datasets, we created\\na new dataset called SelfAware. This dataset com-\\nprises 1,032 unanswerable questions, which are dis-\\ntributed across five distinct categories, along with\\nan additional 2,337 questions that are classified as\\nanswerable. Experimental results on GPT-3, In-\\nstructGPT, LLaMA, and other LLMs demonstrate\\nthat in-context learning and instruction tuning can\\neffectively enhance the self-knowledge of LLMs.\\nHowever, the self-knowledge exhibited by the cur-\\nrent state-of-the-art model, GPT-4, measures at\\n75.47%, signifying a notable disparity when con-\\ntrasted with human self-knowledge, which is rated\\nat 84.93%.\\nOur key contributions to this field are summa-\\nrized as follows:\\n• We have developed a new dataset,SelfAware,\\nthat comprises a diverse range of commonly\\nposed unanswerable questions.\\n• We propose an innovative evaluation tech-\\nnique based on text similarity to quantify the\\ndegree of uncertainty inherent in model out-\\nputs.\\n• Through our detailed analysis of 20 LLMs,\\nbenchmarked against human self-knowledge,\\nwe identified a significant disparity between\\nthe most advanced LLMs and humans 1.\\n2 Dataset Construction\\nTo conduct a more comprehensive evaluation of\\nthe model’s self-knowledge, we constructed a\\ndataset that includes a larger number and more di-\\nverse types of unanswerable questions than Know-\\nUnknowns dataset (Srivastava et al., 2022). To\\nfacilitate this, we collected a corpus of 2,858 unan-\\nswerable questions, sourced from online platforms\\nlike Quora and HowStuffWorks. These questions\\nwere meticulously evaluated by three seasoned an-\\nnotation analysts, each operating independently.\\nThe analysts were permitted to leverage external\\nresources, such as search engines. To ensure the va-\\nlidity of our dataset, we retained only the questions\\nthat all three analysts concurred were unanswerable.\\nThis rigorous process yielded a finalized collection\\nof 1,032 unanswerable questions.\\nIn pursuit of a comprehensive evaluation, we\\nopted for answerable questions drawn from three\\ndatasets: SQuAD (Rajpurkar et al., 2016), Hot-\\npotQA (Yang et al., 2018), and TriviaQA (Joshi\\net al., 2017). Our selection was guided by Sim-\\nCSE (Gao et al., 2021), which allowed us to iden-\\ntify and select the answerable questions semanti-\\ncally closest to the unanswerable ones. From these')],\n",
       " 'answer': 'The dataset referred to in the paper is called \"SelfAware.\" It comprises 1,032 unanswerable questions and 2,337 answerable questions, designed to evaluate the self-knowledge of large language models (LLMs).'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The dataset referred to in the paper is called \"SelfAware.\" It comprises 1,032 unanswerable questions and 2,337 answerable questions, designed to evaluate the self-knowledge of large language models (LLMs)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "final_answer = results[\"answer\"]\n",
    "\n",
    "Markdown(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- The paper evaluates self-knowledge in language models, specifically GPT-3, InstructGPT, LLaMA, Alpaca, and Vicuna, using a dataset called SelfAware.\n",
       "- SelfAware contains 1,032 unanswerable and 2,337 answerable questions, categorized into five types: no scientific consensus, imagination, completely subjective, too many variables, and philosophical.\n",
       "- The study uses a similarity function to identify sentences with uncertain meanings and determines that a threshold of 0.75 provides the best balance of precision and recall for filtering uncertain sentences."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_summary = \"Write a simple bullet points summary about this paper\"\n",
    "\n",
    " # adding chat history so the model remembers previous questions\n",
    "output = rag_chain.invoke({\"input\": query_summary})\n",
    "\n",
    "Markdown(output[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output is easily verifiable, we can see below that the chunk context for the answer came from pages 0,5,7 and 16 in the source pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 8, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}\n",
      "{'page': 2, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}\n",
      "{'page': 7, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}\n",
      "{'page': 1, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(output['context'])):\n",
    "    print(output['context'][i].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now dig deeper into RAG with pdf and construct this rag chain ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_prompt),\n",
    "    ('human', '{input}')\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        'input': lambda x: x['input'],\n",
    "        'context': lambda x: format_docs(x['context']), \n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the input query to the retriever\n",
    "retrieve_docs = (lambda x: x['input']) | retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  context: RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x11a1ced90>)\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: {\n",
       "              input: RunnableLambda(...),\n",
       "              context: RunnableLambda(...)\n",
       "            }\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x12bf88150>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12bff0890>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_api_base='https://api.openai.com/v1', openai_proxy='')\n",
       "            | StrOutputParser()\n",
       "  })"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'According to this paper what are these reusable LLM-profiled components?',\n",
       " 'context': [Document(metadata={'page': 0, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='A Survey on LLM-Based Agents: Common Workflows and Reusable\\nLLM-Profiled Components\\nXinzhe Li\\nSchool of IT, Deakin University, Australia\\nlixinzhe@deakin.edu.au\\nAbstract\\nRecent advancements in Large Language Mod-\\nels (LLMs) have catalyzed the development of so-\\nphisticated frameworks for developing LLM-based\\nagents. However, the complexity of these frame-\\nworks r poses a hurdle for nuanced differentiation\\nat a granular level, a critical aspect for enabling\\nefficient implementations across different frame-\\nworks and fostering future research. Hence, the\\nprimary purpose of this survey is to facilitate a co-\\nhesive understanding of diverse recently proposed\\nframeworks by identifying common workflows and\\nreusable LLM-Profiled Components (LMPCs).\\n1 Introduction\\nGenerative Large Language Models (GLMs or LLMs)\\nhave acquired extensive general knowledge and\\nhuman-like reasoning capabilities (Santurkar et al.,\\n2023; Wang et al., 2022; Zhong et al., 2022, 2023),\\npositioning them as pivotal in constructing AI agents\\nknown as LLM-based agents. In the context of this\\nsurvey, LLM-based agents are defined by their abil-\\nity to interact actively with external tools (such as\\nWikipedia) or environments (such as householding en-\\nvironments) and are designed to function as integral\\ncomponents of agency, including acting, planning, and\\nevaluating.\\nPurpose of the Survey The motivation behind this\\nsurvey stems from the observation that many LLM-\\nbased agents incorporate similar workflows and com-\\nponents, despite the presence of a wide variety of\\ntechnical and conceptual challenges, e.g., search algo-\\nrithms (Yao et al., 2023a), tree structures (Hao et al.,\\n2023), and Reinforcement Learning (RL) components\\n(Shinn et al., 2023). (Wu et al., 2023) offer a modular\\napproach but lack integration with prevalent agentic\\nworkflows. Wang et al. (2024) provide a comprehen-\\nsive review of LLM agents, exploring their capabil-\\nities across profiling, memory, planning, and action.\\nIn contrast, our survey does not attempt to cover all\\ncomponents of LLM-based agents comprehensively.\\nInstead, we concentrate on the involvement of LLMswithin agentic workflows and aim to clarify the roles\\nof LLMs in agent implementations. We create com-\\nmon workflows incorporating reusable LLM-Profiled\\nComponents (LMPCs), as depicted in Figure 1.\\nContributions This survey offers the following con-\\ntributions. 1) Alleviating the understanding of com-\\nplex frameworks : The complexity of existing frame-\\nworks can be simplified into implementable workflows,\\nespecially when they are extracted for specific tasks.\\nThis survey emphasizes reusable workflows and LM-\\nPCs across popular frameworks, such as ReAct (Yao\\net al., 2023b), Reflexion (Shinn et al., 2023) and Tree-\\nof-Thoughts (Yao et al., 2023a). Specifically, based\\non the interaction environments (§2) and the use of\\ncommon LMPCs (§3), we categorize and detail vari-\\nous workflows, e.g., tool-use workflows, search work-\\nflows, and feedback-learning workflows. Many ex-\\nisting frameworks are composed of these workflows\\nand LMPCs, along with some specific non-LLM com-\\nponents. 2) Helping researchers/practitioners as-\\nsess current frameworks at a more granular and\\ncohesive level : Section 4 categorizes prominent frame-\\nworks and demonstrates how they are assembled by the\\ncommon workflows and LMPCs, as summarized in Ta-\\nble 21.3) Facilitating further extensions of existing\\nframeworks : Existing frameworks could be modi-\\nfied by changing the implementations of LMPCs. To\\nenable this, we not only summarize implementations\\nof LMPCs but also their applicability across diverse\\nworkflows and tasks in Section 5.\\n2 Task Environments And Tool\\nEnvironments\\nThis section explores task environments and tool envi-\\nronments, which present different settings compared to\\ntraditional AI and reinforcement learning (RL) agent\\nframeworks (Russell and Norvig, 2010; Sutton and\\nBarto, 2018) . After a brief overview of standard logic-'),\n",
       "  Document(metadata={'page': 7, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='existing workflows through the integration of both\\ntask-specific LMPCs and non-LLM components. This\\napproach aims to foster the development and repro-\\nducibility of agentic workflows.\\nLimitations\\nThis survey omits discussions on memory design15\\nand the integration of peripheral components into agen-\\ntic workflows16, as our focus is on the details of\\ncommon LLM-profiled components within agentic\\nworkflows to facilitate the implementation of reusable\\ncomponents and extensible workflows. This distinctly\\nsets our work apart from other surveys.\\nReferences\\nSijia Chen, Baochun Li, and Di Niu. 2024. Boosting of\\nthoughts: Trial-and-error problem solving with large lan-\\n14Here, bandwidth refers to the volume of information pro-\\ncessed during a single LLM generation\\n15Appendix E provides a brief discussion on memory in LLM-\\nbased agents\\n16These are concisely summarized in Appendix A\\n8'),\n",
       "  Document(metadata={'page': 3, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='3) LLM-Profiled Dynamic Models glmdynamic : They\\npredict or describe changes to the environment. Gen-\\nerally, dynamic models form part of a comprehensive\\nworld model by predicting the next state s′from the\\ncurrent state sand action a. While typical RL uses\\nthe probability distribution p(s′|s, a)to model poten-\\ntial next states, LLM-based dynamic models directly\\npredict the next state s′=glmdynamic (s, a).\\nTask-Dependent LLM-Profiled Components In\\naddition to the universal components, certain LLM-\\nprofiled components are tailored to specific tasks. For\\ninstance, verbalizers are crucial in embodied environ-\\nments but unnecessary in NLIEs. A verbalizer trans-\\nlates actions and observations into inputs for planners;\\nfor example, in the Planner-Actor-Reporter workflow\\n(Wang et al., 2023a), a fine-tuned Visual Language\\nModel (VLM) along with glmplanner translates pixel\\nstates into textual inputs. Similarly, if environmental\\nfeedback is perceivable along with states, a verbalizer\\nmay be needed to translate this feedback into verbal\\ndescriptions for glmpolicy, akin to reward shaping in\\nRL where numerical stimuli are generated for policy\\nlearning. LLMs profiled as verbalizers, glmverbalizer\\n(Shinn et al., 2023), often guide descriptions accord-\\ning to specified criteria.\\nsettings to learn policy models that perform desirable behaviors.\\n3Note that planning algorithms may be utilized to structure a\\nplan of plans; for example, Tree-of-Thought employs tree search,\\nwhere each node potentially represents either a single action or an\\nentire plan.\\n4'),\n",
       "  Document(metadata={'page': 3, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='deployment in planning-based agent systems. Addi-\\ntionally, Wan et al. (2024) suggest that \"splitting an\\noutput sequence into tokens might be a good choice\"\\nfor defining multi-step NLIEs methodically. Further-\\nmore, Yao et al. (2023a) formulate two-step NLIEs for\\ncreative writing by segmenting the problem-solving\\nprocess into distinct planning and execution phases.\\n2.3 Tool Environments\\nModern LLM agents are often enhanced with external\\ntools that improve their problem-solving capabilities\\n(Inaba et al., 2023; Yao et al., 2023b). The design and\\nintegration of these tools add complexity, requiring\\ncareful consideration of how LLMs interact not only\\nwith the task environments but also with these aux-\\niliary tools. Typically, actions in tool environments\\ninvolve interactions with resources that remain unaf-\\nfected by these interactions. For instance, retrieving\\ndata from Wikipedia constitutes a \"read-only\" action,\\nwhich does not modify the Wikipedia database. This\\nfeature distinguishes such tool-use actions from those\\nin conventional task environments or typical reinforce-\\nment learning (RL) settings, where actions generally\\nalter the environmental state. Nevertheless, it is impor-\\ntant to recognize that tool environment can be dynamic\\nthat can undergo changes externally. This aspect re-\\nflects the nature that tools should be considered ex-\\nternal environments rather than the agent’s internal\\nprocesses.\\nNested NLIE-QA + Tool Environments Tool envi-\\nronments are frequently established along with NLIEs\\nto aid in solving QA tasks. Shinn et al. (2023); Yao\\net al. (2023b) incorporate tools to enhance the fac-\\ntuality of responses. They define command-like ac-\\ntions such as “Search” and “LookUp” to interact with\\nWikipedia, with “Search” suggesting the top-5 similar\\nentities from the relevant wiki page, and “LookUp”\\nsimulating the Ctrl+F functionality in a browser. Be-\\nyond simple retrieval, Thoppilan et al. (2022) include\\na language translator and a calculator for dialog tasks.\\nSimilarly, Inaba et al. (2023) employ a calculator, im-\\nplemented using the Python eval function, to resolve\\nnumerical queries within the NumGLUE benchmark.\\n3 LLM-Profiled Components\\nThis section explores common agentic roles for which\\nLLMs are typically profiled. The components leverage\\nthe internal commonsense knowledge and reasoning\\nabilities of LLMs to generate actions, plans, estimate\\nvalues2, and infer subsequent states.\\n2Values refer to the estimated rewards (a quantitative measure\\nof the success or desirability of the outcomes) associated with tak-\\ning a certain action in a state, widely used in typical RL and MDPUniversal LLM-Profiled Components Specifically,\\nthe following task-agnostic components are profiled\\nand commonly used across various workflows. 1)\\nLLM-Profiled Policy glmpolicy: Policy models are de-\\nsigned to generate decisions, which could be an action\\nor a series of actions (plans) for execution in exter-\\nnal environments or use in search and planning algo-\\nrithms.3In contrast to typical RL policy models,\\nwhich learn to maximize cumulative rewards through\\ntrial and error, LLM-profiled policy models, denoted\\nasglmpolicy, utilize pre-trained knowledge and com-\\nmonsense derived from extensive textual data. We\\ndistinguish between two types of glmpolicy: an actor\\nglmactordirectly maps a state to an action, whereas\\na planner glmplanner generates a sequence of actions\\nfrom a given state. 2) LLM-Profiled Evaluators glmeval:\\nglmevalprovide feedback crucial for different work-\\nflows. They evaluate actions and states in search-based\\nworkflows (Hao et al., 2023; Yao et al., 2023a) and re-\\nvise decisions in feedback-learning workflows (Shinn\\net al., 2023; Wang et al., 2023b) (refer to §4 for more\\ndetails). These evaluators are integral to both direct\\naction assessment and broader strategic adjustments.\\n3) LLM-Profiled Dynamic Models glmdynamic : They\\npredict or describe changes to the environment. Gen-\\nerally, dynamic models form part of a comprehensive')],\n",
       " 'answer': 'Reusable LLM-Profiled Components (LMPCs) include various task-agnostic components commonly used across different workflows, such as LLM-Profiled Policy (glmpolicy), LLM-Profiled Evaluators (glmeval), and LLM-Profiled Dynamic Models (glmdynamic). These components leverage the internal knowledge and reasoning abilities of LLMs to generate actions, provide feedback, and predict changes in the environment. The survey emphasizes the importance of these components in simplifying complex frameworks and facilitating the development of LLM-based agents.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"According to this paper what are these reusable LLM-profiled components?\" \n",
    "chain.invoke({'input': query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding structured sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'According to this paper what are these reusable LLM-profiled components?',\n",
       " 'context': [Document(metadata={'page': 0, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='A Survey on LLM-Based Agents: Common Workflows and Reusable\\nLLM-Profiled Components\\nXinzhe Li\\nSchool of IT, Deakin University, Australia\\nlixinzhe@deakin.edu.au\\nAbstract\\nRecent advancements in Large Language Mod-\\nels (LLMs) have catalyzed the development of so-\\nphisticated frameworks for developing LLM-based\\nagents. However, the complexity of these frame-\\nworks r poses a hurdle for nuanced differentiation\\nat a granular level, a critical aspect for enabling\\nefficient implementations across different frame-\\nworks and fostering future research. Hence, the\\nprimary purpose of this survey is to facilitate a co-\\nhesive understanding of diverse recently proposed\\nframeworks by identifying common workflows and\\nreusable LLM-Profiled Components (LMPCs).\\n1 Introduction\\nGenerative Large Language Models (GLMs or LLMs)\\nhave acquired extensive general knowledge and\\nhuman-like reasoning capabilities (Santurkar et al.,\\n2023; Wang et al., 2022; Zhong et al., 2022, 2023),\\npositioning them as pivotal in constructing AI agents\\nknown as LLM-based agents. In the context of this\\nsurvey, LLM-based agents are defined by their abil-\\nity to interact actively with external tools (such as\\nWikipedia) or environments (such as householding en-\\nvironments) and are designed to function as integral\\ncomponents of agency, including acting, planning, and\\nevaluating.\\nPurpose of the Survey The motivation behind this\\nsurvey stems from the observation that many LLM-\\nbased agents incorporate similar workflows and com-\\nponents, despite the presence of a wide variety of\\ntechnical and conceptual challenges, e.g., search algo-\\nrithms (Yao et al., 2023a), tree structures (Hao et al.,\\n2023), and Reinforcement Learning (RL) components\\n(Shinn et al., 2023). (Wu et al., 2023) offer a modular\\napproach but lack integration with prevalent agentic\\nworkflows. Wang et al. (2024) provide a comprehen-\\nsive review of LLM agents, exploring their capabil-\\nities across profiling, memory, planning, and action.\\nIn contrast, our survey does not attempt to cover all\\ncomponents of LLM-based agents comprehensively.\\nInstead, we concentrate on the involvement of LLMswithin agentic workflows and aim to clarify the roles\\nof LLMs in agent implementations. We create com-\\nmon workflows incorporating reusable LLM-Profiled\\nComponents (LMPCs), as depicted in Figure 1.\\nContributions This survey offers the following con-\\ntributions. 1) Alleviating the understanding of com-\\nplex frameworks : The complexity of existing frame-\\nworks can be simplified into implementable workflows,\\nespecially when they are extracted for specific tasks.\\nThis survey emphasizes reusable workflows and LM-\\nPCs across popular frameworks, such as ReAct (Yao\\net al., 2023b), Reflexion (Shinn et al., 2023) and Tree-\\nof-Thoughts (Yao et al., 2023a). Specifically, based\\non the interaction environments (§2) and the use of\\ncommon LMPCs (§3), we categorize and detail vari-\\nous workflows, e.g., tool-use workflows, search work-\\nflows, and feedback-learning workflows. Many ex-\\nisting frameworks are composed of these workflows\\nand LMPCs, along with some specific non-LLM com-\\nponents. 2) Helping researchers/practitioners as-\\nsess current frameworks at a more granular and\\ncohesive level : Section 4 categorizes prominent frame-\\nworks and demonstrates how they are assembled by the\\ncommon workflows and LMPCs, as summarized in Ta-\\nble 21.3) Facilitating further extensions of existing\\nframeworks : Existing frameworks could be modi-\\nfied by changing the implementations of LMPCs. To\\nenable this, we not only summarize implementations\\nof LMPCs but also their applicability across diverse\\nworkflows and tasks in Section 5.\\n2 Task Environments And Tool\\nEnvironments\\nThis section explores task environments and tool envi-\\nronments, which present different settings compared to\\ntraditional AI and reinforcement learning (RL) agent\\nframeworks (Russell and Norvig, 2010; Sutton and\\nBarto, 2018) . After a brief overview of standard logic-'),\n",
       "  Document(metadata={'page': 7, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='existing workflows through the integration of both\\ntask-specific LMPCs and non-LLM components. This\\napproach aims to foster the development and repro-\\nducibility of agentic workflows.\\nLimitations\\nThis survey omits discussions on memory design15\\nand the integration of peripheral components into agen-\\ntic workflows16, as our focus is on the details of\\ncommon LLM-profiled components within agentic\\nworkflows to facilitate the implementation of reusable\\ncomponents and extensible workflows. This distinctly\\nsets our work apart from other surveys.\\nReferences\\nSijia Chen, Baochun Li, and Di Niu. 2024. Boosting of\\nthoughts: Trial-and-error problem solving with large lan-\\n14Here, bandwidth refers to the volume of information pro-\\ncessed during a single LLM generation\\n15Appendix E provides a brief discussion on memory in LLM-\\nbased agents\\n16These are concisely summarized in Appendix A\\n8'),\n",
       "  Document(metadata={'page': 3, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='3) LLM-Profiled Dynamic Models glmdynamic : They\\npredict or describe changes to the environment. Gen-\\nerally, dynamic models form part of a comprehensive\\nworld model by predicting the next state s′from the\\ncurrent state sand action a. While typical RL uses\\nthe probability distribution p(s′|s, a)to model poten-\\ntial next states, LLM-based dynamic models directly\\npredict the next state s′=glmdynamic (s, a).\\nTask-Dependent LLM-Profiled Components In\\naddition to the universal components, certain LLM-\\nprofiled components are tailored to specific tasks. For\\ninstance, verbalizers are crucial in embodied environ-\\nments but unnecessary in NLIEs. A verbalizer trans-\\nlates actions and observations into inputs for planners;\\nfor example, in the Planner-Actor-Reporter workflow\\n(Wang et al., 2023a), a fine-tuned Visual Language\\nModel (VLM) along with glmplanner translates pixel\\nstates into textual inputs. Similarly, if environmental\\nfeedback is perceivable along with states, a verbalizer\\nmay be needed to translate this feedback into verbal\\ndescriptions for glmpolicy, akin to reward shaping in\\nRL where numerical stimuli are generated for policy\\nlearning. LLMs profiled as verbalizers, glmverbalizer\\n(Shinn et al., 2023), often guide descriptions accord-\\ning to specified criteria.\\nsettings to learn policy models that perform desirable behaviors.\\n3Note that planning algorithms may be utilized to structure a\\nplan of plans; for example, Tree-of-Thought employs tree search,\\nwhere each node potentially represents either a single action or an\\nentire plan.\\n4'),\n",
       "  Document(metadata={'page': 3, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='deployment in planning-based agent systems. Addi-\\ntionally, Wan et al. (2024) suggest that \"splitting an\\noutput sequence into tokens might be a good choice\"\\nfor defining multi-step NLIEs methodically. Further-\\nmore, Yao et al. (2023a) formulate two-step NLIEs for\\ncreative writing by segmenting the problem-solving\\nprocess into distinct planning and execution phases.\\n2.3 Tool Environments\\nModern LLM agents are often enhanced with external\\ntools that improve their problem-solving capabilities\\n(Inaba et al., 2023; Yao et al., 2023b). The design and\\nintegration of these tools add complexity, requiring\\ncareful consideration of how LLMs interact not only\\nwith the task environments but also with these aux-\\niliary tools. Typically, actions in tool environments\\ninvolve interactions with resources that remain unaf-\\nfected by these interactions. For instance, retrieving\\ndata from Wikipedia constitutes a \"read-only\" action,\\nwhich does not modify the Wikipedia database. This\\nfeature distinguishes such tool-use actions from those\\nin conventional task environments or typical reinforce-\\nment learning (RL) settings, where actions generally\\nalter the environmental state. Nevertheless, it is impor-\\ntant to recognize that tool environment can be dynamic\\nthat can undergo changes externally. This aspect re-\\nflects the nature that tools should be considered ex-\\nternal environments rather than the agent’s internal\\nprocesses.\\nNested NLIE-QA + Tool Environments Tool envi-\\nronments are frequently established along with NLIEs\\nto aid in solving QA tasks. Shinn et al. (2023); Yao\\net al. (2023b) incorporate tools to enhance the fac-\\ntuality of responses. They define command-like ac-\\ntions such as “Search” and “LookUp” to interact with\\nWikipedia, with “Search” suggesting the top-5 similar\\nentities from the relevant wiki page, and “LookUp”\\nsimulating the Ctrl+F functionality in a browser. Be-\\nyond simple retrieval, Thoppilan et al. (2022) include\\na language translator and a calculator for dialog tasks.\\nSimilarly, Inaba et al. (2023) employ a calculator, im-\\nplemented using the Python eval function, to resolve\\nnumerical queries within the NumGLUE benchmark.\\n3 LLM-Profiled Components\\nThis section explores common agentic roles for which\\nLLMs are typically profiled. The components leverage\\nthe internal commonsense knowledge and reasoning\\nabilities of LLMs to generate actions, plans, estimate\\nvalues2, and infer subsequent states.\\n2Values refer to the estimated rewards (a quantitative measure\\nof the success or desirability of the outcomes) associated with tak-\\ning a certain action in a state, widely used in typical RL and MDPUniversal LLM-Profiled Components Specifically,\\nthe following task-agnostic components are profiled\\nand commonly used across various workflows. 1)\\nLLM-Profiled Policy glmpolicy: Policy models are de-\\nsigned to generate decisions, which could be an action\\nor a series of actions (plans) for execution in exter-\\nnal environments or use in search and planning algo-\\nrithms.3In contrast to typical RL policy models,\\nwhich learn to maximize cumulative rewards through\\ntrial and error, LLM-profiled policy models, denoted\\nasglmpolicy, utilize pre-trained knowledge and com-\\nmonsense derived from extensive textual data. We\\ndistinguish between two types of glmpolicy: an actor\\nglmactordirectly maps a state to an action, whereas\\na planner glmplanner generates a sequence of actions\\nfrom a given state. 2) LLM-Profiled Evaluators glmeval:\\nglmevalprovide feedback crucial for different work-\\nflows. They evaluate actions and states in search-based\\nworkflows (Hao et al., 2023; Yao et al., 2023a) and re-\\nvise decisions in feedback-learning workflows (Shinn\\net al., 2023; Wang et al., 2023b) (refer to §4 for more\\ndetails). These evaluators are integral to both direct\\naction assessment and broader strategic adjustments.\\n3) LLM-Profiled Dynamic Models glmdynamic : They\\npredict or describe changes to the environment. Gen-\\nerally, dynamic models form part of a comprehensive')],\n",
       " 'answer': {'answer': 'Reusable LLM-profiled components (LMPCs) include various task-agnostic components commonly used across different workflows. These components are: 1) LLM-Profiled Policy (glmpolicy) for generating decisions or actions; 2) LLM-Profiled Evaluators (glmeval) for providing feedback on actions and states; and 3) LLM-Profiled Dynamic Models (glmdynamic) for predicting changes in the environment.',\n",
       "  'sources': ['Li 2024']}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://python.langchain.com/v0.2/docs/how_to/qa_sources/\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# Desired schema for response\n",
    "class AnswerWithSources(TypedDict):\n",
    "    \"\"\"An answer to the question, with sources.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    sources: Annotated[\n",
    "        List[str],\n",
    "        ...,\n",
    "        \"List of sources (author + year) used to answer the question\",\n",
    "    ]\n",
    "\n",
    "\n",
    "# Our rag_chain_from_docs has the following changes:\n",
    "# - add `.with_structured_output` to the LLM;\n",
    "# - remove the output parser\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"context\": lambda x: format_docs(x[\"context\"]),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.with_structured_output(AnswerWithSources)\n",
    ")\n",
    "\n",
    "retrieve_docs = (lambda x: x[\"input\"]) | retriever\n",
    "\n",
    "chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"input\": query})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb \n",
    "Below are notebook from openai cookbook on these topics of search and embeddings:\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Code_search.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb\n",
    "- https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "- [In-context learning abilities of ChatGPT models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "- [Issue with long context](https://arxiv.org/pdf/2303.18223.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-chatgpt-apps",
   "language": "python",
   "name": "oreilly-chatgpt-apps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
