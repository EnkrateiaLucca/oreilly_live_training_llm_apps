{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-07-24-10-52-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple QA System for Chatting with a PDF\n",
    "\n",
    "This part of the training will be mostly hands on with the code for building the qa PDF system with langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.2.14\n",
    "!pip install langchain-openai==0.1.8\n",
    "!pip install langchainhub==0.1.20\n",
    "!pip install pypdf==4.2.0\n",
    "!pip install chromadb==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# # Set OPENAI API Key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your openai key\"\n",
    "\n",
    "# OR (load from .env file)\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./assets-resources/paper-llm-components.pdf\"\n",
    "loader = PyPDFLoader(pdf_path) # LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 0}, page_content='A Survey on LLM-Based Agents: Common Workflows and Reusable\\nLLM-Profiled Components\\nXinzhe Li\\nSchool of IT, Deakin University, Australia\\nlixinzhe@deakin.edu.au\\nAbstract\\nRecent advancements in Large Language Mod-\\nels (LLMs) have catalyzed the development of so-\\nphisticated frameworks for developing LLM-based\\nagents. However, the complexity of these frame-\\nworks r poses a hurdle for nuanced differentiation\\nat a granular level, a critical aspect for enabling\\nefficient implementations across different frame-\\nworks and fostering future research. Hence, the\\nprimary purpose of this survey is to facilitate a co-\\nhesive understanding of diverse recently proposed\\nframeworks by identifying common workflows and\\nreusable LLM-Profiled Components (LMPCs).\\n1 Introduction\\nGenerative Large Language Models (GLMs or LLMs)\\nhave acquired extensive general knowledge and\\nhuman-like reasoning capabilities (Santurkar et al.,\\n2023; Wang et al., 2022; Zhong et al., 2022, 2023),\\npositioning them as pivotal in constructing AI agents\\nknown as LLM-based agents. In the context of this\\nsurvey, LLM-based agents are defined by their abil-\\nity to interact actively with external tools (such as\\nWikipedia) or environments (such as householding en-\\nvironments) and are designed to function as integral\\ncomponents of agency, including acting, planning, and\\nevaluating.\\nPurpose of the Survey The motivation behind this\\nsurvey stems from the observation that many LLM-\\nbased agents incorporate similar workflows and com-\\nponents, despite the presence of a wide variety of\\ntechnical and conceptual challenges, e.g., search algo-\\nrithms (Yao et al., 2023a), tree structures (Hao et al.,\\n2023), and Reinforcement Learning (RL) components\\n(Shinn et al., 2023). (Wu et al., 2023) offer a modular\\napproach but lack integration with prevalent agentic\\nworkflows. Wang et al. (2024) provide a comprehen-\\nsive review of LLM agents, exploring their capabil-\\nities across profiling, memory, planning, and action.\\nIn contrast, our survey does not attempt to cover all\\ncomponents of LLM-based agents comprehensively.\\nInstead, we concentrate on the involvement of LLMswithin agentic workflows and aim to clarify the roles\\nof LLMs in agent implementations. We create com-\\nmon workflows incorporating reusable LLM-Profiled\\nComponents (LMPCs), as depicted in Figure 1.\\nContributions This survey offers the following con-\\ntributions. 1) Alleviating the understanding of com-\\nplex frameworks : The complexity of existing frame-\\nworks can be simplified into implementable workflows,\\nespecially when they are extracted for specific tasks.\\nThis survey emphasizes reusable workflows and LM-\\nPCs across popular frameworks, such as ReAct (Yao\\net al., 2023b), Reflexion (Shinn et al., 2023) and Tree-\\nof-Thoughts (Yao et al., 2023a). Specifically, based\\non the interaction environments (§2) and the use of\\ncommon LMPCs (§3), we categorize and detail vari-\\nous workflows, e.g., tool-use workflows, search work-\\nflows, and feedback-learning workflows. Many ex-\\nisting frameworks are composed of these workflows\\nand LMPCs, along with some specific non-LLM com-\\nponents. 2) Helping researchers/practitioners as-\\nsess current frameworks at a more granular and\\ncohesive level : Section 4 categorizes prominent frame-\\nworks and demonstrates how they are assembled by the\\ncommon workflows and LMPCs, as summarized in Ta-\\nble 21.3) Facilitating further extensions of existing\\nframeworks : Existing frameworks could be modi-\\nfied by changing the implementations of LMPCs. To\\nenable this, we not only summarize implementations\\nof LMPCs but also their applicability across diverse\\nworkflows and tasks in Section 5.\\n2 Task Environments And Tool\\nEnvironments\\nThis section explores task environments and tool envi-\\nronments, which present different settings compared to\\ntraditional AI and reinforcement learning (RL) agent\\nframeworks (Russell and Norvig, 2010; Sutton and\\nBarto, 2018) . After a brief overview of standard logic-'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 0}, page_content='traditional AI and reinforcement learning (RL) agent\\nframeworks (Russell and Norvig, 2010; Sutton and\\nBarto, 2018) . After a brief overview of standard logic-\\nbased gaming and simulated embodied environments,\\nwe focus on two specific areas: Natural Language\\n1A more detailed summarization is demonstrated in Ap-\\npendix A\\n1arXiv:2406.05804v2  [cs.AI]  16 Jun 2024'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 1}, page_content='(a) Policy-Only Workflows.\\n(b) Search Workflows.\\n(c) Feedback-Learning Workflows.\\nFigure 1: Eight Common Workflows based on Three LLM-profiled Components (Policy, Evaluator and Dynamic Model)\\nunder Task or/and Tool-Use Environments.\\nInteraction Environments (NLIEs) and Tool Environ-\\nments.2.1 Typical Task Environments\\nTypically, there are two common types of task environ-\\nments: 1) Rule-Based Gaming Environments : These\\nenvironments, which are deterministic and fully ob-\\n2'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 2}, page_content='Env Types Entities Interacted\\nWith by AgentAction Properties Examples of Action\\nInstancesExamples of Env Instances\\nTask Environments\\nGaming\\nEnvironmentsVirtual game\\nelements (objects,\\navatars, other\\ncharacters), and\\npossibly other\\nplayers or game\\nnarrativesDiscrete,\\nExecutable,\\nDeterministicMove(Right) BlocksWorld, CrossWords\\nEmbodied\\nEnvironmentsPhysical world\\n(through sensors\\nand actuators)Discrete,\\nExecutable,\\nDeterministicPick_Up[Object] AlfWorld (Shridhar et al., 2021),\\nVirtualHome,\\nMinecraft (Fan et al., 2022)\\nNLIEs Humans (through\\nconversation or\\ntext)Free-form,\\nDiscrete,\\nDeterministic\\n(Single-step QA)\\nStochastic\\n(Multi-step)The answer is Answer\\nFinish[Answer]GSM8K,\\nHotpotQA\\nTool Environments ( Nested with Task Environments)\\nRetrieval Retrieval Discrete,\\nExecutable,\\nDeterministic,\\nNon-State-\\nAlteringWiki_Search[Entity] A Wikipedia API (Goldsmith,\\n2023) (used by ReAct (Yao\\net al., 2023b))\\nCalculator Calculator Executable,\\nDeterministic,\\nNon-State-\\nAltering2 x 62 = « Calculator » Python’s eval function (used\\nby MultiTool-CoT (Inaba et al.,\\n2023))\\nTable 1: Common task environments and tool-use environments. We categorize all the benchmarks existing in the work of\\nthe 12 agentic workflows into four environment types. An action instance is commonly formalized by action predicates and\\naction arguments. Tool use can be considered internal environments of an agent and commonly defined for QA tasks under\\nNLIEs.\\nservable, include a variety of abstract strategy games\\nlike Chess and Go, and logic puzzles such as the Game\\nof 24 (Yao et al., 2023a) and Blocksworld (Hao et al.,\\n2023). They demand deep logical reasoning and strate-\\ngic planning to navigate and solve. 2) Simulated\\nEmbodied Environments : These settings simulate\\nreal-world physical interactions and spatial relation-\\nships. They require agents to engage in navigation,\\nobject manipulation, and other complex physical tasks,\\nreflecting changes in the physical environment.\\n2.2 Natural Language Interaction Environments\\nWith the rise of LLM agents, there is a growing trend\\namong NLP researchers to recontextualize typical\\nNLP tasks as agentic environments (Yao et al., 2023b;\\nHao et al., 2023; Yao et al., 2023a). These settings are\\nreferred to as Natural Language Interaction Environ-\\nments in our survey.\\nIn NLIEs, the environment remains static until the\\nagent acts. Unlike typical task environments where\\nnatural language serves as an intermediary, in NLIEs,\\nboth the states and actions are defined linguistically,\\nmaking the states conceptual and the actions oftenambiguous and broadly defined.\\nSingle-Step NLIEs for Question Answering Many\\nworks (Yao et al., 2023b; Shinn et al., 2023) formu-\\nlate the traditional QA setup as a single-step decision-\\nmaking process, where the agent generates an answer\\nin response to a question. The process starts with the\\nquestion as the initial state and concludes when the\\nanswer is provided as an action.\\nDeliberate Multi-step NLIEs For tasks where \"in-\\ntermediate steps are not explicitly defined\", several\\nstudies have transformed NLP tasks into a Markov\\nDecision Process to facilitate agentic workflows. For\\nexample, Hao et al. (2023) reformulate subquestions in\\nQA tasks as actions, enabling responses to user queries\\nthrough a multi-step process. This approach allows\\nthe initial question to serve as the beginning of a series\\nof state transitions. Actions may vary from providing\\ndirect, free-form answers in single-step QA to strategi-\\ncally formulating subquestions that navigate the agent\\nthrough sequential updates toward a comprehensive\\nsolution. This method aligns more closely with a se-\\nquential decision-making process, making it apt for\\n3'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 3}, page_content='deployment in planning-based agent systems. Addi-\\ntionally, Wan et al. (2024) suggest that \"splitting an\\noutput sequence into tokens might be a good choice\"\\nfor defining multi-step NLIEs methodically. Further-\\nmore, Yao et al. (2023a) formulate two-step NLIEs for\\ncreative writing by segmenting the problem-solving\\nprocess into distinct planning and execution phases.\\n2.3 Tool Environments\\nModern LLM agents are often enhanced with external\\ntools that improve their problem-solving capabilities\\n(Inaba et al., 2023; Yao et al., 2023b). The design and\\nintegration of these tools add complexity, requiring\\ncareful consideration of how LLMs interact not only\\nwith the task environments but also with these aux-\\niliary tools. Typically, actions in tool environments\\ninvolve interactions with resources that remain unaf-\\nfected by these interactions. For instance, retrieving\\ndata from Wikipedia constitutes a \"read-only\" action,\\nwhich does not modify the Wikipedia database. This\\nfeature distinguishes such tool-use actions from those\\nin conventional task environments or typical reinforce-\\nment learning (RL) settings, where actions generally\\nalter the environmental state. Nevertheless, it is impor-\\ntant to recognize that tool environment can be dynamic\\nthat can undergo changes externally. This aspect re-\\nflects the nature that tools should be considered ex-\\nternal environments rather than the agent’s internal\\nprocesses.\\nNested NLIE-QA + Tool Environments Tool envi-\\nronments are frequently established along with NLIEs\\nto aid in solving QA tasks. Shinn et al. (2023); Yao\\net al. (2023b) incorporate tools to enhance the fac-\\ntuality of responses. They define command-like ac-\\ntions such as “Search” and “LookUp” to interact with\\nWikipedia, with “Search” suggesting the top-5 similar\\nentities from the relevant wiki page, and “LookUp”\\nsimulating the Ctrl+F functionality in a browser. Be-\\nyond simple retrieval, Thoppilan et al. (2022) include\\na language translator and a calculator for dialog tasks.\\nSimilarly, Inaba et al. (2023) employ a calculator, im-\\nplemented using the Python eval function, to resolve\\nnumerical queries within the NumGLUE benchmark.\\n3 LLM-Profiled Components\\nThis section explores common agentic roles for which\\nLLMs are typically profiled. The components leverage\\nthe internal commonsense knowledge and reasoning\\nabilities of LLMs to generate actions, plans, estimate\\nvalues2, and infer subsequent states.\\n2Values refer to the estimated rewards (a quantitative measure\\nof the success or desirability of the outcomes) associated with tak-\\ning a certain action in a state, widely used in typical RL and MDPUniversal LLM-Profiled Components Specifically,\\nthe following task-agnostic components are profiled\\nand commonly used across various workflows. 1)\\nLLM-Profiled Policy glmpolicy: Policy models are de-\\nsigned to generate decisions, which could be an action\\nor a series of actions (plans) for execution in exter-\\nnal environments or use in search and planning algo-\\nrithms.3In contrast to typical RL policy models,\\nwhich learn to maximize cumulative rewards through\\ntrial and error, LLM-profiled policy models, denoted\\nasglmpolicy, utilize pre-trained knowledge and com-\\nmonsense derived from extensive textual data. We\\ndistinguish between two types of glmpolicy: an actor\\nglmactordirectly maps a state to an action, whereas\\na planner glmplanner generates a sequence of actions\\nfrom a given state. 2) LLM-Profiled Evaluators glmeval:\\nglmevalprovide feedback crucial for different work-\\nflows. They evaluate actions and states in search-based\\nworkflows (Hao et al., 2023; Yao et al., 2023a) and re-\\nvise decisions in feedback-learning workflows (Shinn\\net al., 2023; Wang et al., 2023b) (refer to §4 for more\\ndetails). These evaluators are integral to both direct\\naction assessment and broader strategic adjustments.\\n3) LLM-Profiled Dynamic Models glmdynamic : They\\npredict or describe changes to the environment. Gen-\\nerally, dynamic models form part of a comprehensive'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 3}, page_content='3) LLM-Profiled Dynamic Models glmdynamic : They\\npredict or describe changes to the environment. Gen-\\nerally, dynamic models form part of a comprehensive\\nworld model by predicting the next state s′from the\\ncurrent state sand action a. While typical RL uses\\nthe probability distribution p(s′|s, a)to model poten-\\ntial next states, LLM-based dynamic models directly\\npredict the next state s′=glmdynamic (s, a).\\nTask-Dependent LLM-Profiled Components In\\naddition to the universal components, certain LLM-\\nprofiled components are tailored to specific tasks. For\\ninstance, verbalizers are crucial in embodied environ-\\nments but unnecessary in NLIEs. A verbalizer trans-\\nlates actions and observations into inputs for planners;\\nfor example, in the Planner-Actor-Reporter workflow\\n(Wang et al., 2023a), a fine-tuned Visual Language\\nModel (VLM) along with glmplanner translates pixel\\nstates into textual inputs. Similarly, if environmental\\nfeedback is perceivable along with states, a verbalizer\\nmay be needed to translate this feedback into verbal\\ndescriptions for glmpolicy, akin to reward shaping in\\nRL where numerical stimuli are generated for policy\\nlearning. LLMs profiled as verbalizers, glmverbalizer\\n(Shinn et al., 2023), often guide descriptions accord-\\ning to specified criteria.\\nsettings to learn policy models that perform desirable behaviors.\\n3Note that planning algorithms may be utilized to structure a\\nplan of plans; for example, Tree-of-Thought employs tree search,\\nwhere each node potentially represents either a single action or an\\nentire plan.\\n4'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 4}, page_content='Workflows Related Frameworks LMPCs Applicable Environments\\nPolicy-only Base LLM Planner (Huang et al.,\\n2022), DEPS (Wang et al.,\\n2023b), Planner-Actor-Reporter\\n(Dasgupta et al., 2022), Plan-\\nand-solve (Wang et al., 2023a),\\nReAct (Yao et al., 2023b)glmplanner ,\\nglmaction_selector (Optional\\nfor Embodied Env),\\nglmverbalizer (Optional for\\nEmbodied Env)Embodied Env, NLIEs-\\nWriting, NLIE-QA\\nTool-Use MultiTool-CoT (Inaba et al.,\\n2023), ReAct (Yao et al.,\\n2023b), Reflexion (Shinn et al.,\\n2023)glmactor NLIEs\\nSearch Traversal & Heuristic Tree-of-Thoughts (ToT) (Yao\\net al., 2023a), Tree-BeamSearch\\n(Xie et al., 2023), Boost-of-\\nThoughts (Chen et al., 2024)glmactor or glmplanner ,\\nglmevalGaming, NLIEs-Writing,\\nNLIE-QA\\nMCTS RAP (Hao et al., 2023), Wan\\net al. (2024)glmactor,glmdynamic ,glmeval Gaming, NLIEs-QA\\nFeedback\\nLearningfrom glmeval Reflexion (Shinn et al., 2023),\\nSelf-refine (Madaan et al., 2023)glmactor, glmeval NLIEs-QA\\nfrom glmeval& Task Env Reflexion (Shinn et al., 2023) glmactor, glmeval,\\nglmfb_verbalizerEmbodied Env\\nfrom Tools & Humans Guan et al. (2023) glmplanner , glmpddl_translator Embodied Env\\nfrom Tools & glmeval CRITIC (Gou et al., 2024) glmactor, glmeval NLIEs\\nTable 2: Workflows of LLM-Based Agents and Related Frameworks. We summarize LLM-Profiled Components (LMPCs)\\nand applicable environments based on the original papers of the listed frameworks, although other possibilities exist, e.g.,\\nusing glmplanner within the tool-use workflow and applying the base workflow to gaming environments.\\n4 Workflows of LLM-Based Agents\\nThis section explores different workflows and the uti-\\nlization of various LLM-Profiled Components (LM-\\nPCs), as illustrated in Figure 1. Specifically, we will\\naggregate proposed frameworks based on workflow\\ntypes, as demonstrated in Table 2. This section delves\\ninto diverse workflows and the application of distinct\\nLLM-Profiled Components (LMPCs), as visualized\\nin Figure 1. We categorize and consolidate the re-\\nlated frameworks according to the workflows, exem-\\nplified in Table 2. It is noteworthy that one framework\\ncould employ distinct workflows according to tasks\\nor environments. More details of the frameworks are\\nsummarized in Appendix A.\\n4.1 Policy-Only Workflows\\nBase and tool-use workflows only require LLMs to\\nbe profiled as policy models. In the realm of embod-\\nied tasks, many projects deploy base workflows with\\nglmplanner to generate plans using LLM agents, such as\\nthe LLM Planner (Huang et al., 2022), Planner-Actor-\\nReporter (Dasgupta et al., 2022), and DEPS (Wang\\net al., 2023b). The Plan-and-solve approach (Wang\\net al., 2023a) applies a base workflow to NLIEs-QA.\\nSuch base workflow can also be applied to other NLIE\\ntasks, e.g., creative writing (Yao et al., 2023a). In\\ncontrast, the tool-use workflow with glmactoris always\\napplied to NLIEs like ReAct (Yao et al., 2023b), Re-flexion (Shinn et al., 2023), and MultiTool-CoT (Inaba\\net al., 2023).\\n4.2 Search Workflows\\nUnlike base agents with glmplanner , which generates\\na sequence of actions for a plan at one generation,\\nactions in search workflows are organized into tree\\n(Yao et al., 2023a; Hao et al., 2023) and graph (Liu\\net al., 2023) for exploration. Planning or search algo-\\nrithms can explore sequential decisions in a non-linear\\nmanner. During the process, the tree (or solution)\\nis constructed by adding nodes, each representing a\\npartial solution with the input and the sequence of\\nthoughts/actions so far. Data structures such as trees\\nenable strategic searches over actions derived from\\nmultiple reasoning paths. This is accomplished using\\nalgorithms like beam search (Xie et al., 2023), depth-\\nfirst and breadth-first search (DFS and BFS) (Yao et al.,\\n2023a), and Monte-Carlo Tree Search (MCTS) (Hao\\net al., 2023).\\nGenerally, LMPCs are used to explore the path to-\\nwards the goal. Instead of directly applying actions on\\nexternal environments within policy-only workflows,\\nglmpolicy generates multiple action samples to facilitate'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 4}, page_content='wards the goal. Instead of directly applying actions on\\nexternal environments within policy-only workflows,\\nglmpolicy generates multiple action samples to facilitate\\naction selection for a search process, while glmevalis\\nused to calculate values for action/state evaluation dur-\\ning exploration (Yao et al., 2023a; Chen et al., 2024)\\nor as a reward model (Hao et al., 2023).\\n5'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 5}, page_content='Search via Traversal and Heuristic The Tree-of-\\nThoughts (ToT) workflow (Yao et al., 2023a) uses\\nglmpolicy to expand nodes over a tree, and glmevalpro-\\nvides a fixed value estimate to select a node for further\\nexpansion. To expand a tree, the Tree-BeamSearch\\nworkflow (Xie et al., 2023) employs beam search,\\nwhile ToT appies depth-/breadth-first search (DFS and\\nBFS). However, the BFS is indeed beam search with N\\nbeams since the values generated by the utility model\\nglmevalto maintain the Nmost promising nodes.4\\nSimulated-Based Search via MCTS The RAP\\nworkflow (Hao et al., 2023) also builds a tree through\\nsearching and includes glmpolicy andglmevalto expand\\nnodes. However, through the employment of MCTS,\\na simulation-driven search strategy, the nodes chosen\\nfor expansion are determined not only by the static\\noutputs from glmevalor other heuristics (whether they\\nindicate goal attainment), but also by the cumulative\\nstatistics accrued over multiple simulations. Specif-\\nically, nodes that lead to better average rewards for\\nsubsequent nodes across all simulations (or trajecto-\\nries) are indeed more likely to be expanded further.\\nFollowing the selection phase, glmpolicy participates\\nin action sampling during the expansion phase. Then,\\nit collaborates intimately with glmdynamic andglmeval\\nin the simulation phase, functioning as the roll-out\\npolicy. Specifically, glmpolicy samples an action at\\ngiven the current state st, which in turn, is assessed\\nbyglmeval. The top-scoring action is selected, with\\nglmdynamic using it to derive st+1, iteratively simulat-\\ning the trajectory.\\n4.3 Feedback-Learning Workflows\\nThere are primarily four main sources of feedback:\\nglmeval(internal feedback), humans, task environ-\\nments, and tools.\\nReflexion (Shinn et al., 2023) and Self-Refine\\n(Madaan et al., 2023) utilize glmevalto reflect on the\\nprior generations of glmpolicy, enabling glmpolicy to\\nlearn from such reflections. Unlike search workflows\\nwhere the outputs of glmevalare employed for ac-\\ntion selection during tree expansion, here, the feed-\\nback is used to revise one complete decision, allowing\\nglmpolicy to re-generate another. In tasks that involve\\nphysical interaction, glmevalin Reflexion also inte-\\ngrates external information from the task environments\\n(Shinn et al., 2023). Similarly, glmevalcan receive in-\\nformation from tools to generate feedback, as demon-\\nstrated in the CRITIC workflow (Gou et al., 2024). In\\nthis setup, the necessity of invoking tools for feedback\\n4Typically, BFS does not use a utility model to decide which\\nnodes to expand because it systematically explores all possible\\nnodes at each depth.is autonomously determined by glmeval(See Table 15\\nfor an example), whereas in Reflexion, feedback trans-\\nmission is hardcoded by the workflow design. Humans\\ncould provide direct feedback to glmpolicy without the\\nneed of glmeval, as noted by the workflow from Guan\\net al. (2023).\\n5 Implementations of LMPCs\\nIn this section, we explore different implementation\\napproaches for LMPCs, covering strategies that are\\nindependent of specific workflows and tasks, imple-\\nmentations specifically designed for certain tasks, and\\nthose tailored to particular workflows.\\n5.1 Universal Implementations\\nLLMs are normally profiled by leveraging Chain-of-\\nThought (CoT) prompting to transcend basic input-\\noutput inference, facilitating the creation of intermedi-\\nate reasoning steps. This is achieved through two tech-\\nniques: Zero-shot CoT integrates a CoT trigger, such\\nas “Let’s think step-by-step” (Kojima et al., 2022),\\nwithin task instructions, while while few-shot CoT\\nincorporates handcrafted reasoning steps within pro-\\nvided examples for in-context learning (Wei et al.,\\n2022).\\nAs shown in Table 3, some studies (Wang et al.,\\n2023a) employ zero-shot CoT prompting, but most\\n(Yao et al., 2023b; Shinn et al., 2023; Hao et al.,\\n2023) implement LLM policy models via few-shot\\nCoT. Zero-shot CoT implementation of glmplanner of-'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 5}, page_content='2023a) employ zero-shot CoT prompting, but most\\n(Yao et al., 2023b; Shinn et al., 2023; Hao et al.,\\n2023) implement LLM policy models via few-shot\\nCoT. Zero-shot CoT implementation of glmplanner of-\\nten fails to produce long-horizon plans, unlike few-\\nshot CoT prompting (Wang et al., 2023b). While effec-\\ntive, few-shot prompting requires manual compilation\\nof demonstrations with reasoning sequences, leading\\nto increased manual work and computational resource\\nuse. Methods like Auto CoTs (Zhang et al., 2023) that\\nautomatically generate few-shot demonstrations could\\nmitigate this challenge.\\n5.2 Workflow-Specific Implementations\\nglmpolicy Implementations Two distinct implemen-\\ntations can be employed to enable glmpolicy to trigger\\ntool usage within tool-use and feedback learning work-\\nflows (receiving feedback from tools).\\n1.Using In-Generation Triggers : Tools could be\\ninvoked during the reasoning generation process,\\ne.g., MultiTool-CoT (Inaba et al., 2023). The\\nagent program monitors each token produced,\\npausing text generation when a tool trigger is\\ndetected. This pause allows for the invocation\\nof tools, whose outputs are then inserted into\\nthe prompt to complete the reasoning. The trig-\\ngers for these tools are defined either through\\n6'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 6}, page_content='Prompting Example Works Example Prompts\\n(in Appendix)\\nglmactor Few-shot ReAct (Yao et al., 2023b), Reflexion (Shinn et al., 2023), RAP (Hao et al., 2023),\\nMultiTool-CoT (Inaba et al., 2023)Table 8, 10\\nglmplanner Zero-shot Plan-and-Solve (Wang et al., 2023a), LLM Planner (Huang et al., 2022) Table 6\\nFew-shot DEPS (Wang et al., 2023b), Planner-Actor-Reporter (Dasgupta et al., 2022)\\nglmevaluator Few-shot RAP (Hao et al., 2023), Tree-BeamSearch (Xie et al., 2023), Reflexion (Shinn\\net al., 2023), CRITIC (Gou et al., 2024)Table 12, 13\\nglmdynamic Few-shot RAP (Hao et al., 2023) Table 16\\nTable 3: Prompting Methods of LLM-Profiled Components\\nTask Formulation Feedback Types Applicable Workflows Example Works\\nText Generation Free-form reflection Feedback-learning\\nworkflowsSelf-Refine (Madaan et al., 2023), Reflexion (Shinn\\net al., 2023), CRITIC (Gou et al., 2024)\\nBinary/Multi-class\\nClassificationDiscrete values Search workflows RAP (Hao et al., 2023), Tree-BeamSearch (Xie et al.,\\n2023) ToT (Yao et al., 2023a)\\nBinary Classifica-\\ntionContinuous values (log-\\nits)Search workflow via\\nMCTSRAP (Hao et al., 2023)\\nMulti-choice QA Choices of top-N ac-\\ntionsSearch workflows via\\ntraversal and heuristicToT (Yao et al., 2023a)\\nTable 4: Workflow-Specific LLM-Profiled Evaluators According to Task Formulation and Feedback Types\\ntool descriptions, few-shot demonstrations5, or a\\ncombination of both6.\\n2.Reasoning-Acting (ReAct) Strategy for Tool\\nUse: Introduced by Yao et al. (2023b), each rea-\\nsoning or acting step is separately verbalized\\nvia a complete generation. Although the ReAct\\nframework (Yao et al., 2023b) unifies tool actions\\nwithin the tool-use workflow and task-specific\\nactions within the base workflow, we argue that\\nthe strategies for tool actions and task-specific\\nactions should be distinguished. Because tool\\nactions and task-specific actions are triggered dif-\\nferently, an aspect that will be emphasized further\\nin the subsequent section about task-specific im-\\nplementations.\\nglmeval Implementations Commonly, different\\nworkflows require distinct feedback types and task for-\\nmulations. There are four scenarios regarding the two\\nperspectives, as summarized in Table 4: 1) Generat-\\ning free-form reflection : This reflective output is fre-\\nquently integrated into the prompt of glmpolicy within\\nfeedback-learning workflows (Shinn et al., 2023; Gou\\net al., 2024). glmevalis designed to reflect on previous\\nstates and actions within feedback-learning workflows.\\nDepending on specific feedback-learning workflows,\\nit could incorporate external inputs from task or tool\\nenvironments to enrich the reflection process. 2) Bi-\\nnary/Multiclass Classification : Feedback is obtained\\n5In-generation triggers via few-shot demos: see an example\\nprompt in Table 15)\\n6In-generation triggers via descriptions and few-shot demos:\\nsee an example prompt in Table 8)from discrete output tokens, commonly \"no\" or \"yes.\"\\nThese can be converted into 0/1 values for search work-\\nflows. This scalar values can be employed as reward\\nsignals within Monte Carlo Tree Search (MCTS) sim-\\nulations (Hao et al., 2023), or they may be employed\\ndirectly to guide decision-making at each step of tree\\ntraversal (Yao et al., 2023a). 3) Binary classification\\nwith scalar values : This approach differs from the\\nprevious one by employing the logit values of tokens\\nto calculate scalar feedback values. For instance, the\\nprobability of a “yes” response is computed using the\\nformula:\\nv=probs\"yes\"=el\"yes\"\\nel\"yes\"+el\"no\",\\nwhere l\"yes\" andl\"no\"are the logits for the “yes” and\\n“no” tokens, respectively.7These scalar values can\\nthen be utilized as rewards in search workflows for\\nMCTS. 4) Multi-choice QA : Employed in settings\\nwhere a selection from multiple choices is required,\\nsupporting tasks that involve choosing from top-N\\npossible actions, as utilized in search workflows for\\naction selection (Yao et al., 2023a).\\n5.3 Task-Specific Implementations\\nglmpolicy Implementations The noteworthy details'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 6}, page_content='possible actions, as utilized in search workflows for\\naction selection (Yao et al., 2023a).\\n5.3 Task-Specific Implementations\\nglmpolicy Implementations The noteworthy details\\ninclude: 1) Multi-Step Generation : For tasks that\\ninherently involve sequential decision-making (e.g.,\\n“put a cool tomato in the microwave”), post-processing\\nsteps are often required for glmpolicy to work as a pol-\\nicy model. After a reasoning path is generated via\\n7Note that such implementations of glmevalare not accessible\\nvia black-box LLMs.\\n7'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 7}, page_content='CoT implementations of glmpolicy, a subsequent call\\nis made to extract executable actions. Besides, the\\nfirst generation of glmplanner often contains high-level\\nactions (HLA) that must be further transformed into\\nprimitive actions, before executable actions are ex-\\ntracted. This is, normally, unnecessary for tasks from\\nNLIEs since both plan generation and execution may\\noccur within a single LLM generation8.2) Implicit\\nPlanning : Another point to consider is that although\\nglmactorare not explicitly implemented to generate\\nplans, it may autonomously formulate plans during the\\nreasoning phase before deciding on the current action\\n(Shinn et al., 2023; Yao et al., 2023b)9. These gen-\\nerated plans are maintained as internal states and do\\nnot serve as communication signals with other compo-\\nnents.\\nReAct Implementation of glmactor The ReAct\\nframework (Yao et al., 2023b) unifies tool actions\\nwithin the tool-use workflow and task-specific actions\\nwithin the base workflow. However, the sequence in\\nwhich reasoning and action outputs alternate is task-\\ndependent. For question answering (QA), the gener-\\nations of reasoning steps and tool actions are fixed,\\nwith alternating prompts for thinking and acting.10.\\nIn contrast, for embodied tasks, the decision whether\\nto proceed with thinking or acting in the next step is\\nautonomously determined by glmpolicy11.\\nglmevalImplementations glmevalcan be configured\\nto assess different task-specific perspectives, with spe-\\ncific agentic prompts determining the evaluation cri-\\nteria. Generally, usefulness could be used to evaluate\\nany actions (Hao et al., 2023)12. In NLIE-QA scenar-\\nios, a common metric is the factuality (truthfulness) of\\nresponses (Gou et al., 2024)13.\\n6 Future Work\\nAs we delve deeper into LMPCs and agentic work-\\nflows, several key directions for future research are\\nidentified to advance the development of fully au-\\ntonomous agents across various tasks.\\nUniversal Tool Use One direction is to move be-\\nyond predefined tool use for specific tasks and develop\\n8Simultaneous plan generation and execution: See an example\\nprompt in Table 6\\n9Implicit Planning: See an example prompt in Table 7\\n10Reasoning-Acting Strategy (QA tasks): See an example\\nprompt in Table 10\\n11Reasoning-Acting Strategy (embodied tasks): See an example\\nprompt in Table 7\\n12Evaluation Criteria (Usefulness): See an example prompt in\\nTable 12\\n13Evaluation Criteria (Factuality/Truthfulness): See example\\nprompts in Table 14 and 15strategies that enable LLMs to autonomously deter-\\nmine tool usage based on the specific requirements\\nof the task at hand. Another direction is to integrate\\ntool use for both policy models and evaluators. In\\nother words, LLMs could reason over the use of the\\ntool across various tasks and flexibly jump between\\ndifferent roles. The insight of its potential is detailed\\nin Appendix C.\\nUnified Frameworks Across Tasks As detailed in\\n§5, despite many frameworks are presented with inte-\\ngrated conceptual workflows, many components are\\ntask-specific. For example, ReAct seeks to harmonize\\ntool actions and task-specific actions. However, these\\nworkflows and specific implementations of LMPCs\\nare different across tasks, such as hardcoded reasoning\\nand acting steps in NLIE-QA versus autonomously\\ndetermined reasoning and acting steps in embodied en-\\nvironments. Similarly, although the feedback-learning\\nloop in Reflexion is theoretically unified, in practice,\\nexternal feedback is generated only in embodied envi-\\nronments, not in NLIE-QA.\\nReducing Bandwidth There are several potential\\nstrategies for reducing the bandwidth required for\\nLLM inference14, including using Stochastic glmactor\\n(Details in Appendix D).\\n7 Conclusion\\nThis survey provides a summary of common work-\\nflows and LLM-Profiled Components to encourage\\nthe reuse of these components and the expansion of\\nexisting workflows through the integration of both\\ntask-specific LMPCs and non-LLM components. This\\napproach aims to foster the development and repro-'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 7}, page_content='existing workflows through the integration of both\\ntask-specific LMPCs and non-LLM components. This\\napproach aims to foster the development and repro-\\nducibility of agentic workflows.\\nLimitations\\nThis survey omits discussions on memory design15\\nand the integration of peripheral components into agen-\\ntic workflows16, as our focus is on the details of\\ncommon LLM-profiled components within agentic\\nworkflows to facilitate the implementation of reusable\\ncomponents and extensible workflows. This distinctly\\nsets our work apart from other surveys.\\nReferences\\nSijia Chen, Baochun Li, and Di Niu. 2024. Boosting of\\nthoughts: Trial-and-error problem solving with large lan-\\n14Here, bandwidth refers to the volume of information pro-\\ncessed during a single LLM generation\\n15Appendix E provides a brief discussion on memory in LLM-\\nbased agents\\n16These are concisely summarized in Appendix A\\n8'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 8}, page_content='guage models. In The Twelfth International Conference\\non Learning Representations .\\nIshita Dasgupta, Christine Kaeser-Chen, Kenneth Marino,\\nArun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus.\\n2022. Collaborating with language models for embod-\\nied reasoning. In Second Workshop on Language and\\nReinforcement Learning .\\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar,\\nYuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\\nYuke Zhu, and Anima Anandkumar. 2022. Minedojo:\\nBuilding open-ended embodied agents with internet-\\nscale knowledge. In Thirty-sixth Conference on Neural\\nInformation Processing Systems Datasets and Bench-\\nmarks Track .\\nJonathan Goldsmith. 2023. Wikipedia: A python li-\\nbrary that makes it easy to access and parse data from\\nwikipedia. Python Package Index.\\nZhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yu-\\njiu Yang, Nan Duan, and Weizhu Chen. 2024. CRITIC:\\nLarge language models can self-correct with tool-\\ninteractive critiquing. In The Twelfth International Con-\\nference on Learning Representations .\\nLin Guan, Karthik Valmeekam, Sarath Sreedharan, and\\nSubbarao Kambhampati. 2023. Leveraging pre-trained\\nlarge language models to construct and utilize world\\nmodels for model-based task planning. In Thirty-seventh\\nConference on Neural Information Processing Systems .\\nShibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang,\\nDaisy Wang, and Zhiting Hu. 2023. Reasoning with\\nlanguage model is planning with world model. In Pro-\\nceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing , pages 8154–8173, Sin-\\ngapore. Association for Computational Linguistics.\\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor\\nMordatch. 2022. Language models as zero-shot planners:\\nExtracting actionable knowledge for embodied agents.\\nInInternational Conference on Machine Learning , pages\\n9118–9147. PMLR.\\nTatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, and Sadao\\nKurohashi. 2023. MultiTool-CoT: GPT-3 can use mul-\\ntiple external tools with chain of thought prompting. In\\nProceedings of the 61st Annual Meeting of the Associ-\\nation for Computational Linguistics (Volume 2: Short\\nPapers) , pages 1522–1532, Toronto, Canada. Associa-\\ntion for Computational Linguistics.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka\\nMatsuo, and Yusuke Iwasawa. 2022. Large language\\nmodels are zero-shot reasoners. In Advances in Neural\\nInformation Processing Systems .\\nHanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang,\\nQiji Zhou, and Yue Zhang. 2023. Logicot: Logical chain-\\nof-thought instruction-tuning.\\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Halli-\\nnan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri,\\nShrimai Prabhumoye, Yiming Yang, et al. 2023. Self-\\nrefine: Iterative refinement with self-feedback. arXiv\\npreprint arXiv:2303.17651 .Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023.\\nSelfcheckgpt: Zero-resource black-box hallucination de-\\ntection for generative large language models. arXiv\\npreprint arXiv:2303.08896 .\\nStuart J Russell and Peter Norvig. 2010. Artificial intelli-\\ngence a modern approach . London.\\nShibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee,\\nPercy Liang, and Tatsunori Hashimoto. 2023. Whose\\nopinions do language models reflect? arXiv preprint\\narXiv:2303.17548 .\\nNoah Shinn, Federico Cassano, Edward Berman, Ashwin\\nGopinath, Karthik Narasimhan, and Shunyu Yao. 2023.\\nReflexion: Language agents with verbal reinforcement\\nlearning.\\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote,\\nYonatan Bisk, Adam Trischler, and Matthew Hausknecht.\\n2021. {ALFW}orld: Aligning text and embodied en-\\nvironments for interactive learning. In International\\nConference on Learning Representations .\\nRichard S Sutton and Andrew G Barto. 2018. Reinforce-\\nment learning: An introduction . MIT press.\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia\\nJin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda:\\nLanguage models for dialog applications. arXiv preprint'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 8}, page_content='Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia\\nJin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda:\\nLanguage models for dialog applications. arXiv preprint\\narXiv:2201.08239 .\\nZiyu Wan, Xidong Feng, Muning Wen, Ying Wen, Weinan\\nZhang, and Jun Wang. 2024. Alphazero-like tree-search\\ncan guide large language model decoding and training.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao\\nYang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,\\nXu Chen, Yankai Lin, et al. 2024. A survey on large\\nlanguage model based autonomous agents. Frontiers of\\nComputer Science , 18(6):1–26.\\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yun-\\nshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023a.\\nPlan-and-solve prompting: Improving zero-shot chain-\\nof-thought reasoning by large language models. In Pro-\\nceedings of the 61st Annual Meeting of the Associa-\\ntion for Computational Linguistics (Volume 1: Long\\nPapers) , pages 2609–2634, Toronto, Canada. Associa-\\ntion for Computational Linguistics.\\nSiyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou,\\nZhongyu Wei, Zhumin Chen, and Nan Duan. 2022. From\\nlsat: The progress and challenges of complex reason-\\ning. IEEE/ACM Trans. Audio, Speech and Lang. Proc. ,\\n30:2201–2216.\\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiao-\\njian Ma, and Yitao Liang. 2023b. Describe, explain, plan\\nand select: Interactive planning with LLMs enables open-\\nworld multi-task agents. In Thirty-seventh Conference\\non Neural Information Processing Systems .\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\\nand Denny Zhou. 2022. Chain of thought prompting\\nelicits reasoning in large language models. In Advances\\nin Neural Information Processing Systems .\\n9'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 9}, page_content='Brandon T Willard and Rémi Louf. 2023. Efficient guided\\ngeneration for llms. arXiv preprint arXiv:2307.09702 .\\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,\\nShaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xi-\\naoyun Zhang, and Chi Wang. 2023. Autogen: Enabling\\nnext-gen llm applications via multi-agent conversation\\nframework. arXiv preprint arXiv:2308.08155 .\\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen\\nKan, Junxian He, and Qizhe Xie. 2023. Self-evaluation\\nguided beam search for reasoning. In Thirty-seventh\\nConference on Neural Information Processing Systems .\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nThomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.\\n2023a. Tree of thoughts: Deliberate problem solving\\nwith large language models.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,\\nKarthik R Narasimhan, and Yuan Cao. 2023b. React:\\nSynergizing reasoning and acting in language models.\\nInThe Eleventh International Conference on Learning\\nRepresentations .\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola.\\n2023. Automatic chain of thought prompting in large lan-\\nguage models. In The Eleventh International Conference\\non Learning Representations (ICLR 2023) .\\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang,\\nShuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and\\nNan Duan. 2023. Agieval: A human-centric bench-\\nmark for evaluating foundation models. arXiv preprint\\narXiv:2304.06364 .\\nWanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya\\nGuo, Yining Chen, Jiahai Wang, Jian Yin, Ming Zhou,\\nand Nan Duan. 2022. Analytical reasoning of text. In\\nFindings of the Association for Computational Linguis-\\ntics: NAACL 2022 , pages 2306–2319, Seattle, United\\nStates. Association for Computational Linguistics.\\nA Frameworks of LLM-Based Agents\\nTable 5 demonstrates the workflow(s), LMPCs, and\\nnon-LMPC components of each framework.\\nB Examples Prompts\\nB.1 LLM-Profiled Policy Models\\n1) Base Workflow : A planner for the NLIE-QA and an\\nactor for the householding environment (ALFRED) are\\ndemonstrated in Table 6 and 7, respectively. 2) Tool-\\nUse Workflow : Actors under NLIE-QA is demon-\\nstrated in Table 8, 9, and 10. The first two tables show\\nthe implementation of in-generation triggers, while\\nthe last one demonstrates the ReAct implementation.\\n3) Search Workflow : An actor is required during the\\nexpansion stage of MCTS in the RAP workflow. The\\nprompts and expected generations are shown in Table\\n11.B.2 Prompts for LLM Evaluators\\nTable 12 and 13 show LLM evaluators that are profiled\\nas classification tasks, while Table 15 demonstrates\\nanother to general free-form text.\\nB.3 Prompts as Dynamic Models\\nTable 16 is profiled as a dynamic model.\\nC Creation of a Task-Agnostic Tool\\nEnvironment\\nPrevious work always limits tools to specific appli-\\ncations like NLIE-QA, future work should aim to\\nestablish a comprehensive tool environment that en-\\ncompasses a wide array of tools suitable for various\\ntasks. A major challenge here is adapting a single\\nactor to utilize such an environment effectively. While\\nin-generation strategies are constrained as triggers are\\ntypically only straightforward for basic tools with sim-\\nple arguments, a reasoning-acting strategy might offer\\nmore promise. Nonetheless, defining tools remains a\\nchallenge, especially in terms of efficient in-context\\nlearning or fine-tuning for tool utilization.\\nD Stochastic glmactor\\nTypically, a single action is sampled from the output of\\nglmactor . Exploring a stochastic glmactor , which pro-\\nvides a distribution over possible actions, can enhance\\nthe stochastic nature of the glmpolicy and improve\\nefficiency. This approach could include investigating\\nconstrained generation techniques (Willard and Louf,\\n2023). Additionally, using such a distribution could\\nefficiently serve as rewards for all possible actions, po-\\ntentially eliminating the need for a separate glmeval to\\nmodel rewards in certain workflows. This method al-\\nlows for the simultaneous expansion of multiple poten-'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 9}, page_content='tentially eliminating the need for a separate glmeval to\\nmodel rewards in certain workflows. This method al-\\nlows for the simultaneous expansion of multiple poten-\\ntial nodes in one generation step, rather than expand-\\ning each node individually in search-based workflows\\n(Hao et al., 2023).\\nE Memory\\nThe implementations of memory in the reviewed\\nworks are typically straightforward and arbitrary. Com-\\nmonly, static information (e.g., profiling messages) is\\nmanually constructed and stored, whereas dynamic\\ninformation (e.g., feedback) is handled via runtime\\ndata structures during interactions within each work-\\nflow. While the management of hybrid memory sys-\\ntems—requiring the explicit processing and manage-\\nment of short-term and long-term memory—is exten-\\nsively discussed in a previous survey by Wang et al.\\n(2024), such memory management aspects are beyond\\nthe focus of this survey, which centers on LLM-based\\nworkflows.\\n10'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 10}, page_content='Involved Workflows LMPCs Non-LMPC\\nComponentsApplied\\nEnvironments\\nTree-of-Thoughts\\n(ToT) (Yao et al.,\\n2023a)Search\\nvia Traversal & Heuristicglmactor, glmeval, glmplanner\\n(only for NLIEs-Writing)Search Tree Gaming;\\nNLIEs-Writing\\nTree-BeamSearch\\n(Xie et al., 2023)Search-based\\nvia Traversal & Heuristicglmactor, glmeval Search Tree NLIE-QA\\nRAP (Hao et al.,\\n2023)Search via MCTS glmactor, glmdynamic ,glmeval Search Tree Gaming;\\nNLIEs-QA\\nLLM Planner\\n(Huang et al., 2022)Base glmplanner MLM for action\\ntranslationEmbodied Env\\nDEPS (Wang et al.,\\n2023b)Base glmplanner ,glmaction_selector ,\\nglmverbalizerImmediate\\nactor,\\nVLM+GLM as\\nverbalizerEmbodied Env\\nPlanner-Actor-\\nReporter (Dasgupta\\net al., 2022)Base glmplanner RL actor,\\nTrained classi-\\nfier+Hard code\\nas verbalizerEmbodied Env\\nPlan-and-solve\\n(Wang et al., 2023a)Base glmplanner / NLIEs-QA\\nMultiTool-CoT (In-\\naba et al., 2023)Tool-Use glmactor / NLIEs\\nReAct (Yao et al.,\\n2023b)Tool-Use glmactor / NLIEs\\nBase glmactor / Embodied Env\\nGuan et al. (2023) Feedback Learning\\n(from Tools & Humans)glmplanner , glmpddl_translator Domain Ex-\\nperts, Domain-\\nindependent\\nPlannerEmbodied Env\\nCRITIC (Gou et al.,\\n2024)Feedback Learning\\n(from Tool & glmeval)glmactor / NLIEs\\nSelf-refine (Madaan\\net al., 2023)Feedback Learning\\n(from glmeval)glmactor, glmeval, / NLIEs\\nReflexion (Shinn\\net al., 2023)Feedback Learning\\n(from glmeval),\\nTool-Useglmactor, glmeval, / NLIEs-QA\\nFeedback Learning\\n(from Task Env & glmeval),glmactor, glmeval,\\nglmfb_verbalizerheuristic_func eval Embodied Env\\nTable 5: Frameworks for LLM-based Agents. MLM: Masked Language Model.\\n11'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 11}, page_content='Q: In a dance class of 20 students, 20% enrolled in contemporary dance, 25%\\nof the remaining enrolled in jazz dance, and the rest enrolled in hip-hop dance.\\nWhat percentage of the entire students enrolled in hip-hop dance?\\nA: Let’s first understand the problem and devise a plan to solve the problem.\\nThen, let’s carry out the plan and solve the problem step by step.\\nPlan:\\nStep 1: Calculate the total number of students who enrolled in\\ncontemporary and jazz dance.\\nStep 2: Calculate the total number of students who enrolled in hiphop\\ndance.\\nStep 3: Calculate the percentage of students who enrolled in hiphop\\ndance.\\nSolution:\\nStep 1: 20% of 20 students is 4 students. 25% of the remaining 16\\nstudents is 4 students. So, a total of 8 students enrolled in contemporary\\nand jazz dance.\\nStep 2: The remaining 12 students enrolled in hip-hop dance.\\nStep 3: The percentage of students who enrolled in hip-hop dance is\\n12/20 = 60% .\\nTherefore, the answer (arabic numerals) is\\n60%\\nTable 6: An LLM Planner under NLIE-QA, implemented by the zero-shot CoT method. The CoT trigger is highlighted in\\nred font. The content in shadow box is generated by LLM. Source from Wang et al. (2023a)\\n12'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 12}, page_content='Interact with a household to solve a task. Here are two examples. You are in the\\nmiddle of a room. Looking quickly around you, you see a cabinet 9, a cabinet\\n8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a\\ncabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a\\ndrawer 9, a drawer 8, a drawer 7, a drawer 6, a drawer 5, a drawer 4, a drawer 3,\\na drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a\\nshelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner\\n2, a stoveburner 1, and a toaster 1.\\nYour task is to: put a cool mug in shelf.\\n> think: To solve the task, I need to find and take a mug, then cool it with\\nfridge, then put it in shelf.\\nOK.\\n> think: First I need to find a mug. A mug is more likely to appear in countertop\\n(1-3), coffeemachine (1), cabinet (1-9), shelf (1-3), drawer (1-9). I can check\\none by one, starting with countertop 1.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you see a knife 1, a pan 2, a pan 1, and a plate 1.\\n...\\n> think: Now I cool the mug. Next, I need to put it in/on shelf 1. OK.\\n> go to shelf 1\\nOn the shelf 1, you see a cup 2, a cup 1, a peppershaker 1, and a saltshaker 1.\\n> put mug 3 in/on shelf 1\\nYou put the mug 3 in/on the shelf 1.\\nHere is the task: You are in the middle of a room. Looking quickly around you,\\nyou see a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1,\\na coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a drawer 3, a\\ndrawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a\\nshelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner\\n2, a stoveburner 1, and a toaster 1.\\nYour task is to: put a cool tomato in microwave.\\n>\\nthink: To solve the task, I need to find a tomato, then cool it with the\\nfridge, and finally put it in the microwave.\\nOK.\\n>\\nthink: First, I need to find a tomato. A tomato is more likely to be found\\non the countertop (1-3), cabinet (1-6), drawer (1-3), shelf (1-3), or the\\nsinkbasin. I can start by checking the countertop 1.\\nOK. >\\ngo to countertop 1\\nOn the countertop 1, you see a dishsponge 1, and a mug 2. ...\\nTable 7: An LLM Actor within Base Workflow under the Alfred World Benchmark, implemented by the few-shot CoT\\nmethod. The content in the shadow box is generated by LLM. Source from (Yao et al., 2023b).\\n13'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 13}, page_content='Use the following tools at your disposal and answer some questions:\\n- «Calculator»\\n- «Chemical reaction predictor»\\n- «Molar mass list»\\nQ: Find the amount of H2CO3 formed on combining 2 moles of CO2 and 2\\nmoles of H2O .\\nA: First, use the chemical reaction predictor to determine the chemical equation\\nfor the reaction.\\nReactants: 2CO2 ,2H2O\\nProducts: ? H2CO3\\n«Chemical reaction predictor»\\n2CO2 + 2H2O →2H2CO3\\nSecond, use the molar mass list to determine the molar mass of H2CO3 .\\nH2CO3 «Molar mass list» 62 g/mol\\nThird, use the calculator to calculate the amount of 2 moles of H2CO3 .2×62 =\\n«Calculator» 124\\nTherefore, 124 g ofH2CO3 is formed.\\nQ: Find the amount of Calcium hydroxide that is required to react with 2 moles\\nof Carbon dioxide to form 2 moles of Calcium carbonate along with 2 moles of\\nWater.\\nA:\\nTable 8: An LLM actor with tool use implemented by MultiTool-CoT (Inaba et al., 2023).\\n14'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 14}, page_content='Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages\\nand today, she read twice as many pages as yesterday. If she wants to read half\\nof the remaining pages tomorrow, how many pages should she read?\\nA: Julie read twice as many pages as yesterday, so she read 12 * 2 = 24 pages\\ntoday. Since yesterday, Julie read 12 + 24 = 36 pages. So, there are 120 - 36 =\\n84 pages left to be read. Since she wants to read half of the remaining pages,\\nshe should read 84 / 2 = 42 pages. The answer is 42.\\nQ: Ken created a care package to send to his brother, who was away\\nat boarding school. Ken placed a box on a scale, and then he poured into the\\nbox enough jelly beans to bring the weight to 2 pounds. Then, he added enough\\nbrownies to cause the weight to triple. Next, he added another 2 pounds of jelly\\nbeans. And finally, he added enough gummy worms to double the weight once\\nagain. What was the final weight of the box of goodies, in pounds?\\nA: Ken poured jelly beans into the box until the weight was 2 pounds, so the\\nweight of the box was 2 pounds at first. Then Ken added enough brownies to\\ncause the weight to triple, so the weight of the box was 2 * 3 = 6 pounds. After\\nKen added another 2 pounds of jelly beans, the weight of the box was 6 + 2 = 8\\npounds. Finally, he added enough gummy worms to double the weight once\\nagain, so the weight of the box was 8 * 2 = 16 pounds. The answer is 16.\\nQ: James writes a 3-page letter to 2 different friends twice a week.\\nHow many pages does he write a year?\\nA: James writes a 3-page letter to 2 different friends twice a week, so he writes\\n3 * 2 * 2 = 12 pages every week. There are 52 weeks in a year, so he writes 12\\n* 52 = 624 pages a year. The answer is 624.\\nQ: Natalia sold clips to 48 of her friends in April, and then she sold\\nhalf as many clips in May. How many clips did Natalia sell altogether in April\\nand May?\\nA: Natalia sold 48 clips in April and half as many clips in May, so she sold 48 /\\n2 = 24 clips in May. Altogether, she sold 48 + 24 = 72 clips. The answer is 72.\\nQ: Janet’s ducks lay 16 eggs per day. She eats three for breakfast ev-\\nery morning and bakes muffins for her friends every day with four. She sells\\nthe remainder at the farmers’ market daily for $2 per fresh duck egg. How\\nmuch in dollars does she make every day at the farmers’ market?\\nA:\"\\nJanet’s ducks lay 16 eggs per day. She eats three for breakfast every\\nmorning and bakes muffins for her friends every day with four, so she\\nuses 3 + 4 = «3+4=7»7 eggs daily.\\nThus, she sells 16 - 7 = «16-7=9»9 eggs at the farmers’ market daily.\\nTherefore, she makes 9 * $2 = $«9*2=18»18 every day at the farmers’\\nmarket. Answer: 18.\\nTable 9: An LLM Actor within Tool-Use Workflow under NLIE-QA, implemented by the few-Shot CoT method. The\\ncontent in the shadow box is generated by LLM.\\n15'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 15}, page_content='Solve a question answering task by having a Thought, then Finish with your\\nanswer. Thought can reason about the current situation. Finish[answer] returns\\nthe answer and finishes the task.\\nHere are some examples:\\nQuestion 1: What is the elevation range for the area that the eastern sector of\\nthe Colorado orogeny extends into?\\nThought: Let’s think step by step. The eastern sector of Colorado orogeny\\nextends into the High Plains. High Plains rise in elevation from around 1,800 to\\n7,000 ft, so the answer is 1,800 to 7,000 ft.\\nAction: Finish[1,800 to 7,000 ft]\\n...\\nQuestion 6: Were Pavel Urysohn and Leonid Levin known for the same type\\nof work?\\nThought: Let’s think step by step. Pavel Urysohn is a mathematician. Leonid\\nLevin is a mathematician and computer scientist. So Pavel Urysohn and Leonid\\nLevin have the same type of work.\\nAction: Finish[Yes]\\nEND OF EXAMPLES)\\nQuestion : VIV A Media AG changed it ´s name in 2004. What does their new\\nacronym stand for?\\nThought :\\nLet’s think step by step. VIV A Media AG changed its name in 2004.\\nThe new acronym must stand for the new name they adopted. To find\\nthe answer, we need to research the new name of VIV A Media AG after\\n2004.Action: Finish[Further research is needed to determine the new\\nacronym.]\\nAction :\\nFinish[Further research is needed to determine the new acronym.]\\nTable 10: An LLM Actor within tool-use workflow for the HotPot-QA Benchmark, implemented by few-Shot CoT. The\\ncontent in the shadow box is generated by LLM. Source from Yao et al. (2023b).\\n16'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 16}, page_content='Given a question, please decompose it into sub-questions. For each sub-\\nquestion, please answer it in a complete sentence, ending with \"The answer\\nis\". When the original question is answerable, please start the subquestion with\\n\"Now we can answer the question:\"\\nQuestion 1: James writes a 3-page letter to 2 different friends twice a week.\\nHow many pages does he write a year?\\nQuestion 1.1: How many pages does he write every week?\\nAnswer 1.1: James writes a 3-page letter to 2 different friends twice a week, so\\nhe writes 3 * 2 * 2 = 12 pages every week. The answer is 12.\\nQuestion 1.2: How many weeks are there in a year?\\nAnswer 1.2: There are 52 weeks in a year. The answer is 52.\\nQuestion 1.3: Now we can answer the question: How many pages does he\\nwrite a year?\\nAnswer 1.3: James writes 12 pages every week, so he writes 12 * 52 = 624\\npages a year. The answer is 624.\\n...\\nQuestion 5: Janet’s ducks lay 16 eggs per day. She eats three for breakfast\\nevery morning and bakes muffins for her friends every day with four. She sells\\nthe remainder at the farmers’ market daily for $2 per fresh duck egg. How much\\nin dollars does she make every day at the farmers’ market?\\nQuestion 5.1:\\nHow many eggs does Janet have left after eating three for breakfast and\\nbaking muffins with four?\\nTable 11: An LLM Actor under the GSM8K Benchmark. The content in the shadow box is generated by LLM. Source\\nfrom Hao et al. (2023).\\nGiven a question and some sub-questions, determine whether the last sub-\\nquestion is useful to answer the question. Output ’Yes’ or ’No’, and a reason.\\nQuestion 1: Four years ago, Kody was only half as old as Mohamed. If\\nMohamed is currently twice as 30 years old, how old is Kody?\\nQuestion 1.1: How old is Mohamed?\\nQuestion 1.2: How old was Mohamed four years ago?\\nNew question 1.3: How old was Kody four years ago?\\nIs the new question useful? Yes. We need the answer to calculate how old is\\nKody now.\\n...\\nQuestion 5: Janet’s ducks lay 16 eggs per day. She eats three for breakfast\\nevery morning and bakes muffins for her friends every day with four. She sells\\nthe remainder at the farmers’ market daily for $2 per fresh duck egg. How much\\nin dollars does she make every day at the farmers’ market?\\nNew question 5.1: Now we can answer the question: How much in dollars\\ndoes she make every day at the farmers’ market?\\nIs the new question useful?\\nTable 12: An LLM Evaluator within RAP Workflow under NLIE-QA, implemented by few-Shot CoT prompting. It\\nassesses the usefulness of new sub-questions in solving the original question. Source from Hao et al. (2023)\\n17'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 17}, page_content='Evaluate if given numbers can reach 24 (sure/likely/impossible)\\n10 14\\n10 + 14 = 24\\nsure\\n...\\n1 3 3\\n1 * 3 * 3 = 9\\n(1 + 3) * 3 = 12\\n1 3 3 are all too small\\nimpossible\\n11, 12\\nTable 13: An LLM Evaluator within Tree-of-Thought Workflow under Game 24, implemented by few-Shot CoT prompting.\\nThe LLM is profiled for multi-class classification. Source from Yao et al. (2023a).\\nContext: ...\\nSentence: ...\\nIs the sentence supported by the context above?\\nAnswer Yes or No:\\nTable 14: An LLM Evaluator, implemented by zero-Shot CoT prompting. The LLM is profiled for multi-class classification.\\nSource from Manakul et al. (2023).\\n18'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 18}, page_content='...\\nQuestion: Serianna is a band of what genre that combines elements of heavy\\nmetal and hardcore punk?\\nProposed Answer: Let’s think step by step. Serianna is a band of metalcore\\ngenre. Metalcore is a subgenre of heavy metal and hardcore punk. So Serianna\\nis a band of heavy metal and hardcore punk. So the answer is: heavy metal and\\nhardcore punk.\\n1. Plausibility:\\nThe question asks for the genre that combines elements of heavy metal\\nand hardcore punk, and the answer is \"heavy metal and hardcore punk\",\\nsimply repeat the question. So it’s not plausible.\\n2. Truthfulness: Let’s search the question in google:\\n> Search Query: Serianna is a band of what genre that combines elements\\nof heavy metal and hardcore punk? site: wikipedia.org\\n> Evidence:\\n[Metalcore - Wikipedia] Metalcore is a fusion music genre that combines\\nelements of extreme metal and hardcore punk.\\nThe evidence suggests that metalcore is a genre that combines elements\\nof extreme metal and hardcore punk, as stated in the proposed answer.\\nLet’s search the proposed answer in google:\\n> Search Query: Serianna is a band of metalcore genre. site:\\nwikipedia.org > Evidence:\\n[Serianna - Wikipedia] Serianna was a metalcore band from Madison, Wiscon-\\nsin. The band formed in 2006...\\nThe evidence suggests Serianna is a metalcore band, the proposed\\nanswer is correct about this.\\nAbove all, the proposed answer correctly identifies that Serianna is a\\nband of the metalcore genre, which combines elements of heavy metal\\nand hardcore punk. However, the final answer is not plausible since it\\njust lists the genres that metalcore combines.\\nTable 15: An LLM Evaluator Within Feedback-Learning Workflow (Tools), implemented with few-shot demonstrations.\\nThe content in the shadow box is generated by LLM. We omit some demonstrations for brevity. Source from Gou et al.\\n(2024).\\n19'),\n",
       " Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 19}, page_content='Given a question, please decompose it into sub-questions. For each sub-\\nquestion, please answer it in a complete sentence, ending with \"The answer\\nis\". When the original question is answerable, please start the subquestion with\\n\"Now we can answer the question: \".\\nQuestion 1: Weng earns $12 an hour for babysitting. Yesterday, she just did 50\\nminutes of babysitting. How much did she earn?\\nQuestion 1.1: How much does Weng earn per minute?\\nAnswer 1.1: Since Weng earns $12 an hour for babysitting, she earns $12 / 60\\n= $0.2 per minute. The answer is 0.2.\\nQuestion 1.2: Now we can answer the question: How much did she earn?\\nAnswer 1.2: Working 50 minutes, she earned $0.2 x 50 = $10. The answer is\\n10.\\n...\\nQuestion 5: Janet’s ducks lay 16 eggs per day. She eats three for breakfast\\nevery morning and bakes muffins for her friends every day with four. She sells\\nthe remainder at the farmers’ market daily for $2 per fresh duck egg. How much\\nin dollars does she make every day at the farmers’ market?\\nQuestion 5.1: How many eggs does Janet have left after eating three for\\nbreakfast and using four for muffins?\\nAnswer 5.1:\\nTable 16: An LLM-Profiled Dynamic Model.\\n20')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs = loader.load_and_split() # SPLIT\n",
    "pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './assets-resources/paper-llm-components.pdf', 'page': 0}, page_content='A Survey on LLM-Based Agents: Common Workflows and Reusable\\nLLM-Profiled Components\\nXinzhe Li\\nSchool of IT, Deakin University, Australia\\nlixinzhe@deakin.edu.au\\nAbstract\\nRecent advancements in Large Language Mod-\\nels (LLMs) have catalyzed the development of so-\\nphisticated frameworks for developing LLM-based\\nagents. However, the complexity of these frame-\\nworks r poses a hurdle for nuanced differentiation\\nat a granular level, a critical aspect for enabling\\nefficient implementations across different frame-\\nworks and fostering future research. Hence, the\\nprimary purpose of this survey is to facilitate a co-\\nhesive understanding of diverse recently proposed\\nframeworks by identifying common workflows and\\nreusable LLM-Profiled Components (LMPCs).\\n1 Introduction\\nGenerative Large Language Models (GLMs or LLMs)\\nhave acquired extensive general knowledge and\\nhuman-like reasoning capabilities (Santurkar et al.,\\n2023; Wang et al., 2022; Zhong et al., 2022, 2023),\\npositioning them as pivotal in constructing AI agents\\nknown as LLM-based agents. In the context of this\\nsurvey, LLM-based agents are defined by their abil-\\nity to interact actively with external tools (such as\\nWikipedia) or environments (such as householding en-\\nvironments) and are designed to function as integral\\ncomponents of agency, including acting, planning, and\\nevaluating.\\nPurpose of the Survey The motivation behind this\\nsurvey stems from the observation that many LLM-\\nbased agents incorporate similar workflows and com-\\nponents, despite the presence of a wide variety of\\ntechnical and conceptual challenges, e.g., search algo-\\nrithms (Yao et al., 2023a), tree structures (Hao et al.,\\n2023), and Reinforcement Learning (RL) components\\n(Shinn et al., 2023). (Wu et al., 2023) offer a modular\\napproach but lack integration with prevalent agentic\\nworkflows. Wang et al. (2024) provide a comprehen-\\nsive review of LLM agents, exploring their capabil-\\nities across profiling, memory, planning, and action.\\nIn contrast, our survey does not attempt to cover all\\ncomponents of LLM-based agents comprehensively.\\nInstead, we concentrate on the involvement of LLMswithin agentic workflows and aim to clarify the roles\\nof LLMs in agent implementations. We create com-\\nmon workflows incorporating reusable LLM-Profiled\\nComponents (LMPCs), as depicted in Figure 1.\\nContributions This survey offers the following con-\\ntributions. 1) Alleviating the understanding of com-\\nplex frameworks : The complexity of existing frame-\\nworks can be simplified into implementable workflows,\\nespecially when they are extracted for specific tasks.\\nThis survey emphasizes reusable workflows and LM-\\nPCs across popular frameworks, such as ReAct (Yao\\net al., 2023b), Reflexion (Shinn et al., 2023) and Tree-\\nof-Thoughts (Yao et al., 2023a). Specifically, based\\non the interaction environments (§2) and the use of\\ncommon LMPCs (§3), we categorize and detail vari-\\nous workflows, e.g., tool-use workflows, search work-\\nflows, and feedback-learning workflows. Many ex-\\nisting frameworks are composed of these workflows\\nand LMPCs, along with some specific non-LLM com-\\nponents. 2) Helping researchers/practitioners as-\\nsess current frameworks at a more granular and\\ncohesive level : Section 4 categorizes prominent frame-\\nworks and demonstrates how they are assembled by the\\ncommon workflows and LMPCs, as summarized in Ta-\\nble 21.3) Facilitating further extensions of existing\\nframeworks : Existing frameworks could be modi-\\nfied by changing the implementations of LMPCs. To\\nenable this, we not only summarize implementations\\nof LMPCs but also their applicability across diverse\\nworkflows and tasks in Section 5.\\n2 Task Environments And Tool\\nEnvironments\\nThis section explores task environments and tool envi-\\nronments, which present different settings compared to\\ntraditional AI and reinforcement learning (RL) agent\\nframeworks (Russell and Norvig, 2010; Sutton and\\nBarto, 2018) . After a brief overview of standard logic-')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_obj = pdf_docs[0]\n",
    "doc_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Survey on LLM-Based Agents: Common Workflows and Reusable\\nLLM-Profiled Components\\nXinzhe Li\\nSchool of IT, Deakin University, Australia\\nlixinzhe@deakin.edu.au\\nAbstract\\nRecent advancements in Large Language Mod-\\nels (LLMs) have catalyzed the development of so-\\nphisticated frameworks for developing LLM-based\\nagents. However, the complexity of these frame-\\nworks r poses a hurdle for nuanced differentiation\\nat a granular level, a critical aspect for enabling\\nefficient implementations across different frame-\\nworks and fostering future research. Hence, the\\nprimary purpose of this survey is to facilitate a co-\\nhesive understanding of diverse recently proposed\\nframeworks by identifying common workflows and\\nreusable LLM-Profiled Components (LMPCs).\\n1 Introduction\\nGenerative Large Language Models (GLMs or LLMs)\\nhave acquired extensive general knowledge and\\nhuman-like reasoning capabilities (Santurkar et al.,\\n2023; Wang et al., 2022; Zhong et al., 2022, 2023),\\npositioning them as pivotal in constructing AI agents\\nknown as LLM-based agents. In the context of this\\nsurvey, LLM-based agents are defined by their abil-\\nity to interact actively with external tools (such as\\nWikipedia) or environments (such as householding en-\\nvironments) and are designed to function as integral\\ncomponents of agency, including acting, planning, and\\nevaluating.\\nPurpose of the Survey The motivation behind this\\nsurvey stems from the observation that many LLM-\\nbased agents incorporate similar workflows and com-\\nponents, despite the presence of a wide variety of\\ntechnical and conceptual challenges, e.g., search algo-\\nrithms (Yao et al., 2023a), tree structures (Hao et al.,\\n2023), and Reinforcement Learning (RL) components\\n(Shinn et al., 2023). (Wu et al., 2023) offer a modular\\napproach but lack integration with prevalent agentic\\nworkflows. Wang et al. (2024) provide a comprehen-\\nsive review of LLM agents, exploring their capabil-\\nities across profiling, memory, planning, and action.\\nIn contrast, our survey does not attempt to cover all\\ncomponents of LLM-based agents comprehensively.\\nInstead, we concentrate on the involvement of LLMswithin agentic workflows and aim to clarify the roles\\nof LLMs in agent implementations. We create com-\\nmon workflows incorporating reusable LLM-Profiled\\nComponents (LMPCs), as depicted in Figure 1.\\nContributions This survey offers the following con-\\ntributions. 1) Alleviating the understanding of com-\\nplex frameworks : The complexity of existing frame-\\nworks can be simplified into implementable workflows,\\nespecially when they are extracted for specific tasks.\\nThis survey emphasizes reusable workflows and LM-\\nPCs across popular frameworks, such as ReAct (Yao\\net al., 2023b), Reflexion (Shinn et al., 2023) and Tree-\\nof-Thoughts (Yao et al., 2023a). Specifically, based\\non the interaction environments (§2) and the use of\\ncommon LMPCs (§3), we categorize and detail vari-\\nous workflows, e.g., tool-use workflows, search work-\\nflows, and feedback-learning workflows. Many ex-\\nisting frameworks are composed of these workflows\\nand LMPCs, along with some specific non-LLM com-\\nponents. 2) Helping researchers/practitioners as-\\nsess current frameworks at a more granular and\\ncohesive level : Section 4 categorizes prominent frame-\\nworks and demonstrates how they are assembled by the\\ncommon workflows and LMPCs, as summarized in Ta-\\nble 21.3) Facilitating further extensions of existing\\nframeworks : Existing frameworks could be modi-\\nfied by changing the implementations of LMPCs. To\\nenable this, we not only summarize implementations\\nof LMPCs but also their applicability across diverse\\nworkflows and tasks in Section 5.\\n2 Task Environments And Tool\\nEnvironments\\nThis section explores task environments and tool envi-\\nronments, which present different settings compared to\\ntraditional AI and reinforcement learning (RL) agent\\nframeworks (Russell and Norvig, 2010; Sutton and\\nBarto, 2018) . After a brief overview of standard logic-'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_obj.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A Survey on LLM-Based Agents: Common Workflows and Reusable\n",
       "LLM-Profiled Components\n",
       "Xinzhe Li\n",
       "School of IT, Deakin University, Australia\n",
       "lixinzhe@deakin.edu.au\n",
       "Abstract\n",
       "Recent advancements in Large Language Mod-\n",
       "els (LLMs) have catalyzed the development of so-\n",
       "phisticated frameworks for developing LLM-based\n",
       "agents. However, the complexity of these frame-\n",
       "works r poses a hurdle for nuanced differentiation\n",
       "at a granular level, a critical aspect for enabling\n",
       "efficient implementations across different frame-\n",
       "works and fostering future research. Hence, the\n",
       "primary purpose of this survey is to facilitate a co-\n",
       "hesive understanding of diverse recently proposed\n",
       "frameworks by identifying common workflows and\n",
       "reusable LLM-Profiled Components (LMPCs).\n",
       "1 Introduction\n",
       "Generative Large Language Models (GLMs or LLMs)\n",
       "have acquired extensive general knowledge and\n",
       "human-like reasoning capabilities (Santurkar et al.,\n",
       "2023; Wang et al., 2022; Zhong et al., 2022, 2023),\n",
       "positioning them as pivotal in constructing AI agents\n",
       "known as LLM-based agents. In the context of this\n",
       "survey, LLM-based agents are defined by their abil-\n",
       "ity to interact actively with external tools (such as\n",
       "Wikipedia) or environments (such as householding en-\n",
       "vironments) and are designed to function as integral\n",
       "components of agency, including acting, planning, and\n",
       "evaluating.\n",
       "Purpose of the Survey The motivation behind this\n",
       "survey stems from the observation that many LLM-\n",
       "based agents incorporate similar workflows and com-\n",
       "ponents, despite the presence of a wide variety of\n",
       "technical and conceptual challenges, e.g., search algo-\n",
       "rithms (Yao et al., 2023a), tree structures (Hao et al.,\n",
       "2023), and Reinforcement Learning (RL) components\n",
       "(Shinn et al., 2023). (Wu et al., 2023) offer a modular\n",
       "approach but lack integration with prevalent agentic\n",
       "workflows. Wang et al. (2024) provide a comprehen-\n",
       "sive review of LLM agents, exploring their capabil-\n",
       "ities across profiling, memory, planning, and action.\n",
       "In contrast, our survey does not attempt to cover all\n",
       "components of LLM-based agents comprehensively.\n",
       "Instead, we concentrate on the involvement of LLMswithin agentic workflows and aim to clarify the roles\n",
       "of LLMs in agent implementations. We create com-\n",
       "mon workflows incorporating reusable LLM-Profiled\n",
       "Components (LMPCs), as depicted in Figure 1.\n",
       "Contributions This survey offers the following con-\n",
       "tributions. 1) Alleviating the understanding of com-\n",
       "plex frameworks : The complexity of existing frame-\n",
       "works can be simplified into implementable workflows,\n",
       "especially when they are extracted for specific tasks.\n",
       "This survey emphasizes reusable workflows and LM-\n",
       "PCs across popular frameworks, such as ReAct (Yao\n",
       "et al., 2023b), Reflexion (Shinn et al., 2023) and Tree-\n",
       "of-Thoughts (Yao et al., 2023a). Specifically, based\n",
       "on the interaction environments (§2) and the use of\n",
       "common LMPCs (§3), we categorize and detail vari-\n",
       "ous workflows, e.g., tool-use workflows, search work-\n",
       "flows, and feedback-learning workflows. Many ex-\n",
       "isting frameworks are composed of these workflows\n",
       "and LMPCs, along with some specific non-LLM com-\n",
       "ponents. 2) Helping researchers/practitioners as-\n",
       "sess current frameworks at a more granular and\n",
       "cohesive level : Section 4 categorizes prominent frame-\n",
       "works and demonstrates how they are assembled by the\n",
       "common workflows and LMPCs, as summarized in Ta-\n",
       "ble 21.3) Facilitating further extensions of existing\n",
       "frameworks : Existing frameworks could be modi-\n",
       "fied by changing the implementations of LMPCs. To\n",
       "enable this, we not only summarize implementations\n",
       "of LMPCs but also their applicability across diverse\n",
       "workflows and tasks in Section 5.\n",
       "2 Task Environments And Tool\n",
       "Environments\n",
       "This section explores task environments and tool envi-\n",
       "ronments, which present different settings compared to\n",
       "traditional AI and reinforcement learning (RL) agent\n",
       "frameworks (Russell and Norvig, 2010; Sutton and\n",
       "Barto, 2018) . After a brief overview of standard logic-"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "Markdown(doc_obj.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x11ad38d90>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x11ad3cc10>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='https://api.openai.com/v1', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings() # EMBED\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x11a1ced90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(pdf_docs, embedding=embeddings) # STORE\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of a [retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/#:~:text=A%20retriever%20is,Document's%20as%20output.):\n",
    "\n",
    "> A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x11a1ced90>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever() \n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=MODEL, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://python.langchain.com/v0.2/docs/tutorials/pdf_qa/#question-answering-with-rag\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), config={'run_name': 'format_inputs'})\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x12bf88150>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12bff0890>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_api_base='https://api.openai.com/v1', openai_proxy='')\n",
       "| StrOutputParser(), config={'run_name': 'stuff_documents_chain'})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "question_answer_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method `create_stuff_documents_chain` [outputs an LCEL runnable](https://arc.net/l/quote/bnsztwth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"According to this paper what are these reusable LLM-profiled components?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'According to this paper what are these reusable LLM-profiled components?',\n",
       " 'context': [Document(metadata={'page': 0, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='A Survey on LLM-Based Agents: Common Workflows and Reusable\\nLLM-Profiled Components\\nXinzhe Li\\nSchool of IT, Deakin University, Australia\\nlixinzhe@deakin.edu.au\\nAbstract\\nRecent advancements in Large Language Mod-\\nels (LLMs) have catalyzed the development of so-\\nphisticated frameworks for developing LLM-based\\nagents. However, the complexity of these frame-\\nworks r poses a hurdle for nuanced differentiation\\nat a granular level, a critical aspect for enabling\\nefficient implementations across different frame-\\nworks and fostering future research. Hence, the\\nprimary purpose of this survey is to facilitate a co-\\nhesive understanding of diverse recently proposed\\nframeworks by identifying common workflows and\\nreusable LLM-Profiled Components (LMPCs).\\n1 Introduction\\nGenerative Large Language Models (GLMs or LLMs)\\nhave acquired extensive general knowledge and\\nhuman-like reasoning capabilities (Santurkar et al.,\\n2023; Wang et al., 2022; Zhong et al., 2022, 2023),\\npositioning them as pivotal in constructing AI agents\\nknown as LLM-based agents. In the context of this\\nsurvey, LLM-based agents are defined by their abil-\\nity to interact actively with external tools (such as\\nWikipedia) or environments (such as householding en-\\nvironments) and are designed to function as integral\\ncomponents of agency, including acting, planning, and\\nevaluating.\\nPurpose of the Survey The motivation behind this\\nsurvey stems from the observation that many LLM-\\nbased agents incorporate similar workflows and com-\\nponents, despite the presence of a wide variety of\\ntechnical and conceptual challenges, e.g., search algo-\\nrithms (Yao et al., 2023a), tree structures (Hao et al.,\\n2023), and Reinforcement Learning (RL) components\\n(Shinn et al., 2023). (Wu et al., 2023) offer a modular\\napproach but lack integration with prevalent agentic\\nworkflows. Wang et al. (2024) provide a comprehen-\\nsive review of LLM agents, exploring their capabil-\\nities across profiling, memory, planning, and action.\\nIn contrast, our survey does not attempt to cover all\\ncomponents of LLM-based agents comprehensively.\\nInstead, we concentrate on the involvement of LLMswithin agentic workflows and aim to clarify the roles\\nof LLMs in agent implementations. We create com-\\nmon workflows incorporating reusable LLM-Profiled\\nComponents (LMPCs), as depicted in Figure 1.\\nContributions This survey offers the following con-\\ntributions. 1) Alleviating the understanding of com-\\nplex frameworks : The complexity of existing frame-\\nworks can be simplified into implementable workflows,\\nespecially when they are extracted for specific tasks.\\nThis survey emphasizes reusable workflows and LM-\\nPCs across popular frameworks, such as ReAct (Yao\\net al., 2023b), Reflexion (Shinn et al., 2023) and Tree-\\nof-Thoughts (Yao et al., 2023a). Specifically, based\\non the interaction environments (§2) and the use of\\ncommon LMPCs (§3), we categorize and detail vari-\\nous workflows, e.g., tool-use workflows, search work-\\nflows, and feedback-learning workflows. Many ex-\\nisting frameworks are composed of these workflows\\nand LMPCs, along with some specific non-LLM com-\\nponents. 2) Helping researchers/practitioners as-\\nsess current frameworks at a more granular and\\ncohesive level : Section 4 categorizes prominent frame-\\nworks and demonstrates how they are assembled by the\\ncommon workflows and LMPCs, as summarized in Ta-\\nble 21.3) Facilitating further extensions of existing\\nframeworks : Existing frameworks could be modi-\\nfied by changing the implementations of LMPCs. To\\nenable this, we not only summarize implementations\\nof LMPCs but also their applicability across diverse\\nworkflows and tasks in Section 5.\\n2 Task Environments And Tool\\nEnvironments\\nThis section explores task environments and tool envi-\\nronments, which present different settings compared to\\ntraditional AI and reinforcement learning (RL) agent\\nframeworks (Russell and Norvig, 2010; Sutton and\\nBarto, 2018) . After a brief overview of standard logic-'),\n",
       "  Document(metadata={'page': 7, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='existing workflows through the integration of both\\ntask-specific LMPCs and non-LLM components. This\\napproach aims to foster the development and repro-\\nducibility of agentic workflows.\\nLimitations\\nThis survey omits discussions on memory design15\\nand the integration of peripheral components into agen-\\ntic workflows16, as our focus is on the details of\\ncommon LLM-profiled components within agentic\\nworkflows to facilitate the implementation of reusable\\ncomponents and extensible workflows. This distinctly\\nsets our work apart from other surveys.\\nReferences\\nSijia Chen, Baochun Li, and Di Niu. 2024. Boosting of\\nthoughts: Trial-and-error problem solving with large lan-\\n14Here, bandwidth refers to the volume of information pro-\\ncessed during a single LLM generation\\n15Appendix E provides a brief discussion on memory in LLM-\\nbased agents\\n16These are concisely summarized in Appendix A\\n8'),\n",
       "  Document(metadata={'page': 3, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='3) LLM-Profiled Dynamic Models glmdynamic : They\\npredict or describe changes to the environment. Gen-\\nerally, dynamic models form part of a comprehensive\\nworld model by predicting the next state s′from the\\ncurrent state sand action a. While typical RL uses\\nthe probability distribution p(s′|s, a)to model poten-\\ntial next states, LLM-based dynamic models directly\\npredict the next state s′=glmdynamic (s, a).\\nTask-Dependent LLM-Profiled Components In\\naddition to the universal components, certain LLM-\\nprofiled components are tailored to specific tasks. For\\ninstance, verbalizers are crucial in embodied environ-\\nments but unnecessary in NLIEs. A verbalizer trans-\\nlates actions and observations into inputs for planners;\\nfor example, in the Planner-Actor-Reporter workflow\\n(Wang et al., 2023a), a fine-tuned Visual Language\\nModel (VLM) along with glmplanner translates pixel\\nstates into textual inputs. Similarly, if environmental\\nfeedback is perceivable along with states, a verbalizer\\nmay be needed to translate this feedback into verbal\\ndescriptions for glmpolicy, akin to reward shaping in\\nRL where numerical stimuli are generated for policy\\nlearning. LLMs profiled as verbalizers, glmverbalizer\\n(Shinn et al., 2023), often guide descriptions accord-\\ning to specified criteria.\\nsettings to learn policy models that perform desirable behaviors.\\n3Note that planning algorithms may be utilized to structure a\\nplan of plans; for example, Tree-of-Thought employs tree search,\\nwhere each node potentially represents either a single action or an\\nentire plan.\\n4'),\n",
       "  Document(metadata={'page': 3, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='deployment in planning-based agent systems. Addi-\\ntionally, Wan et al. (2024) suggest that \"splitting an\\noutput sequence into tokens might be a good choice\"\\nfor defining multi-step NLIEs methodically. Further-\\nmore, Yao et al. (2023a) formulate two-step NLIEs for\\ncreative writing by segmenting the problem-solving\\nprocess into distinct planning and execution phases.\\n2.3 Tool Environments\\nModern LLM agents are often enhanced with external\\ntools that improve their problem-solving capabilities\\n(Inaba et al., 2023; Yao et al., 2023b). The design and\\nintegration of these tools add complexity, requiring\\ncareful consideration of how LLMs interact not only\\nwith the task environments but also with these aux-\\niliary tools. Typically, actions in tool environments\\ninvolve interactions with resources that remain unaf-\\nfected by these interactions. For instance, retrieving\\ndata from Wikipedia constitutes a \"read-only\" action,\\nwhich does not modify the Wikipedia database. This\\nfeature distinguishes such tool-use actions from those\\nin conventional task environments or typical reinforce-\\nment learning (RL) settings, where actions generally\\nalter the environmental state. Nevertheless, it is impor-\\ntant to recognize that tool environment can be dynamic\\nthat can undergo changes externally. This aspect re-\\nflects the nature that tools should be considered ex-\\nternal environments rather than the agent’s internal\\nprocesses.\\nNested NLIE-QA + Tool Environments Tool envi-\\nronments are frequently established along with NLIEs\\nto aid in solving QA tasks. Shinn et al. (2023); Yao\\net al. (2023b) incorporate tools to enhance the fac-\\ntuality of responses. They define command-like ac-\\ntions such as “Search” and “LookUp” to interact with\\nWikipedia, with “Search” suggesting the top-5 similar\\nentities from the relevant wiki page, and “LookUp”\\nsimulating the Ctrl+F functionality in a browser. Be-\\nyond simple retrieval, Thoppilan et al. (2022) include\\na language translator and a calculator for dialog tasks.\\nSimilarly, Inaba et al. (2023) employ a calculator, im-\\nplemented using the Python eval function, to resolve\\nnumerical queries within the NumGLUE benchmark.\\n3 LLM-Profiled Components\\nThis section explores common agentic roles for which\\nLLMs are typically profiled. The components leverage\\nthe internal commonsense knowledge and reasoning\\nabilities of LLMs to generate actions, plans, estimate\\nvalues2, and infer subsequent states.\\n2Values refer to the estimated rewards (a quantitative measure\\nof the success or desirability of the outcomes) associated with tak-\\ning a certain action in a state, widely used in typical RL and MDPUniversal LLM-Profiled Components Specifically,\\nthe following task-agnostic components are profiled\\nand commonly used across various workflows. 1)\\nLLM-Profiled Policy glmpolicy: Policy models are de-\\nsigned to generate decisions, which could be an action\\nor a series of actions (plans) for execution in exter-\\nnal environments or use in search and planning algo-\\nrithms.3In contrast to typical RL policy models,\\nwhich learn to maximize cumulative rewards through\\ntrial and error, LLM-profiled policy models, denoted\\nasglmpolicy, utilize pre-trained knowledge and com-\\nmonsense derived from extensive textual data. We\\ndistinguish between two types of glmpolicy: an actor\\nglmactordirectly maps a state to an action, whereas\\na planner glmplanner generates a sequence of actions\\nfrom a given state. 2) LLM-Profiled Evaluators glmeval:\\nglmevalprovide feedback crucial for different work-\\nflows. They evaluate actions and states in search-based\\nworkflows (Hao et al., 2023; Yao et al., 2023a) and re-\\nvise decisions in feedback-learning workflows (Shinn\\net al., 2023; Wang et al., 2023b) (refer to §4 for more\\ndetails). These evaluators are integral to both direct\\naction assessment and broader strategic adjustments.\\n3) LLM-Profiled Dynamic Models glmdynamic : They\\npredict or describe changes to the environment. Gen-\\nerally, dynamic models form part of a comprehensive')],\n",
       " 'answer': 'Reusable LLM-Profiled Components (LMPCs) include various task-agnostic components commonly used across different workflows, such as LLM-Profiled Policy (glmpolicy), LLM-Profiled Evaluators (glmeval), and LLM-Profiled Dynamic Models (glmdynamic). These components leverage the internal knowledge and reasoning abilities of LLMs to generate actions, provide feedback, and predict changes in the environment. The survey emphasizes the importance of these components in simplifying complex frameworks and facilitating the development of LLM-based agents.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reusable LLM-Profiled Components (LMPCs) include various task-agnostic components commonly used across different workflows, such as LLM-Profiled Policy (glmpolicy), LLM-Profiled Evaluators (glmeval), and LLM-Profiled Dynamic Models (glmdynamic). These components leverage the internal knowledge and reasoning abilities of LLMs to generate actions, provide feedback, and predict changes in the environment. The survey emphasizes the importance of these components in simplifying complex frameworks and facilitating the development of LLM-based agents."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "final_answer = results[\"answer\"]\n",
    "\n",
    "Markdown(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- The paper discusses the integration of task-specific LMPCs and non-LLM components to enhance agentic workflows.\n",
       "- It focuses on common LLM-profiled components while omitting memory design and peripheral component integration.\n",
       "- Various workflows, such as Tree-of-Thoughts and RAP, are explored, highlighting their search strategies and feedback-learning mechanisms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_summary = \"Write a simple bullet points summary about this paper\"\n",
    "\n",
    " # adding chat history so the model remembers previous questions\n",
    "output = rag_chain.invoke({\"input\": query_summary})\n",
    "\n",
    "Markdown(output[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output is easily verifiable, we can see below that the chunk context for the answer came from pages 0,5,7 and 16 in the source pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 7, 'source': './assets-resources/paper-llm-components.pdf'}\n",
      "{'page': 0, 'source': './assets-resources/paper-llm-components.pdf'}\n",
      "{'page': 5, 'source': './assets-resources/paper-llm-components.pdf'}\n",
      "{'page': 16, 'source': './assets-resources/paper-llm-components.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(output['context'])):\n",
    "    print(output['context'][i].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now dig deeper into RAG with pdf and construct this rag chain ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_prompt),\n",
    "    ('human', '{input}')\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        'input': lambda x: x['input'],\n",
    "        'context': lambda x: format_docs(x['context']), \n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the input query to the retriever\n",
    "retrieve_docs = (lambda x: x['input']) | retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  context: RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x11a1ced90>)\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: {\n",
       "              input: RunnableLambda(...),\n",
       "              context: RunnableLambda(...)\n",
       "            }\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x12bf88150>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12bff0890>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_api_base='https://api.openai.com/v1', openai_proxy='')\n",
       "            | StrOutputParser()\n",
       "  })"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'According to this paper what are these reusable LLM-profiled components?',\n",
       " 'context': [Document(metadata={'page': 0, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='A Survey on LLM-Based Agents: Common Workflows and Reusable\\nLLM-Profiled Components\\nXinzhe Li\\nSchool of IT, Deakin University, Australia\\nlixinzhe@deakin.edu.au\\nAbstract\\nRecent advancements in Large Language Mod-\\nels (LLMs) have catalyzed the development of so-\\nphisticated frameworks for developing LLM-based\\nagents. However, the complexity of these frame-\\nworks r poses a hurdle for nuanced differentiation\\nat a granular level, a critical aspect for enabling\\nefficient implementations across different frame-\\nworks and fostering future research. Hence, the\\nprimary purpose of this survey is to facilitate a co-\\nhesive understanding of diverse recently proposed\\nframeworks by identifying common workflows and\\nreusable LLM-Profiled Components (LMPCs).\\n1 Introduction\\nGenerative Large Language Models (GLMs or LLMs)\\nhave acquired extensive general knowledge and\\nhuman-like reasoning capabilities (Santurkar et al.,\\n2023; Wang et al., 2022; Zhong et al., 2022, 2023),\\npositioning them as pivotal in constructing AI agents\\nknown as LLM-based agents. In the context of this\\nsurvey, LLM-based agents are defined by their abil-\\nity to interact actively with external tools (such as\\nWikipedia) or environments (such as householding en-\\nvironments) and are designed to function as integral\\ncomponents of agency, including acting, planning, and\\nevaluating.\\nPurpose of the Survey The motivation behind this\\nsurvey stems from the observation that many LLM-\\nbased agents incorporate similar workflows and com-\\nponents, despite the presence of a wide variety of\\ntechnical and conceptual challenges, e.g., search algo-\\nrithms (Yao et al., 2023a), tree structures (Hao et al.,\\n2023), and Reinforcement Learning (RL) components\\n(Shinn et al., 2023). (Wu et al., 2023) offer a modular\\napproach but lack integration with prevalent agentic\\nworkflows. Wang et al. (2024) provide a comprehen-\\nsive review of LLM agents, exploring their capabil-\\nities across profiling, memory, planning, and action.\\nIn contrast, our survey does not attempt to cover all\\ncomponents of LLM-based agents comprehensively.\\nInstead, we concentrate on the involvement of LLMswithin agentic workflows and aim to clarify the roles\\nof LLMs in agent implementations. We create com-\\nmon workflows incorporating reusable LLM-Profiled\\nComponents (LMPCs), as depicted in Figure 1.\\nContributions This survey offers the following con-\\ntributions. 1) Alleviating the understanding of com-\\nplex frameworks : The complexity of existing frame-\\nworks can be simplified into implementable workflows,\\nespecially when they are extracted for specific tasks.\\nThis survey emphasizes reusable workflows and LM-\\nPCs across popular frameworks, such as ReAct (Yao\\net al., 2023b), Reflexion (Shinn et al., 2023) and Tree-\\nof-Thoughts (Yao et al., 2023a). Specifically, based\\non the interaction environments (§2) and the use of\\ncommon LMPCs (§3), we categorize and detail vari-\\nous workflows, e.g., tool-use workflows, search work-\\nflows, and feedback-learning workflows. Many ex-\\nisting frameworks are composed of these workflows\\nand LMPCs, along with some specific non-LLM com-\\nponents. 2) Helping researchers/practitioners as-\\nsess current frameworks at a more granular and\\ncohesive level : Section 4 categorizes prominent frame-\\nworks and demonstrates how they are assembled by the\\ncommon workflows and LMPCs, as summarized in Ta-\\nble 21.3) Facilitating further extensions of existing\\nframeworks : Existing frameworks could be modi-\\nfied by changing the implementations of LMPCs. To\\nenable this, we not only summarize implementations\\nof LMPCs but also their applicability across diverse\\nworkflows and tasks in Section 5.\\n2 Task Environments And Tool\\nEnvironments\\nThis section explores task environments and tool envi-\\nronments, which present different settings compared to\\ntraditional AI and reinforcement learning (RL) agent\\nframeworks (Russell and Norvig, 2010; Sutton and\\nBarto, 2018) . After a brief overview of standard logic-'),\n",
       "  Document(metadata={'page': 7, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='existing workflows through the integration of both\\ntask-specific LMPCs and non-LLM components. This\\napproach aims to foster the development and repro-\\nducibility of agentic workflows.\\nLimitations\\nThis survey omits discussions on memory design15\\nand the integration of peripheral components into agen-\\ntic workflows16, as our focus is on the details of\\ncommon LLM-profiled components within agentic\\nworkflows to facilitate the implementation of reusable\\ncomponents and extensible workflows. This distinctly\\nsets our work apart from other surveys.\\nReferences\\nSijia Chen, Baochun Li, and Di Niu. 2024. Boosting of\\nthoughts: Trial-and-error problem solving with large lan-\\n14Here, bandwidth refers to the volume of information pro-\\ncessed during a single LLM generation\\n15Appendix E provides a brief discussion on memory in LLM-\\nbased agents\\n16These are concisely summarized in Appendix A\\n8'),\n",
       "  Document(metadata={'page': 3, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='3) LLM-Profiled Dynamic Models glmdynamic : They\\npredict or describe changes to the environment. Gen-\\nerally, dynamic models form part of a comprehensive\\nworld model by predicting the next state s′from the\\ncurrent state sand action a. While typical RL uses\\nthe probability distribution p(s′|s, a)to model poten-\\ntial next states, LLM-based dynamic models directly\\npredict the next state s′=glmdynamic (s, a).\\nTask-Dependent LLM-Profiled Components In\\naddition to the universal components, certain LLM-\\nprofiled components are tailored to specific tasks. For\\ninstance, verbalizers are crucial in embodied environ-\\nments but unnecessary in NLIEs. A verbalizer trans-\\nlates actions and observations into inputs for planners;\\nfor example, in the Planner-Actor-Reporter workflow\\n(Wang et al., 2023a), a fine-tuned Visual Language\\nModel (VLM) along with glmplanner translates pixel\\nstates into textual inputs. Similarly, if environmental\\nfeedback is perceivable along with states, a verbalizer\\nmay be needed to translate this feedback into verbal\\ndescriptions for glmpolicy, akin to reward shaping in\\nRL where numerical stimuli are generated for policy\\nlearning. LLMs profiled as verbalizers, glmverbalizer\\n(Shinn et al., 2023), often guide descriptions accord-\\ning to specified criteria.\\nsettings to learn policy models that perform desirable behaviors.\\n3Note that planning algorithms may be utilized to structure a\\nplan of plans; for example, Tree-of-Thought employs tree search,\\nwhere each node potentially represents either a single action or an\\nentire plan.\\n4'),\n",
       "  Document(metadata={'page': 3, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='deployment in planning-based agent systems. Addi-\\ntionally, Wan et al. (2024) suggest that \"splitting an\\noutput sequence into tokens might be a good choice\"\\nfor defining multi-step NLIEs methodically. Further-\\nmore, Yao et al. (2023a) formulate two-step NLIEs for\\ncreative writing by segmenting the problem-solving\\nprocess into distinct planning and execution phases.\\n2.3 Tool Environments\\nModern LLM agents are often enhanced with external\\ntools that improve their problem-solving capabilities\\n(Inaba et al., 2023; Yao et al., 2023b). The design and\\nintegration of these tools add complexity, requiring\\ncareful consideration of how LLMs interact not only\\nwith the task environments but also with these aux-\\niliary tools. Typically, actions in tool environments\\ninvolve interactions with resources that remain unaf-\\nfected by these interactions. For instance, retrieving\\ndata from Wikipedia constitutes a \"read-only\" action,\\nwhich does not modify the Wikipedia database. This\\nfeature distinguishes such tool-use actions from those\\nin conventional task environments or typical reinforce-\\nment learning (RL) settings, where actions generally\\nalter the environmental state. Nevertheless, it is impor-\\ntant to recognize that tool environment can be dynamic\\nthat can undergo changes externally. This aspect re-\\nflects the nature that tools should be considered ex-\\nternal environments rather than the agent’s internal\\nprocesses.\\nNested NLIE-QA + Tool Environments Tool envi-\\nronments are frequently established along with NLIEs\\nto aid in solving QA tasks. Shinn et al. (2023); Yao\\net al. (2023b) incorporate tools to enhance the fac-\\ntuality of responses. They define command-like ac-\\ntions such as “Search” and “LookUp” to interact with\\nWikipedia, with “Search” suggesting the top-5 similar\\nentities from the relevant wiki page, and “LookUp”\\nsimulating the Ctrl+F functionality in a browser. Be-\\nyond simple retrieval, Thoppilan et al. (2022) include\\na language translator and a calculator for dialog tasks.\\nSimilarly, Inaba et al. (2023) employ a calculator, im-\\nplemented using the Python eval function, to resolve\\nnumerical queries within the NumGLUE benchmark.\\n3 LLM-Profiled Components\\nThis section explores common agentic roles for which\\nLLMs are typically profiled. The components leverage\\nthe internal commonsense knowledge and reasoning\\nabilities of LLMs to generate actions, plans, estimate\\nvalues2, and infer subsequent states.\\n2Values refer to the estimated rewards (a quantitative measure\\nof the success or desirability of the outcomes) associated with tak-\\ning a certain action in a state, widely used in typical RL and MDPUniversal LLM-Profiled Components Specifically,\\nthe following task-agnostic components are profiled\\nand commonly used across various workflows. 1)\\nLLM-Profiled Policy glmpolicy: Policy models are de-\\nsigned to generate decisions, which could be an action\\nor a series of actions (plans) for execution in exter-\\nnal environments or use in search and planning algo-\\nrithms.3In contrast to typical RL policy models,\\nwhich learn to maximize cumulative rewards through\\ntrial and error, LLM-profiled policy models, denoted\\nasglmpolicy, utilize pre-trained knowledge and com-\\nmonsense derived from extensive textual data. We\\ndistinguish between two types of glmpolicy: an actor\\nglmactordirectly maps a state to an action, whereas\\na planner glmplanner generates a sequence of actions\\nfrom a given state. 2) LLM-Profiled Evaluators glmeval:\\nglmevalprovide feedback crucial for different work-\\nflows. They evaluate actions and states in search-based\\nworkflows (Hao et al., 2023; Yao et al., 2023a) and re-\\nvise decisions in feedback-learning workflows (Shinn\\net al., 2023; Wang et al., 2023b) (refer to §4 for more\\ndetails). These evaluators are integral to both direct\\naction assessment and broader strategic adjustments.\\n3) LLM-Profiled Dynamic Models glmdynamic : They\\npredict or describe changes to the environment. Gen-\\nerally, dynamic models form part of a comprehensive')],\n",
       " 'answer': 'Reusable LLM-Profiled Components (LMPCs) include various task-agnostic components commonly used across different workflows, such as LLM-Profiled Policy (glmpolicy), LLM-Profiled Evaluators (glmeval), and LLM-Profiled Dynamic Models (glmdynamic). These components leverage the internal knowledge and reasoning abilities of LLMs to generate actions, provide feedback, and predict changes in the environment. The survey emphasizes the importance of these components in simplifying complex frameworks and facilitating the development of LLM-based agents.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"According to this paper what are these reusable LLM-profiled components?\" \n",
    "chain.invoke({'input': query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding structured sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'According to this paper what are these reusable LLM-profiled components?',\n",
       " 'context': [Document(metadata={'page': 0, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='A Survey on LLM-Based Agents: Common Workflows and Reusable\\nLLM-Profiled Components\\nXinzhe Li\\nSchool of IT, Deakin University, Australia\\nlixinzhe@deakin.edu.au\\nAbstract\\nRecent advancements in Large Language Mod-\\nels (LLMs) have catalyzed the development of so-\\nphisticated frameworks for developing LLM-based\\nagents. However, the complexity of these frame-\\nworks r poses a hurdle for nuanced differentiation\\nat a granular level, a critical aspect for enabling\\nefficient implementations across different frame-\\nworks and fostering future research. Hence, the\\nprimary purpose of this survey is to facilitate a co-\\nhesive understanding of diverse recently proposed\\nframeworks by identifying common workflows and\\nreusable LLM-Profiled Components (LMPCs).\\n1 Introduction\\nGenerative Large Language Models (GLMs or LLMs)\\nhave acquired extensive general knowledge and\\nhuman-like reasoning capabilities (Santurkar et al.,\\n2023; Wang et al., 2022; Zhong et al., 2022, 2023),\\npositioning them as pivotal in constructing AI agents\\nknown as LLM-based agents. In the context of this\\nsurvey, LLM-based agents are defined by their abil-\\nity to interact actively with external tools (such as\\nWikipedia) or environments (such as householding en-\\nvironments) and are designed to function as integral\\ncomponents of agency, including acting, planning, and\\nevaluating.\\nPurpose of the Survey The motivation behind this\\nsurvey stems from the observation that many LLM-\\nbased agents incorporate similar workflows and com-\\nponents, despite the presence of a wide variety of\\ntechnical and conceptual challenges, e.g., search algo-\\nrithms (Yao et al., 2023a), tree structures (Hao et al.,\\n2023), and Reinforcement Learning (RL) components\\n(Shinn et al., 2023). (Wu et al., 2023) offer a modular\\napproach but lack integration with prevalent agentic\\nworkflows. Wang et al. (2024) provide a comprehen-\\nsive review of LLM agents, exploring their capabil-\\nities across profiling, memory, planning, and action.\\nIn contrast, our survey does not attempt to cover all\\ncomponents of LLM-based agents comprehensively.\\nInstead, we concentrate on the involvement of LLMswithin agentic workflows and aim to clarify the roles\\nof LLMs in agent implementations. We create com-\\nmon workflows incorporating reusable LLM-Profiled\\nComponents (LMPCs), as depicted in Figure 1.\\nContributions This survey offers the following con-\\ntributions. 1) Alleviating the understanding of com-\\nplex frameworks : The complexity of existing frame-\\nworks can be simplified into implementable workflows,\\nespecially when they are extracted for specific tasks.\\nThis survey emphasizes reusable workflows and LM-\\nPCs across popular frameworks, such as ReAct (Yao\\net al., 2023b), Reflexion (Shinn et al., 2023) and Tree-\\nof-Thoughts (Yao et al., 2023a). Specifically, based\\non the interaction environments (§2) and the use of\\ncommon LMPCs (§3), we categorize and detail vari-\\nous workflows, e.g., tool-use workflows, search work-\\nflows, and feedback-learning workflows. Many ex-\\nisting frameworks are composed of these workflows\\nand LMPCs, along with some specific non-LLM com-\\nponents. 2) Helping researchers/practitioners as-\\nsess current frameworks at a more granular and\\ncohesive level : Section 4 categorizes prominent frame-\\nworks and demonstrates how they are assembled by the\\ncommon workflows and LMPCs, as summarized in Ta-\\nble 21.3) Facilitating further extensions of existing\\nframeworks : Existing frameworks could be modi-\\nfied by changing the implementations of LMPCs. To\\nenable this, we not only summarize implementations\\nof LMPCs but also their applicability across diverse\\nworkflows and tasks in Section 5.\\n2 Task Environments And Tool\\nEnvironments\\nThis section explores task environments and tool envi-\\nronments, which present different settings compared to\\ntraditional AI and reinforcement learning (RL) agent\\nframeworks (Russell and Norvig, 2010; Sutton and\\nBarto, 2018) . After a brief overview of standard logic-'),\n",
       "  Document(metadata={'page': 7, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='existing workflows through the integration of both\\ntask-specific LMPCs and non-LLM components. This\\napproach aims to foster the development and repro-\\nducibility of agentic workflows.\\nLimitations\\nThis survey omits discussions on memory design15\\nand the integration of peripheral components into agen-\\ntic workflows16, as our focus is on the details of\\ncommon LLM-profiled components within agentic\\nworkflows to facilitate the implementation of reusable\\ncomponents and extensible workflows. This distinctly\\nsets our work apart from other surveys.\\nReferences\\nSijia Chen, Baochun Li, and Di Niu. 2024. Boosting of\\nthoughts: Trial-and-error problem solving with large lan-\\n14Here, bandwidth refers to the volume of information pro-\\ncessed during a single LLM generation\\n15Appendix E provides a brief discussion on memory in LLM-\\nbased agents\\n16These are concisely summarized in Appendix A\\n8'),\n",
       "  Document(metadata={'page': 3, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='3) LLM-Profiled Dynamic Models glmdynamic : They\\npredict or describe changes to the environment. Gen-\\nerally, dynamic models form part of a comprehensive\\nworld model by predicting the next state s′from the\\ncurrent state sand action a. While typical RL uses\\nthe probability distribution p(s′|s, a)to model poten-\\ntial next states, LLM-based dynamic models directly\\npredict the next state s′=glmdynamic (s, a).\\nTask-Dependent LLM-Profiled Components In\\naddition to the universal components, certain LLM-\\nprofiled components are tailored to specific tasks. For\\ninstance, verbalizers are crucial in embodied environ-\\nments but unnecessary in NLIEs. A verbalizer trans-\\nlates actions and observations into inputs for planners;\\nfor example, in the Planner-Actor-Reporter workflow\\n(Wang et al., 2023a), a fine-tuned Visual Language\\nModel (VLM) along with glmplanner translates pixel\\nstates into textual inputs. Similarly, if environmental\\nfeedback is perceivable along with states, a verbalizer\\nmay be needed to translate this feedback into verbal\\ndescriptions for glmpolicy, akin to reward shaping in\\nRL where numerical stimuli are generated for policy\\nlearning. LLMs profiled as verbalizers, glmverbalizer\\n(Shinn et al., 2023), often guide descriptions accord-\\ning to specified criteria.\\nsettings to learn policy models that perform desirable behaviors.\\n3Note that planning algorithms may be utilized to structure a\\nplan of plans; for example, Tree-of-Thought employs tree search,\\nwhere each node potentially represents either a single action or an\\nentire plan.\\n4'),\n",
       "  Document(metadata={'page': 3, 'source': './assets-resources/paper-llm-components.pdf'}, page_content='deployment in planning-based agent systems. Addi-\\ntionally, Wan et al. (2024) suggest that \"splitting an\\noutput sequence into tokens might be a good choice\"\\nfor defining multi-step NLIEs methodically. Further-\\nmore, Yao et al. (2023a) formulate two-step NLIEs for\\ncreative writing by segmenting the problem-solving\\nprocess into distinct planning and execution phases.\\n2.3 Tool Environments\\nModern LLM agents are often enhanced with external\\ntools that improve their problem-solving capabilities\\n(Inaba et al., 2023; Yao et al., 2023b). The design and\\nintegration of these tools add complexity, requiring\\ncareful consideration of how LLMs interact not only\\nwith the task environments but also with these aux-\\niliary tools. Typically, actions in tool environments\\ninvolve interactions with resources that remain unaf-\\nfected by these interactions. For instance, retrieving\\ndata from Wikipedia constitutes a \"read-only\" action,\\nwhich does not modify the Wikipedia database. This\\nfeature distinguishes such tool-use actions from those\\nin conventional task environments or typical reinforce-\\nment learning (RL) settings, where actions generally\\nalter the environmental state. Nevertheless, it is impor-\\ntant to recognize that tool environment can be dynamic\\nthat can undergo changes externally. This aspect re-\\nflects the nature that tools should be considered ex-\\nternal environments rather than the agent’s internal\\nprocesses.\\nNested NLIE-QA + Tool Environments Tool envi-\\nronments are frequently established along with NLIEs\\nto aid in solving QA tasks. Shinn et al. (2023); Yao\\net al. (2023b) incorporate tools to enhance the fac-\\ntuality of responses. They define command-like ac-\\ntions such as “Search” and “LookUp” to interact with\\nWikipedia, with “Search” suggesting the top-5 similar\\nentities from the relevant wiki page, and “LookUp”\\nsimulating the Ctrl+F functionality in a browser. Be-\\nyond simple retrieval, Thoppilan et al. (2022) include\\na language translator and a calculator for dialog tasks.\\nSimilarly, Inaba et al. (2023) employ a calculator, im-\\nplemented using the Python eval function, to resolve\\nnumerical queries within the NumGLUE benchmark.\\n3 LLM-Profiled Components\\nThis section explores common agentic roles for which\\nLLMs are typically profiled. The components leverage\\nthe internal commonsense knowledge and reasoning\\nabilities of LLMs to generate actions, plans, estimate\\nvalues2, and infer subsequent states.\\n2Values refer to the estimated rewards (a quantitative measure\\nof the success or desirability of the outcomes) associated with tak-\\ning a certain action in a state, widely used in typical RL and MDPUniversal LLM-Profiled Components Specifically,\\nthe following task-agnostic components are profiled\\nand commonly used across various workflows. 1)\\nLLM-Profiled Policy glmpolicy: Policy models are de-\\nsigned to generate decisions, which could be an action\\nor a series of actions (plans) for execution in exter-\\nnal environments or use in search and planning algo-\\nrithms.3In contrast to typical RL policy models,\\nwhich learn to maximize cumulative rewards through\\ntrial and error, LLM-profiled policy models, denoted\\nasglmpolicy, utilize pre-trained knowledge and com-\\nmonsense derived from extensive textual data. We\\ndistinguish between two types of glmpolicy: an actor\\nglmactordirectly maps a state to an action, whereas\\na planner glmplanner generates a sequence of actions\\nfrom a given state. 2) LLM-Profiled Evaluators glmeval:\\nglmevalprovide feedback crucial for different work-\\nflows. They evaluate actions and states in search-based\\nworkflows (Hao et al., 2023; Yao et al., 2023a) and re-\\nvise decisions in feedback-learning workflows (Shinn\\net al., 2023; Wang et al., 2023b) (refer to §4 for more\\ndetails). These evaluators are integral to both direct\\naction assessment and broader strategic adjustments.\\n3) LLM-Profiled Dynamic Models glmdynamic : They\\npredict or describe changes to the environment. Gen-\\nerally, dynamic models form part of a comprehensive')],\n",
       " 'answer': {'answer': 'Reusable LLM-profiled components (LMPCs) include various task-agnostic components commonly used across different workflows. These components are: 1) LLM-Profiled Policy (glmpolicy) for generating decisions or actions; 2) LLM-Profiled Evaluators (glmeval) for providing feedback on actions and states; and 3) LLM-Profiled Dynamic Models (glmdynamic) for predicting changes in the environment.',\n",
       "  'sources': ['Li 2024']}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://python.langchain.com/v0.2/docs/how_to/qa_sources/\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# Desired schema for response\n",
    "class AnswerWithSources(TypedDict):\n",
    "    \"\"\"An answer to the question, with sources.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    sources: Annotated[\n",
    "        List[str],\n",
    "        ...,\n",
    "        \"List of sources (author + year) used to answer the question\",\n",
    "    ]\n",
    "\n",
    "\n",
    "# Our rag_chain_from_docs has the following changes:\n",
    "# - add `.with_structured_output` to the LLM;\n",
    "# - remove the output parser\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"context\": lambda x: format_docs(x[\"context\"]),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.with_structured_output(AnswerWithSources)\n",
    ")\n",
    "\n",
    "retrieve_docs = (lambda x: x[\"input\"]) | retriever\n",
    "\n",
    "chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"input\": query})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb \n",
    "Below are notebook from openai cookbook on these topics of search and embeddings:\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Code_search.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb\n",
    "- https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "- [In-context learning abilities of ChatGPT models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "- [Issue with long context](https://arxiv.org/pdf/2303.18223.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
