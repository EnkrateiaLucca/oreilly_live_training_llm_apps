{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-07-24-10-52-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install langchain_community\n",
    "!pip install langchain_openai\n",
    "!pip install langchainhub\n",
    "!pip install chromadb\n",
    "!pip install pypdf\n",
    "!pip install tiktoken\n",
    "!pip install python-dotenv\n",
    "!pip install unstructured\n",
    "!pip install pysqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# loading from a .env file\n",
    "# load_dotenv(dotenv_path=\"/full/path/to/your/.env\")\n",
    "\n",
    "# or \n",
    "# if you're on google colab just uncomment below and replace with your openai api key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<your-openai-api-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple QA System for Chatting with a PDF\n",
    "\n",
    "This part of the training will be mostly hands on with the code for building the qa PDF system with langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./assets-resources/llm_paper_know_dont_know.pdf\"\n",
    "loader = PyPDFLoader(pdf_path) # LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 0}, page_content='Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢Qiushi Sun♠Qipeng Guo♢\\nJiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware , consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗Corresponding author.\\nUnknowsKnows UnknowsKnows\\nKnown KnowsKnown Unknows\\nUnknown Unknows Unknown KnowsUnlockFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know ,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increasearXiv:2305.18153v2  [cs.CL]  30 May 2023'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 1}, page_content='this ratio, resulting in improved performance on\\nNLP tasks. We focus on the ratio of “Known Un-\\nknows” to “Unknown Unknows”, which indicates\\nthe model’s self-knowledge level, specifically un-\\nderstanding its own limitations and deficiencies in\\nthe unknows.\\nExisting datasets such as SQuAD2.0 (Rajpurkar\\net al., 2018) and NewsQA (Trischler et al., 2017),\\nwidely used in question answering (QA), have been\\nutilized to test the self-knowledge of models with\\nunanswerable questions. However, these questions\\nare context-specific and could become answerable\\nwhen supplemented with additional information.\\nSrivastava et al. (2022) attempted to address this by\\nevaluating LLMs’ competence in delineating their\\nknowledge boundaries, employing a set of 23 pairs\\nof answerable and unanswerable multiple-choice\\nquestions. They discovered that these models’ per-\\nformance barely surpassed that of random guessing.\\nKadavath et al. (2022) suggested probing the self-\\nknowledge of LLMs through the implementation\\nof a distinct \"Value Head\". Yet, this approach may\\nencounter difficulties when applied across varied\\ndomains or tasks due to task-specific training. Con-\\nsequently, we redirect our focus to the inherent\\nabilities of LLMs, and pose the pivotal question:\\n“Do large language models know what they don’t\\nknow? ”.\\nIn this study, we investigate the self-knowledge\\nof LLMs using a novel approach. By gathering\\nreference sentences with uncertain meanings, we\\ncan determine whether the model’s responses re-\\nflect uncertainty using a text similarity algorithm.\\nWe quantified the model’s self-knowledge using\\nthe F1 score. To address the small and idiosyn-\\ncratic limitations of existing datasets, we created\\na new dataset called SelfAware . This dataset com-\\nprises 1,032 unanswerable questions, which are dis-\\ntributed across five distinct categories, along with\\nan additional 2,337 questions that are classified as\\nanswerable. Experimental results on GPT-3, In-\\nstructGPT, LLaMA, and other LLMs demonstrate\\nthat in-context learning and instruction tuning can\\neffectively enhance the self-knowledge of LLMs.\\nHowever, the self-knowledge exhibited by the cur-\\nrent state-of-the-art model, GPT-4, measures at\\n75.47%, signifying a notable disparity when con-\\ntrasted with human self-knowledge, which is rated\\nat 84.93%.\\nOur key contributions to this field are summa-\\nrized as follows:•We have developed a new dataset, SelfAware ,\\nthat comprises a diverse range of commonly\\nposed unanswerable questions.\\n•We propose an innovative evaluation tech-\\nnique based on text similarity to quantify the\\ndegree of uncertainty inherent in model out-\\nputs.\\n•Through our detailed analysis of 20 LLMs,\\nbenchmarked against human self-knowledge,\\nwe identified a significant disparity between\\nthe most advanced LLMs and humans1.\\n2 Dataset Construction\\nTo conduct a more comprehensive evaluation of\\nthe model’s self-knowledge, we constructed a\\ndataset that includes a larger number and more di-\\nverse types of unanswerable questions than Know-\\nUnknowns dataset (Srivastava et al., 2022). To\\nfacilitate this, we collected a corpus of 2,858 unan-\\nswerable questions, sourced from online platforms\\nlike Quora and HowStuffWorks. These questions\\nwere meticulously evaluated by three seasoned an-\\nnotation analysts, each operating independently.\\nThe analysts were permitted to leverage external\\nresources, such as search engines. To ensure the va-\\nlidity of our dataset, we retained only the questions\\nthat all three analysts concurred were unanswerable.\\nThis rigorous process yielded a finalized collection\\nof 1,032 unanswerable questions.\\nIn pursuit of a comprehensive evaluation, we\\nopted for answerable questions drawn from three\\ndatasets: SQuAD (Rajpurkar et al., 2016), Hot-\\npotQA (Yang et al., 2018), and TriviaQA (Joshi\\net al., 2017). Our selection was guided by Sim-\\nCSE (Gao et al., 2021), which allowed us to iden-\\ntify and select the answerable questions semanti-\\ncally closest to the unanswerable ones. From these'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 1}, page_content='et al., 2017). Our selection was guided by Sim-\\nCSE (Gao et al., 2021), which allowed us to iden-\\ntify and select the answerable questions semanti-\\ncally closest to the unanswerable ones. From these\\nsources, we accordingly drew samples of 1,487,\\n182, and 668 questions respectively, amassing a\\ntotal of 2,337. Given that these questions can be\\neffectively addressed using information available\\non Wikipedia, the foundational corpus for the train-\\ning of current LLMs, it is plausible to infer that\\nthe model possesses the requisite knowledge to\\ngenerate accurate responses to these questions.\\nOur dataset, christened SelfAware , incorporates\\n1,032 unanswerable and 2,337 answerable ques-\\ntions. To reflect real-world distribution, our dataset\\n1The code pertinent to our study can be accessed\\nhttps://github.com/yinzhangyue/SelfAware'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 2}, page_content='Category Description Example Percentage\\nNo scientific\\nconsensusThe answer is still up\\nfor debate, with no consensus\\nin scientific community.“Are we alone in the universe,\\nor will we discover alien\\nlife at some point?”25%\\nImaginationThe question are about people’s\\nimaginations of the future.\"What will the fastest form of\\ntransportation be in 2050?\"15%\\nCompletely\\nsubjectiveThe answer depends on\\npersonal preference.\"Would you rather be shot\\ninto space or explore the\\ndeepest depths of the sea?\"27%\\nToo many\\nvariablesThe question with too\\nmany variables cannot\\nbe answered accurately.“John made 6 dollars mowing lawns\\nand 18 dollars weed eating.\\nIf he only spent 3 or 5 dollar a week,\\nhow long would the money last him?”10%\\nPhilosophicalThe question can yield\\nmultiple responses, but it\\nlacks a definitive answer.“How come god was\\nborn from nothingness?”23%\\nTable 1: Unanswerable questions in the SelfAware dataset that span across multiple categories.\\ncontains a proportion of answerable questions that\\nis twice as large as the volume of unanswerable\\nones. Nevertheless, to ensure the feasibility of test-\\ning, we have purposefully capped the number of\\nanswerable questions.\\n2.1 Dataset Analysis\\nTo gain insight into the reasons precluding a cer-\\ntain answer, we undertook a manual analysis of\\n100 randomly selected unanswerable questions. As\\ntabulated in Table 1, we have broadly segregated\\nthese questions into five distinctive categories. “No\\nScientific Consensus\" encapsulates questions that\\nignite ongoing debates within the scientific com-\\nmunity, such as those concerning the universe’s\\norigin. “Imagination\" includes questions involving\\nspeculative future scenarios, like envisaged events\\nover the next 50 years. “Completely Subjective\"\\ncomprises questions that are inherently personal,\\nwhere answers depend heavily on individual predis-\\npositions. “Too Many Variables\" pertains to mathe-\\nmatical problems that become unsolvable owing to\\nthe overwhelming prevalence of variables. Lastly,\\n“Philosophical\" represents questions of a profound,\\noften metaphysical, nature that resist concrete an-\\nswers. Ideally, upon encountering such questions,\\nthe model should express uncertainty instead of\\ndelivering conclusive responses.\\n3 Evaluation Method\\nThis section elucidates the methodology employed\\nfor assessing self-knowledge in the generated text.In order to achieve this, we define a similarity func-\\ntion,fsim, to compute the similarity, S, between\\na given sentence, t, and a collection of reference\\nsentences, U={u1, u2, . . . , u n}, endowed with\\nuncertain meanings.\\nSi=fsim(t, ui). (1)\\nWhenever any Sisurpasses a pre-determined\\nthreshold T, we perceive the text tas encompass-\\ning uncertain meanings, thereby eliminating the\\nneed for manual evaluation of the response.\\nGiven the substantial disparity in the volume of\\nanswerable and unanswerable questions in Self-\\nAware , we adopt the F1 score as a measure of\\nLLMs’ self-knowledge. Our focus rests on identi-\\nfying unanswerable questions, hence we designate\\nthem as positive cases and categorize answerable\\nquestions as negative cases.\\n4 Experiment\\n4.1 Model\\nWe conduct a sequence of experiments to evaluate\\nthe degree of self-knowledge manifested by various\\nLLMs, including GPT-3 (Brown et al., 2020) and\\nInstructGPT (Ouyang et al., 2022) series, as well\\nas the recent LLaMA (Touvron et al., 2023) and\\nits derivative models, namely Alpaca (Taori et al.,\\n2023) and Vicuna (Chiang et al., 2023). Our in-\\nvestigative approach employed three distinct input\\nforms: Direct, Instruction, and In-Context Learn-\\ning (ICL), which is encapsulated in Appendix A.4.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 3}, page_content='350M1.3B 6.7B 175B203040506070F1 Scores\\n22.3840.11\\n26.9640.33\\n26.1743.47\\n27.5444.87Direct\\n350M1.3B 6.7B 175B203040506070F1 Scores\\n30.4242.31\\n30.1745.91\\n33.3348.79\\n45.6749.61Instruction\\n350M1.3B 6.7B 175B203040506070F1 Scores\\n34.2747.93\\n36.2748.4247.2455.81 55.565.12In-Context Learning\\nGPT-3\\nInstructGPT\\nModelFigure 2: Experimental results using three different input forms on a series of models from GPT-3(ada, babbage,\\ncurie, and davinci) and InstructGPT(text-ada-001, text-babbage-001, text-curie-001, and text-davinci-001)\\n0 10 20 30 40 50 60 70 80\\nF1 Scoresdavincitext-davinci-001text-davinci-002text-davinci-003gpt-3.5-turbo-0301gpt-4-0314HumanModels\\n45.6749.6147.4851.4354.1275.4784.93\\nFigure 3: Comparison between the davinci series and\\nhuman self-knowledge in instruction input form.\\n4.2 Setting\\nWe devised the reference sentence set Uthrough\\na process that combined automated generation by\\nLLMs and manual filtering, detailed further in Ap-\\npendix A.1. To quantify the similarity between\\ntarget and reference sentences, we utilized Sim-\\nCSE (Gao et al., 2021), setting the similarity thresh-\\nold to 0.75 during our experiments. An exploration\\nof threshold ablation is available in Appendix A.2.\\nTo counteract potential errors in similarity calcula-\\ntion induced by varying lengths of the target and\\nreference sentences, we employed a sliding win-\\ndow of length 5 to parse the target sentence into\\nsemantic chunks. During the generation process,\\nwe set the temperature to 0.7. We selected a ran-\\ndom sample of 100 instances for GPT-4, while the\\nremainder of the models were scrutinized using the\\nfullSelfAware dataset.\\n4.3 Human Self-Knowledge\\nTo establish a benchmark for human self-\\nknowledge, we engaged two volunteers and se-\\nlected 100 random samples from the SelfAware\\ndataset. The volunteers has 30 minutes to make\\ndavinci\\ntext-davinci-001 text-davinci-002 text-davinci-003\\ngpt-3.5-turbo-0301\\nModels0102030405060F1 Scores55.565.1266.46 66.28\\n60.86Figure 4: Experimental comparison of davinci series in\\nICL input form.\\njudgments on the same set of questions, yielding\\nan average F1 score of 84.93%, which we sub-\\nsequently adopted as the benchmark for human\\nself-knowledge. Detailed scores are available in\\nAppendix A.3.\\n4.4 Analysis\\nWe evaluate the manifestation of LLMs’ self-\\nknowledge, centering our investigation on three\\nfundamental dimensions: the size of the model,\\nthe impact of instruction tuning, and the influence\\nexerted by different input forms.\\nModel Size. Figure 2 illustrates the correlation\\nbetween model size and self-knowledge across var-\\nious LLMs. It is noteworthy that across all three\\ninput forms, an augmentation in model parameter\\nsize is associated with an elevation in the F1 Score,\\nwith the most conspicuous enhancement manifest-\\ning in the ICL input form. Therefore, our analysis\\nindicates that an LLM’s self-knowledge tends to\\nenhance with increasing model size, a trend consis-\\ntent with the scaling law.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 4}, page_content='LLaMA-7B Alpaca-7B Vicuna-7BLLaMA-13B Alpaca-13B Vicuna-13B LLaMA-30B LLaMA-65B\\nModels01020304050F1 Scores28.5735.8742.78\\n30.1237.4447.84\\n30.346.89Figure 5: Experimental results obtained from LLaMA\\nand its derived models, Alpaca and Vicuna in instruction\\ninput form.\\nInstruction Tuning. Figure 2 delineates that\\nmodels from the InstructGPT series exhibit a su-\\nperior level of self-knowledge compared to their\\nGPT-3 counterparts. Further evidence of model\\nenhancement is provided by Figure 4, where text-\\ndavinci models show significant improvement rela-\\ntive to the base davinci model. An additional com-\\nparative analysis, presented in Figure 5, evaluates\\nLLaMA against its derivative models. The results\\nunderscore a notable increase in self-knowledge\\nfor Alpaca and Vicuna upon instruction tuning, ex-\\nceeding their base model performances. Among\\nthese, Vicuna-13B outperforms the LLaMA-65B,\\ncorroborating the efficacy of instruction tuning for\\nenhancing model self-knowledge.\\nInput Forms. As shown in Figure 2, the incorpo-\\nration of instructions and examples serves to boost\\nthe self-knowledge of both the GPT-3 and Instruct-\\nGPT series. Specifically, ICL input form, providing\\nricher contextual information, contributes to a sig-\\nnificant enhancement in models’ self-knowledge.\\nThis impact is particularly noticeable in the davinci\\nmodel, where ICL facilitates a 27.96% improve-\\nment over the direct. Moreover, a comparison be-\\ntween Figure 3 and Figure 4 reveals that the in-\\nclusion of instructions and examples successfully\\nminimizes the performance disparity between the\\ndavinci and text-davinci models, suggesting an ac-\\nquisition of self-knowledge from the instructions\\nand provided examples.\\nCompared with Human. Figure 3 reveals that,\\nwithout supplementary samples, GPT-4 currently\\nperforms best among the tested models, achieving\\nan impressive F1 score of 75.47%. However, a no-\\nticeable gap becomes evident when comparing this\\ntext-ada-001\\ntext-babbage-001text-curie-001text-davinci-001 text-davinci-002 text-davinci-003\\ngpt-3.5-turbo-0301gpt-4-0314\\nModels0510152025303540Accuracy\\n2.484.45 4.710.6115.730.2538.2942.64Figure 6: Accuracy of the InstructGPT series when\\nresponding to answerable questions in instruction input\\nform.\\nperformance to the human benchmark of 84.93%.\\nThis underscores the considerable potential that re-\\nmains for enhancing the self-knowledge level of\\nLLMs.\\nAnswerable Questions. Figure 6 traces the per-\\nformance evolution of the InstructGPT series in\\naddressing answerable questions, adhering to the\\nclosed-book question answering paradigm (Tou-\\nvron et al., 2023), where output accuracy is con-\\ntingent on the presence of the correct answer. Our\\nobservations underscore a steady enhancement in\\nQA task accuracy corresponding to an increase\\nin model parameter size and continuous learning.\\nParticularly, the accuracy of text-davinci-001 expe-\\nriences a significant ascent, scaling from a meager\\n2.48% in text-ada-001 to 10.61%, whereas GPT-4\\nmarks an even more striking jump to 42.64%.\\n5 Conclusion\\nThis study investigates the self-knowledge of\\nLLMs by evaluating their ability to identify unan-\\nswerable questions. Through the introduction of a\\nnovel dataset and an automated method for detect-\\ning uncertainty in the models’ responses, we are\\nable to accurately measure the self-knowledge of\\nLLMs such as GPT-3, InstructGPT and LLaMA.\\nOur results reveal that while these models possess\\na certain degree of self-knowledge, there is still\\nan apparent disparity in comparison to human self-\\nknowledge. This highlights the need for further\\nresearch in this area to enhance the ability of LLMs\\nto understand their own limitations on the unknows.\\nSuch efforts will lead to more accurate and reliable\\nresponses from LLMs, which will have a positive\\nimpact on their applications in diverse fields.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 5}, page_content='Limitations\\n•Generalization of reference sentences. At\\npresent, we have selected sentences with un-\\ncertain meanings exclusively from the GPT-3\\nand InstructGPT series, potentially overlook-\\ning uncertainty present in responses generated\\nby other LLMs. However, it is not feasible\\nto catalog all sentences with uncertain mean-\\nings exhaustively. As a direction for future\\nresearch, we propose to concentrate on the\\nautomated acquisition of more accurate refer-\\nence sentences to address this concern.\\n•Limitations of input forms: Our exami-\\nnation was confined to three unique input\\nforms: direct, instruction, and ICL. There\\nis burgeoning research aimed at bridging the\\ngap between models and human-like meth-\\nods of reasoning and problem-solving, includ-\\ning but not limited to approaches like Re-\\nflexion (Shinn et al., 2023), ToT (Yao et al.,\\n2023), MoT (Li and Qiu, 2023). Future en-\\ndeavors will integrate additional cognitive and\\ndecision-making methods to delve deeper into\\nthe self-knowledge exhibited by these LLMs.\\nEthics Statement\\nThe SelfAware dataset, meticulously curated to\\nevaluate LLMs’ ability to discern unanswerable\\nquestions, is composed of unanswerable questions\\nextracted from sources such as Quora and How-\\nStuffWorks, alongside answerable questions pro-\\ncured from three distinct open datasets. Every ques-\\ntion was thoroughly examined for relevance and\\nharmlessness. To ensure content validity, three an-\\nnotation analysts, compensated at local wage stan-\\ndards, dedicated regular working hours to content\\nreview.\\nThroughout our research process, we under-\\nscored the significance of privacy, data security,\\nand strict compliance with dataset licenses. In\\norder to protect data integrity, we implemented\\nanonymization and content filtration mechanisms.\\nOur adherence to OpenAI’s stipulations remained\\nunyielding for the usage of GPT-3 and InstructGPT\\nmodels, and likewise for Meta’s terms pertaining\\nto LLaMA models. We rigorously vetted the li-\\ncenses of the three publicly available datasets for\\ncompliance, ensuring that all our research method-\\nologies were in alignment with ethical standards at\\nthe institutional, national, and global levels.Adhering to the CC-BY-SA-4.0 protocol, the\\ndataset, once publicly released, will be reserved\\nexclusively for research purposes. We pledge to\\npromptly and effectively address any concerns relat-\\ning to the dataset, while concurrently anticipating\\nresearchers to maintain high ethical standards in\\ntheir utilization of this data.\\nAcknowledgement\\nWe wish to express our gratitude to our colleagues\\nin the FudanNLP group whose insightful sugges-\\ntions, perspectives, and thought-provoking discus-\\nsions significantly contributed to this work. Our\\nsincere appreciation also extends to the anonymous\\nreviewers and area chairs, whose constructive feed-\\nback was instrumental in refining the quality of\\nour study. This work was supported by the Na-\\ntional Natural Science Foundation of China (No.\\n62236004 and No. 62022027) and CAAI-Huawei\\nMindSpore Open Fund.\\nReferences\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\\nChen, Eric Chu, Jonathan H. Clark, Laurent El\\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\\nJan Botha, James Bradbury, Siddhartha Brahma,\\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\\nCherry, Christopher A. Choquette-Choo, Aakanksha\\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 5}, page_content='cia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\\nShelby, Ambrose Slone, Daniel Smilkov, David R.\\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\\nVijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,\\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 6}, page_content='ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\\nreport.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In Ad-\\nvances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Process-\\ning Systems 2020, NeurIPS 2020, December 6-12,\\n2020, virtual .\\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\\nWilliam W Cohen. 2022. Program of thoughts\\nprompting: Disentangling computation from reason-\\ning for numerical reasoning tasks. ArXiv preprint ,\\nabs/2211.12588.\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\\nsource chatbot impressing gpt-4 with 90%* chatgpt\\nquality.\\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\\nand Tushar Khot. 2022. Complexity-based prompt-\\ning for multi-step reasoning. ArXiv preprint ,\\nabs/2210.00720.\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\\nSimCSE: Simple contrastive learning of sentence em-\\nbeddings. In Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Process-\\ning, pages 6894–6910, Online and Punta Cana, Do-\\nminican Republic. Association for Computational\\nLinguistics.\\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers) , pages 1601–1611, Vancouver,\\nCanada. Association for Computational Linguistics.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\\nHenighan, Dawn Drain, Ethan Perez, Nicholas\\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\\nEli Tran-Johnson, et al. 2022. Language models\\n(mostly) know what they know. ArXiv preprint ,\\nabs/2207.05221.\\nAitor Lewkowycz, Anders Andreassen, David Dohan,\\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,Ambrose Slone, Cem Anil, Imanol Schlag, Theo\\nGutman-Solo, et al. 2022. Solving quantitative\\nreasoning problems with language models. ArXiv\\npreprint , abs/2206.14858.\\nXiaonan Li and Xipeng Qiu. 2023. Mot: Pre-\\nthinking and recalling enable chatgpt to self-\\nimprove with memory-of-thoughts. ArXiv preprint ,\\nabs/2305.05181.\\nOpenAI. 2023. Gpt-4 technical report.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow in-\\nstructions with human feedback. ArXiv preprint ,\\nabs/2203.02155.\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\\nKnow what you don’t know: Unanswerable ques-\\ntions for SQuAD. In Proceedings of the 56th Annual\\nMeeting of the Association for Computational Lin-\\nguistics (Volume 2: Short Papers) , pages 784–789,\\nMelbourne, Australia. Association for Computational\\nLinguistics.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. SQuAD: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Natu-\\nral Language Processing , pages 2383–2392, Austin,\\nTexas. Association for Computational Linguistics.\\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin\\nGopinath, Karthik Narasimhan, and Shunyu Yao.\\n2023. Reflexion: Language agents with verbal rein-\\nforcement learning.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 6}, page_content='Gopinath, Karthik Narasimhan, and Shunyu Yao.\\n2023. Reflexion: Language agents with verbal rein-\\nforcement learning.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\\nAdam R Brown, Adam Santoro, Aditya Gupta, Adrià\\nGarriga-Alonso, et al. 2022. Beyond the imitation\\ngame: Quantifying and extrapolating the capabilities\\nof language models. ArXiv preprint , abs/2206.04615.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model. https://\\ngithub.com/tatsu-lab/stanford_alpaca .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models. ArXiv\\npreprint , abs/2302.13971.\\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Har-\\nris, Alessandro Sordoni, Philip Bachman, and Kaheer\\nSuleman. 2017. NewsQA: A machine comprehen-\\nsion dataset. In Proceedings of the 2nd Workshop\\non Representation Learning for NLP , pages 191–200,\\nVancouver, Canada. Association for Computational\\nLinguistics.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 7}, page_content='Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\\nproves chain of thought reasoning in language mod-\\nels.ArXiv preprint , abs/2203.11171.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\\nand Denny Zhou. 2022. Chain of thought prompt-\\ning elicits reasoning in large language models. In\\nAdvances in Neural Information Processing Systems .\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainable multi-hop question answering.\\nInProceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n2369–2380, Brussels, Belgium. Association for Com-\\nputational Linguistics.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nThomas L Griffiths, Yuan Cao, and Karthik\\nNarasimhan. 2023. Tree of thoughts: Deliberate\\nproblem solving with large language models. ArXiv\\npreprint , abs/2305.10601.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\\nLeast-to-most prompting enables complex reason-\\ning in large language models. ArXiv preprint ,\\nabs/2205.10625.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 8}, page_content='A Appendix\\nA.1 Uncertainty Text\\nTo assemble a set of reference sentences, we ran-\\ndomly chose 100 entries from the SelfAware dataset.\\nFor each model in the GPT-3 and InstructGPT se-\\nries, we conducted a preliminary test using the\\ndirect input form and manually curated sentences\\nthat displayed uncertainty. From this pre-test, we\\nprocured 16 sentences manifesting uncertain con-\\nnotations to serve as our reference sentences. After\\nnormalizing these sentences by eliminating punc-\\ntuation and converting to lowercase, we utilized\\nthem to compute similarity with target sentences\\nthroughout our experimental procedure.\\n1. The answer is unknown.\\n2. The answer is uncertain.\\n3. The answer is unclear.\\n4. There is no scientific evidence.\\n5. There is no definitive answer.\\n6. There is no right answer.\\n7. There is much debate.\\n8. There is no known case.\\n9. There is no concrete answer to this question.\\n10. There is no public information available.\\n11. It is impossible to know.\\n12. It is impossible to answer.\\n13. It is difficult to predict.\\n14. It is not known.\\n15. We do not know.\\n16. I’m not sure.\\nA.2 Threshold ablation\\nWe generated 100 new responses using the text-\\ndavinci-002 with direct input form and manually\\nfiltered out sentences that contained uncertainty.\\nWe then used SimCSE (Gao et al., 2021) to calcu-\\nlate the similarity between these sentences and the\\nreference sentences in Appendix A.1. We tested\\nvarious thresholds for filtering sentences with un-\\ncertain meanings and compared them to manuallyThreshold Precision Recall F1\\n0.95 100.00 70.00 82.35\\n0.90 100.00 75.00 85.71\\n0.85 100.00 75.00 85.71\\n0.80 100.00 80.00 88.89\\n0.75 100.00 85.00 91.89\\n0.70 89.47 90.00 89.73\\n0.65 86.95 90.00 88.45\\nTable 2: Evaluation results comparing sentences with\\nuncertain meaning filtered by various thresholds.\\nHuman Precision Recall F1\\nV olunteer A 91.52 78.26 84.37\\nV olunteer B 96.36 76.81 85.48\\nTable 3: Evaluation results of 100 responses from two\\nvolunteers.\\nannotated sentences. We considered unanswerable\\nquestions as positive examples and calculated preci-\\nsion, recall, and F1 score. The results in Table 2 in-\\ndicate that a threshold of 0.75 produced the highest\\nF1 score, balancing precision and the inclusion of\\nother uncertain sentences. As a result, we selected\\n0.75 as the similarity threshold for subsequent ex-\\nperiments.\\nA.3 Human Self-Knowledge Test\\nThe evaluation results for the responses from our\\ninvited volunteers are presented in Table 3. The\\nF1 scores for the responses were high, indicating\\nthat both volunteers exhibited a strong level of self-\\nknowledge.\\nA.4 Template\\nThe input templates used in our experiments, Di-\\nrect, Instruction, and ICL, are illustrated in Fig-\\nures 7, 8, and 9, respectively. In the ICL template,\\nwe composed 3 answerable and 3 unanswerable\\nquestions and provided the corresponding answers\\nmanually.'),\n",
       " Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 9}, page_content=\"Q: Are we alone in the universe, or will we discover alien life at some point?\\nA:Figure 7: Direct input form.\\nWhen answering questions, interpret them literally and think them carefully. If the question is \\nunanswerable or unknowable, it is appropriate to say, “The answer is unknown.”\\nQ: Are we alone in the universe, or will we discover alien life at some point?\\nA:\\nFigure 8: Instruction input form.\\nQ: What is the nearest extraterrestrial civilization to us?\\nA: It is currently unknown if there are any extraterrestrial civilizations in proximity to Earth. Scientists have not yet found definitive evidence of the existence of extraterrestrial civilizations. Thus, the answer is unknown.Q: What is the highest building in New York?\\nA: The highest building in New York is the One World Trade Center. It stands at a height of 1,776 feet (541 \\nmeters). Thus, the answer is the One World Trade Center.Q: Will nuclear war break out in the world in 2050?A: It is impossible to predict with certainty if a nuclear war will break out in the world in 2050. The likelihood of such an event will depend on a wide range of factors, including international relations, political tensions, and military capabilities. Thus, the answer is unknown.Q: Alice's class is going on a field trip to the Ocean Park. There are 39 students and several teachers in the class. A bus can seat 20 people. How many buses do they need?\\nA: The number of buses needed for the field trip depends on the number of teachers. If there is only one \\nteacher, then two buses will suffice. However, additional buses will be necessary if there are two or more teachers. Thus, the answer is unknown.Q: Are Local H and For Against both from the United States?A: Local H is a rock band from Zion, Illinois, United States. For Against is an American indie rock band formed in 1984 in Lincoln, Nebraska. Both of these bands are from the United States. Thus, the answer is \\nyes.Q: Gjetost is the national cheese of which country?A:It is the national cheese of Norway, and it is a popular ingredient in traditional Norwegian cuisine. Thus, \\nthe answer is Norway.\\nQ: Are we alone in the universe, or will we discover alien life at some point?\\nA:\\nFigure 9: ICL input form.\")]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs = loader.load_and_split() # SPLIT\n",
    "pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './assets-resources/llm_paper_know_dont_know.pdf', 'page': 0}, page_content='Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢Qiushi Sun♠Qipeng Guo♢\\nJiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware , consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗Corresponding author.\\nUnknowsKnows UnknowsKnows\\nKnown KnowsKnown Unknows\\nUnknown Unknows Unknown KnowsUnlockFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know ,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increasearXiv:2305.18153v2  [cs.CL]  30 May 2023')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_obj = pdf_docs[0]\n",
    "doc_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢Qiushi Sun♠Qipeng Guo♢\\nJiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware , consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗Corresponding author.\\nUnknowsKnows UnknowsKnows\\nKnown KnowsKnown Unknows\\nUnknown Unknows Unknown KnowsUnlockFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know ,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increasearXiv:2305.18153v2  [cs.CL]  30 May 2023'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_obj.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Do Large Language Models Know What They Don’t Know?\n",
       "Zhangyue Yin♢Qiushi Sun♠Qipeng Guo♢\n",
       "Jiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢\n",
       "♢School of Computer Science, Fudan University\n",
       "♠Department of Mathematics, National University of Singapore\n",
       "{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\n",
       "{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\n",
       "Abstract\n",
       "Large language models (LLMs) have a wealth\n",
       "of knowledge that allows them to excel in vari-\n",
       "ous Natural Language Processing (NLP) tasks.\n",
       "Current research focuses on enhancing their\n",
       "performance within their existing knowledge.\n",
       "Despite their vast knowledge, LLMs are still\n",
       "limited by the amount of information they can\n",
       "accommodate and comprehend. Therefore, the\n",
       "ability to understand their own limitations on\n",
       "the unknows, referred to as self-knowledge,\n",
       "is of paramount importance. This study aims\n",
       "to evaluate LLMs’ self-knowledge by assess-\n",
       "ing their ability to identify unanswerable or\n",
       "unknowable questions. We introduce an auto-\n",
       "mated methodology to detect uncertainty in the\n",
       "responses of these models, providing a novel\n",
       "measure of their self-knowledge. We further in-\n",
       "troduce a unique dataset, SelfAware , consisting\n",
       "of unanswerable questions from five diverse cat-\n",
       "egories and their answerable counterparts. Our\n",
       "extensive analysis, involving 20 LLMs includ-\n",
       "ing GPT-3, InstructGPT, and LLaMA, discov-\n",
       "ering an intrinsic capacity for self-knowledge\n",
       "within these models. Moreover, we demon-\n",
       "strate that in-context learning and instruction\n",
       "tuning can further enhance this self-knowledge.\n",
       "Despite this promising insight, our findings also\n",
       "highlight a considerable gap between the capa-\n",
       "bilities of these models and human proficiency\n",
       "in recognizing the limits of their knowledge.\n",
       "“True wisdom is knowing what you don’t know.”\n",
       "–Confucius\n",
       "1 Introduction\n",
       "Recently, Large Language Models (LLMs) such\n",
       "as GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\n",
       "2023), and LLaMA (Touvron et al., 2023) have\n",
       "shown exceptional performance on a wide range\n",
       "of NLP tasks, including common sense reason-\n",
       "ing (Wei et al., 2022; Zhou et al., 2022) and mathe-\n",
       "∗Corresponding author.\n",
       "UnknowsKnows UnknowsKnows\n",
       "Known KnowsKnown Unknows\n",
       "Unknown Unknows Unknown KnowsUnlockFigure 1: Know-Unknow Quadrant. The horizontal axis\n",
       "represents the model’s memory capacity for knowledge,\n",
       "and the vertical axis represents the model’s ability to\n",
       "comprehend and utilize knowledge.\n",
       "matical problem-solving (Lewkowycz et al., 2022;\n",
       "Chen et al., 2022). Despite their ability to learn\n",
       "from huge amounts of data, LLMs still have lim-\n",
       "itations in their capacity to retain and understand\n",
       "information. To ensure responsible usage, it is cru-\n",
       "cial for LLMs to have the capability of recognizing\n",
       "their limitations and conveying uncertainty when\n",
       "responding to unanswerable or unknowable ques-\n",
       "tions. This acknowledgment of limitations, also\n",
       "known as “ knowing what you don’t know ,” is a\n",
       "crucial aspect in determining their practical appli-\n",
       "cability. In this work, we refer to this ability as\n",
       "model self-knowledge.\n",
       "The Know-Unknow quadrant in Figure 1 il-\n",
       "lustrates the relationship between the model’s\n",
       "knowledge and comprehension. The ratio of\n",
       "“Known Knows” to “Unknown Knows” demon-\n",
       "strates the model’s proficiency in understanding\n",
       "and applying existing knowledge. Techniques\n",
       "such as Chain-of-Thought (Wei et al., 2022), Self-\n",
       "Consistency (Wang et al., 2022), and Complex\n",
       "CoT (Fu et al., 2022) can be utilized to increasearXiv:2305.18153v2  [cs.CL]  30 May 2023"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "Markdown(doc_obj.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x1315396d0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x131549b50>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='https://api.openai.com/v1', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings() # EMBED\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x131586890>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(pdf_docs, embedding=embeddings) # STORE\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x131586890>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever() \n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Being an effective and engaging instructor for live trainings about AI, especially on topics like Large Language Models (LLMs), involves a combination of solid content knowledge, interactive teaching techniques, and enthusiasm. Here are some strategies to help you get everyone excited about LLMs:\\n\\n### 1. **Start with a Hook**\\n- **Real-World Examples:** Begin with captivating examples of what LLMs can do, such as generating creative content, aiding in coding, or personalizing user experiences. \\n- **Current Events:** Mention recent advancements or newsworthy applications of LLMs to show their relevance and impact.\\n\\n### 2. **Make It Relatable**\\n- **Everyday Applications:** Discuss how LLMs are used in everyday technologies like chatbots, virtual assistants, and recommendation systems.\\n- **Personal Stories:** Share personal anecdotes or case studies where LLMs made a significant difference.\\n\\n### 3. **Simplify Complex Concepts**\\n- **Analogies and Metaphors:** Use simple analogies to explain complex concepts. For instance, compare the training of an LLM to teaching a child a new language.\\n- **Step-by-Step Explanations:** Break down intricate ideas into more digestible parts and build up gradually.\\n\\n### 4. **Interactive Learning**\\n- **Hands-On Activities:** Include practical exercises where participants can interact with LLMs, like using GPT-3 to generate text.\\n- **Live Demos:** Perform live demonstrations of LLMs in action, showing how they generate responses or complete tasks.\\n- **Q&A Sessions:** Encourage questions and have open discussions to address uncertainties and foster a deeper understanding.\\n\\n### 5. **Use Visual Aids**\\n- **Slides and Diagrams:** Use visually appealing slides and diagrams to illustrate concepts.\\n- **Videos and Animations:** Incorporate short videos or animations to demonstrate how LLMs work.\\n\\n### 6. **Create a Collaborative Environment**\\n- **Group Activities:** Organize group tasks or brainstorming sessions where participants can collaborate and share ideas.\\n- **Discussion Forums:** Set up online forums or chat groups for continuous discussion and peer support.\\n\\n### 7. **Encourage Experimentation**\\n- **Toolkits and Platforms:** Provide access to tools and platforms where participants can experiment with building or using LLMs.\\n- **Mini Projects:** Assign small projects or challenges that encourage creative use of LLMs.\\n\\n### 8. **Highlight Ethical Considerations**\\n- **Responsible AI:** Discuss the ethical implications and responsibilities involved in using LLMs, fostering a sense of conscientiousness.\\n- **Bias and Fairness:** Explain the challenges of bias and fairness in LLMs and discuss ways to mitigate these issues.\\n\\n### 9. **Showcase Future Trends**\\n- **Innovation and Research:** Talk about the latest research and future trends in the field of LLMs and AI.\\n- **Career Opportunities:** Highlight potential career paths and opportunities in the AI industry.\\n\\n### 10. **Provide Resources for Further Learning**\\n- **Reading Materials:** Share articles, papers, and books that delve deeper into LLMs.\\n- **Online Courses and Tutorials:** Recommend online courses, tutorials, and workshops for continued learning.\\n\\n### 11. **Feedback and Adaptation**\\n- **Continuous Improvement:** Collect feedback from participants and continuously adapt your teaching methods to better meet their needs.\\n- **Iterative Approach:** Be open to changing your approach based on what works best for your audience.\\n\\nBy combining these strategies, you can create an engaging and informative learning experience that not only educates your participants about LLMs but also ignites their excitement and curiosity about the possibilities of AI.', response_metadata={'token_usage': {'completion_tokens': 732, 'prompt_tokens': 31, 'total_tokens': 763}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'stop', 'logprobs': None}, id='run-f4b5a904-983e-4c5d-a399-1a0a5e0e494f-0', usage_metadata={'input_tokens': 31, 'output_tokens': 732, 'total_tokens': 763})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"How can I be a better instructor for my live-trainings about AI to get everyone excited about LLMs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x131ca7e10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x16e2cb910>, model_name='gpt-4o', openai_api_key=SecretStr('**********'), openai_api_base='https://api.openai.com/v1', openai_proxy='')), document_prompt=PromptTemplate(input_variables=['page_content'], template='Context:\\n{page_content}'), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x131586890>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_qa = RetrievalQA.from_llm(llm=llm, retriever=retriever, return_source_documents=True) # RETRIEVE\n",
    "pdf_qa\n",
    "# pdf_qa = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=retriever) # RETRIEVE\n",
    "# pdf_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarize this paper into a set of instructive bullet points.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pdf_qa.invoke({\"query\": query, \"chat_history\": []}) # adding chat history so the model remembers previous questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Summarize this paper into a set of instructive bullet points.',\n",
       " 'chat_history': [],\n",
       " 'result': \"- **Objective**: Assess the self-knowledge of language models (LLMs) by evaluating their ability to identify unanswerable questions.\\n- **Dataset**: Created the SelfAware dataset, which includes unanswerable questions from sources like Quora and HowStuffWorks, as well as answerable questions from open datasets.\\n- **Categories of Unanswerable Questions**:\\n  - No scientific consensus\\n  - Imagination (speculative future scenarios)\\n  - Completely subjective (personal preference)\\n  - Too many variables\\n  - Philosophical (profound, metaphysical nature)\\n- **Reference Sentences**: Selected 16 sentences with uncertain meanings from GPT-3 and InstructGPT for similarity comparison.\\n- **Similarity Calculation**: Used SimCSE to compute similarity between generated sentences and reference sentences.\\n- **Threshold Ablation**: Determined that a similarity threshold of 0.75 produced the highest F1 score for identifying uncertain sentences.\\n- **Input Forms**: Evaluated three input forms: Direct, Instruction, and In-Context Learning (ICL).\\n- **Models Tested**: Included GPT-3, InstructGPT, LLaMA, Alpaca, and Vicuna.\\n- **Findings**:\\n  - Instruction tuning significantly enhances model self-knowledge.\\n  - ICL input form improves self-knowledge by providing richer contextual information.\\n  - GPT-4 outperforms other models but still lags behind human performance.\\n- **Human Benchmark**: Volunteers exhibited strong self-knowledge with high F1 scores.\\n- **Limitations**:\\n  - Generalization of reference sentences limited to GPT-3 and InstructGPT.\\n  - Confinement to three input forms.\\n- **Future Directions**: Propose automated acquisition of more accurate reference sentences and integration of additional cognitive and decision-making methods.\\n- **Ethical Considerations**: Ensured relevance, harmlessness, privacy, data security, and compliance with dataset licenses.\\n- **Conclusion**: While LLMs possess some self-knowledge, there's a significant gap compared to humans, indicating the need for further research to enhance LLMs' understanding of their limitations.\",\n",
       " 'source_documents': [Document(metadata={'page': 8, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}, page_content='A Appendix\\nA.1 Uncertainty Text\\nTo assemble a set of reference sentences, we ran-\\ndomly chose 100 entries from the SelfAware dataset.\\nFor each model in the GPT-3 and InstructGPT se-\\nries, we conducted a preliminary test using the\\ndirect input form and manually curated sentences\\nthat displayed uncertainty. From this pre-test, we\\nprocured 16 sentences manifesting uncertain con-\\nnotations to serve as our reference sentences. After\\nnormalizing these sentences by eliminating punc-\\ntuation and converting to lowercase, we utilized\\nthem to compute similarity with target sentences\\nthroughout our experimental procedure.\\n1. The answer is unknown.\\n2. The answer is uncertain.\\n3. The answer is unclear.\\n4. There is no scientific evidence.\\n5. There is no definitive answer.\\n6. There is no right answer.\\n7. There is much debate.\\n8. There is no known case.\\n9. There is no concrete answer to this question.\\n10. There is no public information available.\\n11. It is impossible to know.\\n12. It is impossible to answer.\\n13. It is difficult to predict.\\n14. It is not known.\\n15. We do not know.\\n16. I’m not sure.\\nA.2 Threshold ablation\\nWe generated 100 new responses using the text-\\ndavinci-002 with direct input form and manually\\nfiltered out sentences that contained uncertainty.\\nWe then used SimCSE (Gao et al., 2021) to calcu-\\nlate the similarity between these sentences and the\\nreference sentences in Appendix A.1. We tested\\nvarious thresholds for filtering sentences with un-\\ncertain meanings and compared them to manuallyThreshold Precision Recall F1\\n0.95 100.00 70.00 82.35\\n0.90 100.00 75.00 85.71\\n0.85 100.00 75.00 85.71\\n0.80 100.00 80.00 88.89\\n0.75 100.00 85.00 91.89\\n0.70 89.47 90.00 89.73\\n0.65 86.95 90.00 88.45\\nTable 2: Evaluation results comparing sentences with\\nuncertain meaning filtered by various thresholds.\\nHuman Precision Recall F1\\nV olunteer A 91.52 78.26 84.37\\nV olunteer B 96.36 76.81 85.48\\nTable 3: Evaluation results of 100 responses from two\\nvolunteers.\\nannotated sentences. We considered unanswerable\\nquestions as positive examples and calculated preci-\\nsion, recall, and F1 score. The results in Table 2 in-\\ndicate that a threshold of 0.75 produced the highest\\nF1 score, balancing precision and the inclusion of\\nother uncertain sentences. As a result, we selected\\n0.75 as the similarity threshold for subsequent ex-\\nperiments.\\nA.3 Human Self-Knowledge Test\\nThe evaluation results for the responses from our\\ninvited volunteers are presented in Table 3. The\\nF1 scores for the responses were high, indicating\\nthat both volunteers exhibited a strong level of self-\\nknowledge.\\nA.4 Template\\nThe input templates used in our experiments, Di-\\nrect, Instruction, and ICL, are illustrated in Fig-\\nures 7, 8, and 9, respectively. In the ICL template,\\nwe composed 3 answerable and 3 unanswerable\\nquestions and provided the corresponding answers\\nmanually.'),\n",
       "  Document(metadata={'page': 2, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}, page_content='Category Description Example Percentage\\nNo scientific\\nconsensusThe answer is still up\\nfor debate, with no consensus\\nin scientific community.“Are we alone in the universe,\\nor will we discover alien\\nlife at some point?”25%\\nImaginationThe question are about people’s\\nimaginations of the future.\"What will the fastest form of\\ntransportation be in 2050?\"15%\\nCompletely\\nsubjectiveThe answer depends on\\npersonal preference.\"Would you rather be shot\\ninto space or explore the\\ndeepest depths of the sea?\"27%\\nToo many\\nvariablesThe question with too\\nmany variables cannot\\nbe answered accurately.“John made 6 dollars mowing lawns\\nand 18 dollars weed eating.\\nIf he only spent 3 or 5 dollar a week,\\nhow long would the money last him?”10%\\nPhilosophicalThe question can yield\\nmultiple responses, but it\\nlacks a definitive answer.“How come god was\\nborn from nothingness?”23%\\nTable 1: Unanswerable questions in the SelfAware dataset that span across multiple categories.\\ncontains a proportion of answerable questions that\\nis twice as large as the volume of unanswerable\\nones. Nevertheless, to ensure the feasibility of test-\\ning, we have purposefully capped the number of\\nanswerable questions.\\n2.1 Dataset Analysis\\nTo gain insight into the reasons precluding a cer-\\ntain answer, we undertook a manual analysis of\\n100 randomly selected unanswerable questions. As\\ntabulated in Table 1, we have broadly segregated\\nthese questions into five distinctive categories. “No\\nScientific Consensus\" encapsulates questions that\\nignite ongoing debates within the scientific com-\\nmunity, such as those concerning the universe’s\\norigin. “Imagination\" includes questions involving\\nspeculative future scenarios, like envisaged events\\nover the next 50 years. “Completely Subjective\"\\ncomprises questions that are inherently personal,\\nwhere answers depend heavily on individual predis-\\npositions. “Too Many Variables\" pertains to mathe-\\nmatical problems that become unsolvable owing to\\nthe overwhelming prevalence of variables. Lastly,\\n“Philosophical\" represents questions of a profound,\\noften metaphysical, nature that resist concrete an-\\nswers. Ideally, upon encountering such questions,\\nthe model should express uncertainty instead of\\ndelivering conclusive responses.\\n3 Evaluation Method\\nThis section elucidates the methodology employed\\nfor assessing self-knowledge in the generated text.In order to achieve this, we define a similarity func-\\ntion,fsim, to compute the similarity, S, between\\na given sentence, t, and a collection of reference\\nsentences, U={u1, u2, . . . , u n}, endowed with\\nuncertain meanings.\\nSi=fsim(t, ui). (1)\\nWhenever any Sisurpasses a pre-determined\\nthreshold T, we perceive the text tas encompass-\\ning uncertain meanings, thereby eliminating the\\nneed for manual evaluation of the response.\\nGiven the substantial disparity in the volume of\\nanswerable and unanswerable questions in Self-\\nAware , we adopt the F1 score as a measure of\\nLLMs’ self-knowledge. Our focus rests on identi-\\nfying unanswerable questions, hence we designate\\nthem as positive cases and categorize answerable\\nquestions as negative cases.\\n4 Experiment\\n4.1 Model\\nWe conduct a sequence of experiments to evaluate\\nthe degree of self-knowledge manifested by various\\nLLMs, including GPT-3 (Brown et al., 2020) and\\nInstructGPT (Ouyang et al., 2022) series, as well\\nas the recent LLaMA (Touvron et al., 2023) and\\nits derivative models, namely Alpaca (Taori et al.,\\n2023) and Vicuna (Chiang et al., 2023). Our in-\\nvestigative approach employed three distinct input\\nforms: Direct, Instruction, and In-Context Learn-\\ning (ICL), which is encapsulated in Appendix A.4.'),\n",
       "  Document(metadata={'page': 5, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}, page_content='Limitations\\n•Generalization of reference sentences. At\\npresent, we have selected sentences with un-\\ncertain meanings exclusively from the GPT-3\\nand InstructGPT series, potentially overlook-\\ning uncertainty present in responses generated\\nby other LLMs. However, it is not feasible\\nto catalog all sentences with uncertain mean-\\nings exhaustively. As a direction for future\\nresearch, we propose to concentrate on the\\nautomated acquisition of more accurate refer-\\nence sentences to address this concern.\\n•Limitations of input forms: Our exami-\\nnation was confined to three unique input\\nforms: direct, instruction, and ICL. There\\nis burgeoning research aimed at bridging the\\ngap between models and human-like meth-\\nods of reasoning and problem-solving, includ-\\ning but not limited to approaches like Re-\\nflexion (Shinn et al., 2023), ToT (Yao et al.,\\n2023), MoT (Li and Qiu, 2023). Future en-\\ndeavors will integrate additional cognitive and\\ndecision-making methods to delve deeper into\\nthe self-knowledge exhibited by these LLMs.\\nEthics Statement\\nThe SelfAware dataset, meticulously curated to\\nevaluate LLMs’ ability to discern unanswerable\\nquestions, is composed of unanswerable questions\\nextracted from sources such as Quora and How-\\nStuffWorks, alongside answerable questions pro-\\ncured from three distinct open datasets. Every ques-\\ntion was thoroughly examined for relevance and\\nharmlessness. To ensure content validity, three an-\\nnotation analysts, compensated at local wage stan-\\ndards, dedicated regular working hours to content\\nreview.\\nThroughout our research process, we under-\\nscored the significance of privacy, data security,\\nand strict compliance with dataset licenses. In\\norder to protect data integrity, we implemented\\nanonymization and content filtration mechanisms.\\nOur adherence to OpenAI’s stipulations remained\\nunyielding for the usage of GPT-3 and InstructGPT\\nmodels, and likewise for Meta’s terms pertaining\\nto LLaMA models. We rigorously vetted the li-\\ncenses of the three publicly available datasets for\\ncompliance, ensuring that all our research method-\\nologies were in alignment with ethical standards at\\nthe institutional, national, and global levels.Adhering to the CC-BY-SA-4.0 protocol, the\\ndataset, once publicly released, will be reserved\\nexclusively for research purposes. We pledge to\\npromptly and effectively address any concerns relat-\\ning to the dataset, while concurrently anticipating\\nresearchers to maintain high ethical standards in\\ntheir utilization of this data.\\nAcknowledgement\\nWe wish to express our gratitude to our colleagues\\nin the FudanNLP group whose insightful sugges-\\ntions, perspectives, and thought-provoking discus-\\nsions significantly contributed to this work. Our\\nsincere appreciation also extends to the anonymous\\nreviewers and area chairs, whose constructive feed-\\nback was instrumental in refining the quality of\\nour study. This work was supported by the Na-\\ntional Natural Science Foundation of China (No.\\n62236004 and No. 62022027) and CAAI-Huawei\\nMindSpore Open Fund.\\nReferences\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\\nChen, Eric Chu, Jonathan H. Clark, Laurent El\\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\\nJan Botha, James Bradbury, Siddhartha Brahma,\\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\\nCherry, Christopher A. Choquette-Choo, Aakanksha\\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-'),\n",
       "  Document(metadata={'page': 4, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}, page_content='LLaMA-7B Alpaca-7B Vicuna-7BLLaMA-13B Alpaca-13B Vicuna-13B LLaMA-30B LLaMA-65B\\nModels01020304050F1 Scores28.5735.8742.78\\n30.1237.4447.84\\n30.346.89Figure 5: Experimental results obtained from LLaMA\\nand its derived models, Alpaca and Vicuna in instruction\\ninput form.\\nInstruction Tuning. Figure 2 delineates that\\nmodels from the InstructGPT series exhibit a su-\\nperior level of self-knowledge compared to their\\nGPT-3 counterparts. Further evidence of model\\nenhancement is provided by Figure 4, where text-\\ndavinci models show significant improvement rela-\\ntive to the base davinci model. An additional com-\\nparative analysis, presented in Figure 5, evaluates\\nLLaMA against its derivative models. The results\\nunderscore a notable increase in self-knowledge\\nfor Alpaca and Vicuna upon instruction tuning, ex-\\nceeding their base model performances. Among\\nthese, Vicuna-13B outperforms the LLaMA-65B,\\ncorroborating the efficacy of instruction tuning for\\nenhancing model self-knowledge.\\nInput Forms. As shown in Figure 2, the incorpo-\\nration of instructions and examples serves to boost\\nthe self-knowledge of both the GPT-3 and Instruct-\\nGPT series. Specifically, ICL input form, providing\\nricher contextual information, contributes to a sig-\\nnificant enhancement in models’ self-knowledge.\\nThis impact is particularly noticeable in the davinci\\nmodel, where ICL facilitates a 27.96% improve-\\nment over the direct. Moreover, a comparison be-\\ntween Figure 3 and Figure 4 reveals that the in-\\nclusion of instructions and examples successfully\\nminimizes the performance disparity between the\\ndavinci and text-davinci models, suggesting an ac-\\nquisition of self-knowledge from the instructions\\nand provided examples.\\nCompared with Human. Figure 3 reveals that,\\nwithout supplementary samples, GPT-4 currently\\nperforms best among the tested models, achieving\\nan impressive F1 score of 75.47%. However, a no-\\nticeable gap becomes evident when comparing this\\ntext-ada-001\\ntext-babbage-001text-curie-001text-davinci-001 text-davinci-002 text-davinci-003\\ngpt-3.5-turbo-0301gpt-4-0314\\nModels0510152025303540Accuracy\\n2.484.45 4.710.6115.730.2538.2942.64Figure 6: Accuracy of the InstructGPT series when\\nresponding to answerable questions in instruction input\\nform.\\nperformance to the human benchmark of 84.93%.\\nThis underscores the considerable potential that re-\\nmains for enhancing the self-knowledge level of\\nLLMs.\\nAnswerable Questions. Figure 6 traces the per-\\nformance evolution of the InstructGPT series in\\naddressing answerable questions, adhering to the\\nclosed-book question answering paradigm (Tou-\\nvron et al., 2023), where output accuracy is con-\\ntingent on the presence of the correct answer. Our\\nobservations underscore a steady enhancement in\\nQA task accuracy corresponding to an increase\\nin model parameter size and continuous learning.\\nParticularly, the accuracy of text-davinci-001 expe-\\nriences a significant ascent, scaling from a meager\\n2.48% in text-ada-001 to 10.61%, whereas GPT-4\\nmarks an even more striking jump to 42.64%.\\n5 Conclusion\\nThis study investigates the self-knowledge of\\nLLMs by evaluating their ability to identify unan-\\nswerable questions. Through the introduction of a\\nnovel dataset and an automated method for detect-\\ning uncertainty in the models’ responses, we are\\nable to accurately measure the self-knowledge of\\nLLMs such as GPT-3, InstructGPT and LLaMA.\\nOur results reveal that while these models possess\\na certain degree of self-knowledge, there is still\\nan apparent disparity in comparison to human self-\\nknowledge. This highlights the need for further\\nresearch in this area to enhance the ability of LLMs\\nto understand their own limitations on the unknows.\\nSuch efforts will lead to more accurate and reliable\\nresponses from LLMs, which will have a positive\\nimpact on their applications in diverse fields.')]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Objective**: Assess the self-knowledge of language models (LLMs) by evaluating their ability to identify unanswerable questions.\n",
       "- **Dataset**: Created the SelfAware dataset, which includes unanswerable questions from sources like Quora and HowStuffWorks, as well as answerable questions from open datasets.\n",
       "- **Categories of Unanswerable Questions**:\n",
       "  - No scientific consensus\n",
       "  - Imagination (speculative future scenarios)\n",
       "  - Completely subjective (personal preference)\n",
       "  - Too many variables\n",
       "  - Philosophical (profound, metaphysical nature)\n",
       "- **Reference Sentences**: Selected 16 sentences with uncertain meanings from GPT-3 and InstructGPT for similarity comparison.\n",
       "- **Similarity Calculation**: Used SimCSE to compute similarity between generated sentences and reference sentences.\n",
       "- **Threshold Ablation**: Determined that a similarity threshold of 0.75 produced the highest F1 score for identifying uncertain sentences.\n",
       "- **Input Forms**: Evaluated three input forms: Direct, Instruction, and In-Context Learning (ICL).\n",
       "- **Models Tested**: Included GPT-3, InstructGPT, LLaMA, Alpaca, and Vicuna.\n",
       "- **Findings**:\n",
       "  - Instruction tuning significantly enhances model self-knowledge.\n",
       "  - ICL input form improves self-knowledge by providing richer contextual information.\n",
       "  - GPT-4 outperforms other models but still lags behind human performance.\n",
       "- **Human Benchmark**: Volunteers exhibited strong self-knowledge with high F1 scores.\n",
       "- **Limitations**:\n",
       "  - Generalization of reference sentences limited to GPT-3 and InstructGPT.\n",
       "  - Confinement to three input forms.\n",
       "- **Future Directions**: Propose automated acquisition of more accurate reference sentences and integration of additional cognitive and decision-making methods.\n",
       "- **Ethical Considerations**: Ensured relevance, harmlessness, privacy, data security, and compliance with dataset licenses.\n",
       "- **Conclusion**: While LLMs possess some self-knowledge, there's a significant gap compared to humans, indicating the need for further research to enhance LLMs' understanding of their limitations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_answer = output[\"result\"]\n",
    "\n",
    "Markdown(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_summary = \"What is the main discovery of the paper regarding self-knowledge in LLMs?\"\n",
    "\n",
    "output = pdf_qa.invoke({\"query\": query_summary, \"chat_history\": []}) # adding chat history so the model remembers previous questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the main discovery of the paper regarding self-knowledge in LLMs?',\n",
       " 'chat_history': [],\n",
       " 'result': 'The main discovery of the paper regarding self-knowledge in Large Language Models (LLMs) is that these models inherently possess an intrinsic capacity for self-knowledge. This capacity is demonstrated through their ability to identify unanswerable or unknowable questions to some extent. The paper also finds that techniques such as in-context learning and instruction tuning can further enhance the self-knowledge of LLMs. Despite these promising insights, the study highlights a considerable gap between the capabilities of LLMs and human proficiency in recognizing the limits of their knowledge, with the state-of-the-art model, GPT-4, exhibiting self-knowledge at 75.47% compared to human self-knowledge rated at 84.93%.',\n",
       " 'source_documents': [Document(page_content='Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢Qiushi Sun♠Qipeng Guo♢\\nJiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware , consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗Corresponding author.\\nUnknowsKnows UnknowsKnows\\nKnown KnowsKnown Unknows\\nUnknown Unknows Unknown KnowsUnlockFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know ,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increasearXiv:2305.18153v2  [cs.CL]  30 May 2023', metadata={'page': 0, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}),\n",
       "  Document(page_content='Do Large Language Models Know What They Don’t Know?\\nZhangyue Yin♢Qiushi Sun♠Qipeng Guo♢\\nJiawen Wu♢Xipeng Qiu♢∗Xuanjing Huang♢\\n♢School of Computer Science, Fudan University\\n♠Department of Mathematics, National University of Singapore\\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\\nAbstract\\nLarge language models (LLMs) have a wealth\\nof knowledge that allows them to excel in vari-\\nous Natural Language Processing (NLP) tasks.\\nCurrent research focuses on enhancing their\\nperformance within their existing knowledge.\\nDespite their vast knowledge, LLMs are still\\nlimited by the amount of information they can\\naccommodate and comprehend. Therefore, the\\nability to understand their own limitations on\\nthe unknows, referred to as self-knowledge,\\nis of paramount importance. This study aims\\nto evaluate LLMs’ self-knowledge by assess-\\ning their ability to identify unanswerable or\\nunknowable questions. We introduce an auto-\\nmated methodology to detect uncertainty in the\\nresponses of these models, providing a novel\\nmeasure of their self-knowledge. We further in-\\ntroduce a unique dataset, SelfAware , consisting\\nof unanswerable questions from five diverse cat-\\negories and their answerable counterparts. Our\\nextensive analysis, involving 20 LLMs includ-\\ning GPT-3, InstructGPT, and LLaMA, discov-\\nering an intrinsic capacity for self-knowledge\\nwithin these models. Moreover, we demon-\\nstrate that in-context learning and instruction\\ntuning can further enhance this self-knowledge.\\nDespite this promising insight, our findings also\\nhighlight a considerable gap between the capa-\\nbilities of these models and human proficiency\\nin recognizing the limits of their knowledge.\\n“True wisdom is knowing what you don’t know.”\\n–Confucius\\n1 Introduction\\nRecently, Large Language Models (LLMs) such\\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\\n2023), and LLaMA (Touvron et al., 2023) have\\nshown exceptional performance on a wide range\\nof NLP tasks, including common sense reason-\\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\\n∗Corresponding author.\\nUnknowsKnows UnknowsKnows\\nKnown KnowsKnown Unknows\\nUnknown Unknows Unknown KnowsUnlockFigure 1: Know-Unknow Quadrant. The horizontal axis\\nrepresents the model’s memory capacity for knowledge,\\nand the vertical axis represents the model’s ability to\\ncomprehend and utilize knowledge.\\nmatical problem-solving (Lewkowycz et al., 2022;\\nChen et al., 2022). Despite their ability to learn\\nfrom huge amounts of data, LLMs still have lim-\\nitations in their capacity to retain and understand\\ninformation. To ensure responsible usage, it is cru-\\ncial for LLMs to have the capability of recognizing\\ntheir limitations and conveying uncertainty when\\nresponding to unanswerable or unknowable ques-\\ntions. This acknowledgment of limitations, also\\nknown as “ knowing what you don’t know ,” is a\\ncrucial aspect in determining their practical appli-\\ncability. In this work, we refer to this ability as\\nmodel self-knowledge.\\nThe Know-Unknow quadrant in Figure 1 il-\\nlustrates the relationship between the model’s\\nknowledge and comprehension. The ratio of\\n“Known Knows” to “Unknown Knows” demon-\\nstrates the model’s proficiency in understanding\\nand applying existing knowledge. Techniques\\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\\nConsistency (Wang et al., 2022), and Complex\\nCoT (Fu et al., 2022) can be utilized to increasearXiv:2305.18153v2  [cs.CL]  30 May 2023', metadata={'page': 0, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}),\n",
       "  Document(page_content='this ratio, resulting in improved performance on\\nNLP tasks. We focus on the ratio of “Known Un-\\nknows” to “Unknown Unknows”, which indicates\\nthe model’s self-knowledge level, specifically un-\\nderstanding its own limitations and deficiencies in\\nthe unknows.\\nExisting datasets such as SQuAD2.0 (Rajpurkar\\net al., 2018) and NewsQA (Trischler et al., 2017),\\nwidely used in question answering (QA), have been\\nutilized to test the self-knowledge of models with\\nunanswerable questions. However, these questions\\nare context-specific and could become answerable\\nwhen supplemented with additional information.\\nSrivastava et al. (2022) attempted to address this by\\nevaluating LLMs’ competence in delineating their\\nknowledge boundaries, employing a set of 23 pairs\\nof answerable and unanswerable multiple-choice\\nquestions. They discovered that these models’ per-\\nformance barely surpassed that of random guessing.\\nKadavath et al. (2022) suggested probing the self-\\nknowledge of LLMs through the implementation\\nof a distinct \"Value Head\". Yet, this approach may\\nencounter difficulties when applied across varied\\ndomains or tasks due to task-specific training. Con-\\nsequently, we redirect our focus to the inherent\\nabilities of LLMs, and pose the pivotal question:\\n“Do large language models know what they don’t\\nknow? ”.\\nIn this study, we investigate the self-knowledge\\nof LLMs using a novel approach. By gathering\\nreference sentences with uncertain meanings, we\\ncan determine whether the model’s responses re-\\nflect uncertainty using a text similarity algorithm.\\nWe quantified the model’s self-knowledge using\\nthe F1 score. To address the small and idiosyn-\\ncratic limitations of existing datasets, we created\\na new dataset called SelfAware . This dataset com-\\nprises 1,032 unanswerable questions, which are dis-\\ntributed across five distinct categories, along with\\nan additional 2,337 questions that are classified as\\nanswerable. Experimental results on GPT-3, In-\\nstructGPT, LLaMA, and other LLMs demonstrate\\nthat in-context learning and instruction tuning can\\neffectively enhance the self-knowledge of LLMs.\\nHowever, the self-knowledge exhibited by the cur-\\nrent state-of-the-art model, GPT-4, measures at\\n75.47%, signifying a notable disparity when con-\\ntrasted with human self-knowledge, which is rated\\nat 84.93%.\\nOur key contributions to this field are summa-\\nrized as follows:•We have developed a new dataset, SelfAware ,\\nthat comprises a diverse range of commonly\\nposed unanswerable questions.\\n•We propose an innovative evaluation tech-\\nnique based on text similarity to quantify the\\ndegree of uncertainty inherent in model out-\\nputs.\\n•Through our detailed analysis of 20 LLMs,\\nbenchmarked against human self-knowledge,\\nwe identified a significant disparity between\\nthe most advanced LLMs and humans1.\\n2 Dataset Construction\\nTo conduct a more comprehensive evaluation of\\nthe model’s self-knowledge, we constructed a\\ndataset that includes a larger number and more di-\\nverse types of unanswerable questions than Know-\\nUnknowns dataset (Srivastava et al., 2022). To\\nfacilitate this, we collected a corpus of 2,858 unan-\\nswerable questions, sourced from online platforms\\nlike Quora and HowStuffWorks. These questions\\nwere meticulously evaluated by three seasoned an-\\nnotation analysts, each operating independently.\\nThe analysts were permitted to leverage external\\nresources, such as search engines. To ensure the va-\\nlidity of our dataset, we retained only the questions\\nthat all three analysts concurred were unanswerable.\\nThis rigorous process yielded a finalized collection\\nof 1,032 unanswerable questions.\\nIn pursuit of a comprehensive evaluation, we\\nopted for answerable questions drawn from three\\ndatasets: SQuAD (Rajpurkar et al., 2016), Hot-\\npotQA (Yang et al., 2018), and TriviaQA (Joshi\\net al., 2017). Our selection was guided by Sim-\\nCSE (Gao et al., 2021), which allowed us to iden-\\ntify and select the answerable questions semanti-\\ncally closest to the unanswerable ones. From these', metadata={'page': 1, 'source': './assets-resources/llm_paper_know_dont_know.pdf'}),\n",
       "  Document(page_content='this ratio, resulting in improved performance on\\nNLP tasks. We focus on the ratio of “Known Un-\\nknows” to “Unknown Unknows”, which indicates\\nthe model’s self-knowledge level, specifically un-\\nderstanding its own limitations and deficiencies in\\nthe unknows.\\nExisting datasets such as SQuAD2.0 (Rajpurkar\\net al., 2018) and NewsQA (Trischler et al., 2017),\\nwidely used in question answering (QA), have been\\nutilized to test the self-knowledge of models with\\nunanswerable questions. However, these questions\\nare context-specific and could become answerable\\nwhen supplemented with additional information.\\nSrivastava et al. (2022) attempted to address this by\\nevaluating LLMs’ competence in delineating their\\nknowledge boundaries, employing a set of 23 pairs\\nof answerable and unanswerable multiple-choice\\nquestions. They discovered that these models’ per-\\nformance barely surpassed that of random guessing.\\nKadavath et al. (2022) suggested probing the self-\\nknowledge of LLMs through the implementation\\nof a distinct \"Value Head\". Yet, this approach may\\nencounter difficulties when applied across varied\\ndomains or tasks due to task-specific training. Con-\\nsequently, we redirect our focus to the inherent\\nabilities of LLMs, and pose the pivotal question:\\n“Do large language models know what they don’t\\nknow? ”.\\nIn this study, we investigate the self-knowledge\\nof LLMs using a novel approach. By gathering\\nreference sentences with uncertain meanings, we\\ncan determine whether the model’s responses re-\\nflect uncertainty using a text similarity algorithm.\\nWe quantified the model’s self-knowledge using\\nthe F1 score. To address the small and idiosyn-\\ncratic limitations of existing datasets, we created\\na new dataset called SelfAware . This dataset com-\\nprises 1,032 unanswerable questions, which are dis-\\ntributed across five distinct categories, along with\\nan additional 2,337 questions that are classified as\\nanswerable. Experimental results on GPT-3, In-\\nstructGPT, LLaMA, and other LLMs demonstrate\\nthat in-context learning and instruction tuning can\\neffectively enhance the self-knowledge of LLMs.\\nHowever, the self-knowledge exhibited by the cur-\\nrent state-of-the-art model, GPT-4, measures at\\n75.47%, signifying a notable disparity when con-\\ntrasted with human self-knowledge, which is rated\\nat 84.93%.\\nOur key contributions to this field are summa-\\nrized as follows:•We have developed a new dataset, SelfAware ,\\nthat comprises a diverse range of commonly\\nposed unanswerable questions.\\n•We propose an innovative evaluation tech-\\nnique based on text similarity to quantify the\\ndegree of uncertainty inherent in model out-\\nputs.\\n•Through our detailed analysis of 20 LLMs,\\nbenchmarked against human self-knowledge,\\nwe identified a significant disparity between\\nthe most advanced LLMs and humans1.\\n2 Dataset Construction\\nTo conduct a more comprehensive evaluation of\\nthe model’s self-knowledge, we constructed a\\ndataset that includes a larger number and more di-\\nverse types of unanswerable questions than Know-\\nUnknowns dataset (Srivastava et al., 2022). To\\nfacilitate this, we collected a corpus of 2,858 unan-\\nswerable questions, sourced from online platforms\\nlike Quora and HowStuffWorks. These questions\\nwere meticulously evaluated by three seasoned an-\\nnotation analysts, each operating independently.\\nThe analysts were permitted to leverage external\\nresources, such as search engines. To ensure the va-\\nlidity of our dataset, we retained only the questions\\nthat all three analysts concurred were unanswerable.\\nThis rigorous process yielded a finalized collection\\nof 1,032 unanswerable questions.\\nIn pursuit of a comprehensive evaluation, we\\nopted for answerable questions drawn from three\\ndatasets: SQuAD (Rajpurkar et al., 2016), Hot-\\npotQA (Yang et al., 2018), and TriviaQA (Joshi\\net al., 2017). Our selection was guided by Sim-\\nCSE (Gao et al., 2021), which allowed us to iden-\\ntify and select the answerable questions semanti-\\ncally closest to the unanswerable ones. From these', metadata={'page': 1, 'source': './assets-resources/llm_paper_know_dont_know.pdf'})]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Long context issue in LLMs](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "\n",
    "Long Context. One of the main drawbacks of Transformerbased language models is the context length is limited due to the involved quadratic computational costs in both time\n",
    "and memory. Meanwhile, there is an increasing demand\n",
    "for LLM applications with long context windows, such as\n",
    "in PDF processing and story writing [217]. ChatGPT has\n",
    "recently released an updated variant with a context window\n",
    "size of up to 16K tokens, which is much longer than the\n",
    "initial one, i.e., 4K tokens. Additionally, GPT-4 was launched\n",
    "with variants with context window of 32K tokens [46]. Next,\n",
    "we discuss two important factors that support long context\n",
    "modeling for LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb \n",
    "Below are notebook from openai cookbook on these topics of search and embeddings:\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Code_search.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb\n",
    "- https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "- [In-context learning abilities of ChatGPT models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "- [Issue with long context](https://arxiv.org/pdf/2303.18223.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-chatgpt-apps",
   "language": "python",
   "name": "oreilly-chatgpt-apps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
