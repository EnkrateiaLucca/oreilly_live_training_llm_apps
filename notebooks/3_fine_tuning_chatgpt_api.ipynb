{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tunning ChatGPT to Make Precise Youtube Chapters Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the documentation on the [OPENAI website](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset) to learn how to fine tune a chatgpt model to a specific use case using a custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the basics, we need an example format, which should be as such:\n",
    "\n",
    "```\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to fine tune a ChatGPT model we'll need to create a dataset organized in the format shown above. Essentially we'll have a .json file with several (as many as the number of inputs for the fine tunning) dictionaries that have this format of:\n",
    "\n",
    "```\n",
    "{\"<messages key>\"}: [{\"role\": \"system\", \"<content key>\": \"<some text content>\"},\n",
    "{\"<messages key>\"}: {\"role\": \"user\", \"<content key>\": \"<some text content>\"},\n",
    "{\"<messages key>\"}: {\"role\": \"assistant\", \"<content key>\": \"<some text content>\"}]\n",
    "```\n",
    "\n",
    "Here we'll assume that we will only fine tune actual ChatGPT, so we won't cover the formats for models like 'babbage-002' and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, given this format, let's create our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quote from the actual documentation:\n",
    "\n",
    "\n",
    "> To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples with gpt-3.5-turbo but the right number varies greatly based on the exact use case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get at least 30 examples for our custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in regular fine tunning with any Machine Learning model, its advisable to get stablish a train-test split so that you are on top of your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, its extremely important to be aware of token limits and token costs, given that this will be a paid API.\n",
    "\n",
    "Each training example will be limited to the context length of ChatGPT (therefore 4096 tokens), so its good to set some good practices in place to avoid issues with large inputs, the [OpenAI documentation](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset#:~:text=Each%20training%20example,the%20OpenAI%20cookbook.) recommends you check that the total token count in the message contents are under 4000 as a good rule of thumb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create this dataset programatically using GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def get_response(prompt):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(model=\"gpt-3.5-turbo-1106\", \n",
    "                             messages=\n",
    "                             [\n",
    "                                 {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                                 {\"role\": \"user\", \"content\": prompt}   \n",
    "                             ],\n",
    "                             temperature=0.0,\n",
    "                             n = 1\n",
    "                             )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def get_num_tokens(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Calculates the number of tokens in a text prompt\"\"\"    \n",
    "\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    return len(enc.encode(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, let manually inspect the flashcards generated with gpt4 to select the best ones for our custom training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"Create 15 examples of 1 paragraph first person human-like descriptions of tasks and goals for the upcoming weeks. Each example should be in a bullet point.\"\n",
    "\n",
    "task_paragraphs = get_response(prompt)\n",
    "\n",
    "display(Markdown(task_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list_prompts = task_paragraphs.split(\"\\n\")\n",
    "\n",
    "task_list_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in task_list_prompts:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_steps = []\n",
    "for t in task_list_prompts:\n",
    "    prompt = f\"Given this paragraph description of tasks and goals: \\n '''{t}''' \\n create a bullet point list with all the necessary steps to accomplish all of them.\"\n",
    "    task_steps.append(get_response(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the dataset generated programatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in task_steps:\n",
    "    display(Markdown(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for desc,steps in zip(task_list_prompts, task_steps):\n",
    "    print(desc) \n",
    "    print(steps)\n",
    "    print(\"************\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_indices = [i for i, x in enumerate(task_list_prompts) if not x]\n",
    "print(empty_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the indexes to remove\n",
    "indexes_to_remove = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27]\n",
    "\n",
    "# Remove the elements with the given indexes from task_list_prompts and task_steps\n",
    "task_list_prompts = [t for i, t in enumerate(task_list_prompts) if i not in indexes_to_remove]\n",
    "task_steps = [t for i, t in enumerate(task_steps) if i not in indexes_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_to_gpt4 = \"\"\"Consider this format for a dataset for the chatgpt fine tuning api:\n",
    "# '''\n",
    "# {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "# {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
    "# {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n",
    "# '''\n",
    "\n",
    "# Write the python code to create a dataset like this one programatically given that the system prompt will always be: \"You are a helpful planning assistant\"\n",
    "# and that the user prompts will be inside a list called 'task_list_prompts' and the assistant content will be inside a list called 'task_steps'.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Ensure the lists have the same length\n",
    "assert len(task_list_prompts) == len(task_steps), \"Mismatched lengths between prompts and responses\"\n",
    "\n",
    "# Creating the dataset\n",
    "dataset = []\n",
    "system_prompt = \"You are a helpful planning assistant\"\n",
    "\n",
    "for user_content, assistant_content in zip(task_list_prompts, task_steps):\n",
    "    interaction = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "    }\n",
    "    dataset.append(interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to a file\n",
    "with open('dataset.jsonl', 'w') as f:\n",
    "    for entry in dataset:\n",
    "        f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "# Now 'dataset.jsonl' will contain the dataset in the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://cookbook.openai.com/examples/chat_finetuning_data_prep\n",
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./dataset.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now lets estimate some token counts and costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be checkinf for this validation checklist as described in the [tutorial suggested by the OpenAI docs](https://cookbook.openai.com/examples/chat_finetuning_data_prep#:~:text=Format%20validation,for%20easier%20debugging.).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helpful utilities\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data warnings and token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output obtained, all of our examples are under the context length, which means they won't have to be truncated which is great news! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost estimation\n",
    "\n",
    "Let's estimate the cost of this fine tunning based on the number of tokens in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, as we can see it seems like this fine tunning is not going to cost that much, given that if I check on the [pricing page](https://openai.com/pricing) for the cost associated with this amount of tokens:\n",
    "\n",
    "![](2023-10-16-17-22-32.png)\n",
    "\n",
    "\n",
    "Let's write a little function to calculate these costs automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_for_fine_tunning(token_count):\n",
    "    return (0.008*token_count)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_cost_for_fine_tunning(38478)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so it seems that the amount will be 0.30 cents which seems reasonable, although we should take into account the fact that we'll be charged more for using this fine tuned model as well, and we only used 10 examples for this use case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets upload our newly generated dataset file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that part, lets write a simply Python script to upload the file, we'll use this snippet from the [OpenAI API docs](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset#:~:text=import%20os%20import%20openai%20openai.api_key%20%3D%20os.getenv(%22openai_api_key%22)%20openai.file.create(%20file%3Dopen(%22mydata.jsonl%22%2C%20%22rb%22)%2C%20purpose%3D'fine-tune'%20))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "\n",
    "# openai.File.create(\n",
    "#   file=open(\"./dataset.jsonl\", \"rb\"),\n",
    "#   purpose='fine-tune'\n",
    "# )\n",
    "\n",
    "from openai import OpenAI\n",
    "# OpenAI API key should be set as \n",
    "# environment variable - OPENAI_API_KEY\n",
    "client = OpenAI()\n",
    "client.files.create(\n",
    "  file=open(\"dataset.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-id\", \n",
    "  model=\"gpt-3.5-turbo-1106\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_files = openai.File.list()\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.files.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, we create our fine-tuned model by running this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.FineTuningJob.create(training_file=\"file-ICGS6dyaltzMJuXdnHcpjGTr\", model=\"gpt-3.5-turbo\")\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-abc123\", \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "# Example response object\n",
    "# {\n",
    "#   \"object\": \"fine_tuning.job\",\n",
    "#   \"id\": \"ftjob-abc123\",\n",
    "#   \"model\": \"gpt-3.5-turbo-0613\",\n",
    "#   \"created_at\": 1614807352,\n",
    "#   \"fine_tuned_model\": null,\n",
    "#   \"organization_id\": \"org-123\",\n",
    "#   \"result_files\": [],\n",
    "#   \"status\": \"queued\",\n",
    "#   \"validation_file\": null,\n",
    "#   \"training_file\": \"file-abc123\",\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the `training_file` parameter correspond to the file id generated when running the previous snippet. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wait for an email confirmation that will let us know when the training is done. Which given the size of this job, should be pretty quckly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat thing is that you can promatically query the status of your jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.fine_tuning.jobs.list()\n",
    "\n",
    "# Example response object\n",
    "# {\n",
    "#   \"object\": \"list\",\n",
    "#   \"data\": [\n",
    "#     {\n",
    "#       \"object\": \"fine_tuning.job.event\",\n",
    "#       \"id\": \"ft-event-TjX0lMfOniCZX64t9PUQT5hn\",\n",
    "#       \"created_at\": 1689813489,\n",
    "#       \"level\": \"warn\",\n",
    "#       \"message\": \"Fine tuning process stopping due to job cancellation\",\n",
    "#       \"data\": null,\n",
    "#       \"type\": \"message\"\n",
    "#     },\n",
    "#     { ... },\n",
    "#     { ... }\n",
    "#   ], \"has_more\": true\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve fine tuning jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.fine_tuning.jobs.retrieve(\"ftjob-abc123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List 10 fine-tuning jobs\n",
    "# openai.FineTuningJob.list(limit=10)\n",
    "\n",
    "# # Retrieve the state of a fine-tune\n",
    "# openai.FineTuningJob.retrieve(\"ftjob-rPPzK29a1UkXFSHTdEQX4heY\")\n",
    "\n",
    "# Cancel a job\n",
    "#openai.FineTuningJob.cancel(\"file-kXlvspUQRKFUWcrW5pgzy0PW\")\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "#openai.FineTuningJob.list_events(id=\"file-kXlvspUQRKFUWcrW5pgzy0PW\", limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancel fine tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.fine_tuning.jobs.cancel(\"ftjob-abc123\")\n",
    "\n",
    "# Example response object\n",
    "# {\n",
    "#   \"object\": \"fine_tuning.job\",\n",
    "#   \"id\": \"ftjob-abc123\",\n",
    "#   \"model\": \"gpt-3.5-turbo-0613\",\n",
    "#   \"created_at\": 1689376978,\n",
    "#   \"fine_tuned_model\": null,\n",
    "#   \"organization_id\": \"org-123\",\n",
    "#   \"result_files\": [],\n",
    "#   \"hyperparameters\": {\n",
    "#     \"n_epochs\":  \"auto\"\n",
    "#   },\n",
    "#   \"status\": \"cancelled\",\n",
    "#   \"validation_file\": \"file-abc123\",\n",
    "#   \"training_file\": \"file-abc123\"\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go to the platform to check out our fine tuned model or run inference from it using the API! Let's look at how to run our fine tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN the platform we see:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-10-20-00-02-25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can choose that model and see how it performs for a different new video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying the model:\n",
    "![](2023-10-20-00-05-50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response:\n",
    "\n",
    "![](2023-10-16-17-49-24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks pretty good! :) Let's compare with the output of regular ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-10-16-17-50-32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine tuned model is definitely more thourough than the regular ChatGPT model so I would call this a nice success! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run inference with our fine tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, fine_tuned_model_id=\"ft:gpt-3.5-turbo-0613:personal::8BWIRV1U\"):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(model=fine_tuned_model_id, \n",
    "                             messages=\n",
    "                             [\n",
    "                                 {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                                 {\"role\": \"user\", \"content\": prompt}   \n",
    "                             ],\n",
    "                             temperature=0.0,\n",
    "                             n = 1\n",
    "                             )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "get_response(\"Tomorrow I have to practice for my presentation at the live-training for O'Reilly at least a couple more times, then run through all the slides and notebooks to check everything is in order. After that I have Jiu Jitsu training and one more rehearsal before the live-training at 18:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.Model.delete(\"ft:gpt-3.5-turbo-0613:personal::8BWIRV1U\")\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.files.delete(\"file-abc123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly_env",
   "language": "python",
   "name": "oreilly_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
