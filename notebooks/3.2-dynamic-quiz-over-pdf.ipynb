{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install langchain-openai\n",
    "# !pip install pypdf\n",
    "# !pip install chromadb\n",
    "# !pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# # Set OPENAI API Key\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your openai key\"\n",
    "\n",
    "# OR (load from .env file)\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# make sure you have python-dotenv installed\n",
    "# load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a study workflow using Jupyter Notebooks, LLMs, and langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x15343d890>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1536dbe50>, model_name='gpt-4-1106-preview', openai_api_key=SecretStr('**********'), openai_api_base='https://api.openai.com/v1', openai_proxy='')), document_prompt=PromptTemplate(input_variables=['page_content'], template='Context:\\n{page_content}'), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1529a9a50>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = \"./assets-resources/attention-paper.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path) # LOAD\n",
    "pdf_docs = loader.load_and_split() # SPLIT\n",
    "embeddings = OpenAIEmbeddings() # EMBED\n",
    "vectordb = Chroma.from_documents(pdf_docs, embedding=embeddings) # STORE\n",
    "retriever = vectordb.as_retriever()\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "pdf_qa = RetrievalQA.from_llm(llm=llm, retriever=retriever) # RETRIEVE\n",
    "pdf_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the key components of the transformer architecture?\"\n",
    "result = pdf_qa.invoke({\"query\": query, \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer architecture, as described in the provided context, consists of several key components:\n",
      "\n",
      "1. **Encoder and Decoder Stacks**: Both the encoder and decoder are composed of a stack of six identical layers. \n",
      "\n",
      "   - **Encoder**: Each encoder layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a position-wise fully connected feed-forward network. Each of these sub-layers is followed by a residual connection and layer normalization. \n",
      "\n",
      "   - **Decoder**: Each decoder layer also has two sub-layers similar to the encoder, plus a third sub-layer that performs multi-head attention over the encoder's output. The self-attention mechanism in the decoder is modified to prevent positions from attending to subsequent positions (also known as masked self-attention).\n",
      "\n",
      "2. **Attention**: The attention function in the Transformer is a mapping of a query and a set of key-value pairs to an output, with all components being vectors. The output is computed as a weighted sum. This mechanism allows the model to focus on different parts of the input sequence when producing the output, facilitating the modeling of dependencies without regard to their distance in the sequence.\n",
      "\n",
      "3. **Multi-Head Attention**: This is a technique where the attention mechanism is run in parallel multiple times with different, learned linear projections of the queries, keys, and values. This allows the model to capture information from different representation subspaces at different positions.\n",
      "\n",
      "4. **Position-wise Feed-Forward Networks**: In addition to attention sub-layers, each layer in the encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.\n",
      "\n",
      "5. **Residual Connections**: These are used around each of the sub-layers (self-attention and feed-forward networks) in both the encoder and decoder. They help in avoiding the vanishing gradient problem by allowing gradients to flow through the networks directly.\n",
      "\n",
      "6. **Layer Normalization**: This component is applied after each residual connection, helping in stabilizing the learning process.\n",
      "\n",
      "7. **Positional Encoding**: Since the Transformer does not use recurrence or convolution, positional encodings are added to the input embeddings to give the model information about the position of the tokens in the sequence. The Transformer uses sinusoidal functions to encode the positions, although learned positional embeddings are also an option as they have shown nearly identical results.\n",
      "\n",
      "These components work together to enable the Transformer to handle sequence transduction tasks such as translation and parsing effectively, with the advantage of allowing for more parallelization in computation compared to recurrent or convolutional models.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  How does the self-attention mechanism in transformers differ from traditional sequence alignment methods?\n",
      "ANSWER The self-attention mechanism in transformers differs from traditional sequence alignment methods in the following ways:\n",
      "\n",
      "1. Global Dependency Modeling: Self-attention allows the model to directly compute dependencies between any two positions in the sequence, regardless of their distance. Traditional sequence alignment methods like those in RNNs and CNNs process the sequence step-by-step or in local receptive fields, which can make it harder to capture long-range dependencies.\n",
      "\n",
      "2. Parallelization: Self-attention mechanisms enable parallel computation across all positions in a sequence because they do not require sequential processing. This is in contrast to RNNs, which process elements sequentially and therefore cannot be parallelized across the steps of a sequence.\n",
      "\n",
      "3. Fixed Number of Operations: The Transformer reduces the number of operations required to relate two arbitrary positions in a sequence to a constant, while in RNNs and CNNs, this number grows with the distance between positions (linearly for CNNs like ConvS2S, and potentially unbounded for RNNs).\n",
      "\n",
      "4. Flexibility in Modeling Relationships: Self-attention can flexibly weigh the influence of different positions, whereas traditional models might have fixed patterns of interaction defined by their structure (e.g., convolutional filters or recurrent state transitions).\n",
      "\n",
      "5. Absence of Recurrent or Convolutional Layers: The Transformer model architecture, which uses self-attention, eschews recurrence and convolutions entirely, whereas traditional sequence alignment methods typically rely on these mechanisms to process sequences.\n",
      "\n",
      "6. Positional Encoding: Since self-attention does not inherently consider the order of the sequence, the Transformer uses positional encodings to inject information about the position of the elements in the sequence. Traditional methods like RNNs naturally take the order of elements into account due to their sequential processing.\n",
      "\n",
      "Overall, self-attention mechanisms represent a departure from the incremental processing inherent in traditional sequence alignment methods and provide a more direct way to model relationships in data, which can be especially beneficial for tasks like machine translation and language modeling.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The self-attention mechanism in transformers differs from traditional sequence alignment methods in the following ways:\\n\\n1. Global Dependency Modeling: Self-attention allows the model to directly compute dependencies between any two positions in the sequence, regardless of their distance. Traditional sequence alignment methods like those in RNNs and CNNs process the sequence step-by-step or in local receptive fields, which can make it harder to capture long-range dependencies.\\n\\n2. Parallelization: Self-attention mechanisms enable parallel computation across all positions in a sequence because they do not require sequential processing. This is in contrast to RNNs, which process elements sequentially and therefore cannot be parallelized across the steps of a sequence.\\n\\n3. Fixed Number of Operations: The Transformer reduces the number of operations required to relate two arbitrary positions in a sequence to a constant, while in RNNs and CNNs, this number grows with the distance between positions (linearly for CNNs like ConvS2S, and potentially unbounded for RNNs).\\n\\n4. Flexibility in Modeling Relationships: Self-attention can flexibly weigh the influence of different positions, whereas traditional models might have fixed patterns of interaction defined by their structure (e.g., convolutional filters or recurrent state transitions).\\n\\n5. Absence of Recurrent or Convolutional Layers: The Transformer model architecture, which uses self-attention, eschews recurrence and convolutions entirely, whereas traditional sequence alignment methods typically rely on these mechanisms to process sequences.\\n\\n6. Positional Encoding: Since self-attention does not inherently consider the order of the sequence, the Transformer uses positional encodings to inject information about the position of the elements in the sequence. Traditional methods like RNNs naturally take the order of elements into account due to their sequential processing.\\n\\nOverall, self-attention mechanisms represent a departure from the incremental processing inherent in traditional sequence alignment methods and provide a more direct way to model relationships in data, which can be especially beneficial for tasks like machine translation and language modeling.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_pdf(pdf_qa,query):\n",
    "    print(\"QUERY: \",query)\n",
    "    result = pdf_qa.invoke({\"query\": query, \"chat_history\": []})\n",
    "    answer = result[\"result\"]\n",
    "    print(\"ANSWER\", answer)\n",
    "    return answer\n",
    "\n",
    "\n",
    "ask_pdf(pdf_qa,\"How does the self-attention mechanism in transformers differ from traditional sequence alignment methods?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  Quiz me with 3 simple questions on the positional encodings and the role they play in transformers.\n",
      "ANSWER Sure, here are three simple questions focused on positional encodings in the context of Transformer models:\n",
      "\n",
      "1. What is the main purpose of positional encodings in the Transformer architecture?\n",
      "   A) To indicate the order of words in the input sequence.\n",
      "   B) To help the model perform better on longer sequences.\n",
      "   C) To reduce the training time of the model.\n",
      "   D) To provide a unique identifier for each word in the vocabulary.\n",
      "\n",
      "2. How do positional encodings enable the Transformer model to take into account the order of words in the input sequence?\n",
      "   A) By adding a unique positional signal to each word embedding, which helps the model distinguish the position of each word.\n",
      "   B) By using a separate recurrent neural network to track the positions.\n",
      "   C) By sorting the words in alphabetical order before processing.\n",
      "   D) By creating a separate attention mechanism specifically for positions.\n",
      "\n",
      "3. In the original Transformer paper, what type of positional encoding is mentioned as an alternative to the sinusoidal positional encoding, and what was the observed effect when it was used?\n",
      "   A) Learned positional embeddings; there was a significant increase in translation quality.\n",
      "   B) Fixed positional embeddings; the model's performance significantly decreased.\n",
      "   C) Learned positional embeddings; there were nearly identical results to the base model.\n",
      "   D) Random positional embeddings; the model failed to converge during training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure, here are three simple questions focused on positional encodings in the context of Transformer models:\\n\\n1. What is the main purpose of positional encodings in the Transformer architecture?\\n   A) To indicate the order of words in the input sequence.\\n   B) To help the model perform better on longer sequences.\\n   C) To reduce the training time of the model.\\n   D) To provide a unique identifier for each word in the vocabulary.\\n\\n2. How do positional encodings enable the Transformer model to take into account the order of words in the input sequence?\\n   A) By adding a unique positional signal to each word embedding, which helps the model distinguish the position of each word.\\n   B) By using a separate recurrent neural network to track the positions.\\n   C) By sorting the words in alphabetical order before processing.\\n   D) By creating a separate attention mechanism specifically for positions.\\n\\n3. In the original Transformer paper, what type of positional encoding is mentioned as an alternative to the sinusoidal positional encoding, and what was the observed effect when it was used?\\n   A) Learned positional embeddings; there was a significant increase in translation quality.\\n   B) Fixed positional embeddings; the model's performance significantly decreased.\\n   C) Learned positional embeddings; there were nearly identical results to the base model.\\n   D) Random positional embeddings; the model failed to converge during training.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_questions = ask_pdf(pdf_qa, \"Quiz me with 3 simple questions on the positional encodings and the role they play in transformers.\")\n",
    "\n",
    "quiz_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template = f\"You take in text and spit out Python code doing what the user wants\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\"Return ONLY a PYTHON list containing the questions in this text: {questions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain = chat_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```python\\nquestions = [\\n    \"What is the main purpose of positional encodings in the Transformer architecture?\",\\n    \"How do positional encodings enable the Transformer model to take into account the order of words in the input sequence?\",\\n    \"In the original Transformer paper, what type of positional encoding is mentioned as an alternative to the sinusoidal positional encoding, and what was the observed effect when it was used?\"\\n]\\n```', response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 314, 'total_tokens': 398}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': 'fp_94f711dcf6', 'finish_reason': 'stop', 'logprobs': None}, id='run-cd46aeac-badd-408b-8f5d-bff511988afe-0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_chain.invoke({\"questions\": quiz_questions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_python_code(markdown_string):\n",
    "    pattern = r'```python\\n(.*?)\\n```'\n",
    "    matches = re.findall(pattern, markdown_string, re.DOTALL)\n",
    "\n",
    "    if matches:\n",
    "        python_code = matches[0]\n",
    "        return python_code\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "quiz_chain = chat_prompt | llm | RunnableLambda(lambda x: x.content) | extract_python_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: We haven't discussed runnable at length, but essentially they make up the core of the LCEL interface. \n",
    "\n",
    "`RunnableLambda` allows you to take in an output from part of the chain and pass it along after performing some transformation defined withint its lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = quiz_chain.invoke({\"questions\": quiz_questions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'questions = [\\n    \"What is the main purpose of positional encodings in the Transformer architecture?\",\\n    \"How do positional encodings enable the Transformer model to take into account the order of words in the input sequence?\",\\n    \"In the original Transformer paper, what type of positional encoding is mentioned as an alternative to the sinusoidal positional encoding, and what was the observed effect when it was used?\"\\n]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(questions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the main purpose of positional encodings in the Transformer architecture?',\n",
       " 'How do positional encodings enable the Transformer model to take into account the order of words in the input sequence?',\n",
       " 'In the original Transformer paper, what type of positional encoding is mentioned as an alternative to the sinusoidal positional encoding, and what was the observed effect when it was used?']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  What is the main purpose of positional encodings in the Transformer architecture?\n",
      "ANSWER The main purpose of positional encodings in the Transformer architecture is to provide some information about the order of the tokens in the sequence. Since the Transformer relies entirely on attention mechanisms without any recurrent or convolutional layers, it does not inherently capture the sequential order of the input tokens. Positional encodings enable the model to take into account the position of each token within the sequence, which is crucial for tasks where the order of inputs affects the meaning and hence the output, such as in language modeling and machine translation.\n",
      "\n",
      "The positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks, ensuring that the model can consider the order of the tokens when processing the sequence. This allows the Transformer to maintain high parallelization while still being sensitive to the order of the tokens, which is vital for understanding and generating sequences in a meaningful way.\n",
      "QUERY:  How do positional encodings enable the Transformer model to take into account the order of words in the input sequence?\n",
      "ANSWER Positional encodings in the Transformer model provide a means of incorporating the order of words in the input sequence, which is vital for understanding the meaning in many language-related tasks. Since the Transformer relies entirely on self-attention mechanisms without using recurrence or convolutions, it does not have an inherent way to process the input tokens sequentially or recognize their positions. Without positional encodings, the model would treat the input sequence as if all tokens were presented simultaneously and in no particular order.\n",
      "\n",
      "To overcome this issue, positional encodings are added to the input embeddings at the bottom of the encoder and decoder stacks. These encodings have the same dimension as the embeddings, allowing the two to be summed together. The positional information is encoded using a specific function that varies by the position of each token in the sequence. This function could be a sine and cosine function with different frequencies, as described in the Transformer paper:\n",
      "\n",
      "\\[ PE_{(pos, 2i)} = sin(pos / 10000^{2i/d_{model}}) \\]\n",
      "\\[ PE_{(pos, 2i+1)} = cos(pos / 10000^{2i/d_{model}}) \\]\n",
      "\n",
      "where \\( pos \\) is the position, \\( i \\) is the dimension, and \\( d_{model} \\) is the dimensionality of the token embeddings and positional encodings.\n",
      "\n",
      "These trigonometric functions were chosen because they can provide a unique encoding for each position and allow the model to easily learn to attend by relative positions since for any fixed offset \\( k \\), \\( PE_{pos+k} \\) can be represented as a linear function of \\( PE_{pos} \\).\n",
      "\n",
      "By adding these positional encodings to the input embeddings, the Transformer model can use the added positional information to determine how tokens relate to each other based on their positions in the sequence, which is essential for tasks like translation where the order of words affects the meaning. This allows the model to maintain the order of the sequence throughout the self-attention and subsequent layers.\n",
      "QUERY:  In the original Transformer paper, what type of positional encoding is mentioned as an alternative to the sinusoidal positional encoding, and what was the observed effect when it was used?\n",
      "ANSWER In the original Transformer paper, learned positional embeddings were mentioned as an alternative to the sinusoidal positional encoding. When the learned positional embeddings were used, the observed effect was nearly identical results to the base model that used sinusoidal positional encoding. This is stated in the section that discusses the results in Table 3, specifically in row (E).\n"
     ]
    }
   ],
   "source": [
    "# the questions variable was created within the string inside the `questions_list` variable.\n",
    "answers = []\n",
    "for q in questions:\n",
    "    answers.append(ask_pdf(pdf_qa,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  Is this: The main purpose of positional encodings in the Transformer architecture is to provide some information about the order of the tokens in the sequence. Since the Transformer relies entirely on attention mechanisms without any recurrent or convolutional layers, it does not inherently capture the sequential order of the input tokens. Positional encodings enable the model to take into account the position of each token within the sequence, which is crucial for tasks where the order of inputs affects the meaning and hence the output, such as in language modeling and machine translation.\n",
      "\n",
      "The positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks, ensuring that the model can consider the order of the tokens when processing the sequence. This allows the Transformer to maintain high parallelization while still being sensitive to the order of the tokens, which is vital for understanding and generating sequences in a meaningful way. the correct answer to this question: What is the main purpose of positional encodings in the Transformer architecture? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: Positional encodings in the Transformer model provide a means of incorporating the order of words in the input sequence, which is vital for understanding the meaning in many language-related tasks. Since the Transformer relies entirely on self-attention mechanisms without using recurrence or convolutions, it does not have an inherent way to process the input tokens sequentially or recognize their positions. Without positional encodings, the model would treat the input sequence as if all tokens were presented simultaneously and in no particular order.\n",
      "\n",
      "To overcome this issue, positional encodings are added to the input embeddings at the bottom of the encoder and decoder stacks. These encodings have the same dimension as the embeddings, allowing the two to be summed together. The positional information is encoded using a specific function that varies by the position of each token in the sequence. This function could be a sine and cosine function with different frequencies, as described in the Transformer paper:\n",
      "\n",
      "\\[ PE_{(pos, 2i)} = sin(pos / 10000^{2i/d_{model}}) \\]\n",
      "\\[ PE_{(pos, 2i+1)} = cos(pos / 10000^{2i/d_{model}}) \\]\n",
      "\n",
      "where \\( pos \\) is the position, \\( i \\) is the dimension, and \\( d_{model} \\) is the dimensionality of the token embeddings and positional encodings.\n",
      "\n",
      "These trigonometric functions were chosen because they can provide a unique encoding for each position and allow the model to easily learn to attend by relative positions since for any fixed offset \\( k \\), \\( PE_{pos+k} \\) can be represented as a linear function of \\( PE_{pos} \\).\n",
      "\n",
      "By adding these positional encodings to the input embeddings, the Transformer model can use the added positional information to determine how tokens relate to each other based on their positions in the sequence, which is essential for tasks like translation where the order of words affects the meaning. This allows the model to maintain the order of the sequence throughout the self-attention and subsequent layers. the correct answer to this question: How do positional encodings enable the Transformer model to take into account the order of words in the input sequence? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: In the original Transformer paper, learned positional embeddings were mentioned as an alternative to the sinusoidal positional encoding. When the learned positional embeddings were used, the observed effect was nearly identical results to the base model that used sinusoidal positional encoding. This is stated in the section that discusses the results in Table 3, specifically in row (E). the correct answer to this question: In the original Transformer paper, what type of positional encoding is mentioned as an alternative to the sinusoidal positional encoding, and what was the observed effect when it was used? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['YES', 'YES', 'YES']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for q,a in zip(questions, answers):\n",
    "    # Check for results\n",
    "    evaluations.append(ask_pdf(pdf_qa,f\"Is this: {a} the correct answer to this question: {q} according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\"))\n",
    "\n",
    "evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "yes_count = evaluations.count('YES')\n",
    "scores.append(yes_count)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
