{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install langchain-openai\n",
    "# !pip install pypdf\n",
    "# !pip install chromadb\n",
    "# !pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# # Set OPENAI API Key\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your openai key\"\n",
    "\n",
    "# OR (load from .env file)\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# make sure you have python-dotenv installed\n",
    "# load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a study workflow using Jupyter Notebooks, LLMs, and langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x3306e3190>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x334e15690>, model_name='gpt-4o', openai_api_key=SecretStr('**********'), openai_api_base='https://api.openai.com/v1', openai_proxy='')), document_prompt=PromptTemplate(input_variables=['page_content'], template='Context:\\n{page_content}'), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x334b84c10>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = \"./assets-resources/attention-paper.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path) # LOAD\n",
    "pdf_docs = loader.load_and_split() # SPLIT\n",
    "embeddings = OpenAIEmbeddings() # EMBED\n",
    "vectordb = Chroma.from_documents(pdf_docs, embedding=embeddings) # STORE\n",
    "retriever = vectordb.as_retriever()\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "pdf_qa = RetrievalQA.from_llm(llm=llm, retriever=retriever) # RETRIEVE\n",
    "pdf_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the key components of the transformer architecture?\"\n",
    "result = pdf_qa.invoke({\"query\": query, \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key components of the transformer architecture are:\n",
      "\n",
      "1. **Encoder and Decoder Stacks**:\n",
      "   - **Encoder**: The encoder is composed of a stack of 6 identical layers, each with two sub-layers:\n",
      "     - Multi-head self-attention mechanism\n",
      "     - Position-wise fully connected feed-forward network\n",
      "     - Residual connections around each of the sub-layers followed by layer normalization\n",
      "   - **Decoder**: The decoder also has a stack of 6 identical layers, with three sub-layers:\n",
      "     - Multi-head self-attention mechanism\n",
      "     - Position-wise fully connected feed-forward network\n",
      "     - Multi-head attention over the encoder stack output\n",
      "     - Similar residual connections and layer normalization as in the encoder\n",
      "     - Modified self-attention to prevent positions from attending to subsequent positions\n",
      "\n",
      "2. **Attention Mechanisms**:\n",
      "   - **Self-Attention**: Relates different positions of a single sequence to compute a representation of the sequence.\n",
      "   - **Multi-Head Attention**: Improves the model's ability to focus on different positions. It allows the model to jointly attend to information from different representation subspaces.\n",
      "\n",
      "3. **Residual Connections and Layer Normalization**: These facilitate training by normalizing the outputs and adding connections that skip one or more layers.\n",
      "\n",
      "4. **Feed-Forward Neural Networks**: Applied to each position separately and identically, these networks add non-linearity and depth to the model.\n",
      "\n",
      "5. **Positional Encoding**: Added to the input embeddings to provide the model with information about the relative or absolute position of the tokens in the sequence, since the transformer does not inherently encode sequence order.\n",
      "\n",
      "These components work together to allow the transformer to handle sequence data more efficiently than traditional recurrent neural networks, by leveraging parallelization and the ability to model long-range dependencies directly.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  How does the self-attention mechanism in transformers differ from traditional sequence alignment methods?\n",
      "ANSWER The self-attention mechanism in transformers differs from traditional sequence alignment methods, such as those used in recurrent neural networks (RNNs) and convolutional neural networks (CNNs), in several key ways:\n",
      "\n",
      "1. **Parallelization**:\n",
      "   - **Traditional Sequence Alignment (RNNs)**: These models process sequences sequentially, where the computation at each position \\( t \\) depends on the previous position \\( t-1 \\). This inherently sequential nature limits parallelization during training and inference.\n",
      "   - **Self-Attention (Transformers)**: Self-attention mechanisms allow for parallel computation of representations at all positions in the input sequence. This enables significant parallelization, making the training process much faster.\n",
      "\n",
      "2. **Dependency Modeling**:\n",
      "   - **Traditional Sequence Alignment (RNNs)**: Dependencies between distant positions in the sequence are typically harder to learn because each position depends on the prior positions linearly through time steps. This can lead to vanishing gradient issues and difficulty in capturing long-range dependencies.\n",
      "   - **Self-Attention (Transformers)**: Self-attention computes dependencies between all positions directly in a single step, irrespective of their distance in the sequence. This allows the model to capture long-range dependencies more effectively.\n",
      "\n",
      "3. **Complexity**:\n",
      "   - **Traditional Sequence Alignment (CNNs)**: The number of operations required to relate signals from two arbitrary positions grows with the distance between positions. For ConvS2S models, this grows linearly, and for ByteNet, it grows logarithmically.\n",
      "   - **Self-Attention (Transformers)**: The number of operations required to relate two positions is constant. This efficiency is achieved by comparing each position to every other position in the sequence in parallel, though at the cost of increased computational overhead due to the quadratic scaling in the sequence length.\n",
      "\n",
      "4. **Flexibility and Generalization**:\n",
      "   - **Traditional Sequence Alignment**: Typically tailored to specific sequence lengths and structures, making them less flexible in handling variable-length sequences or diverse tasks without significant modifications.\n",
      "   - **Self-Attention (Transformers)**: Provides a more flexible framework that generalizes well across different tasks, such as translation, reading comprehension, and text summarization, without the need for significant changes in the model architecture.\n",
      "\n",
      "In summary, the self-attention mechanism in transformers offers greater parallelization, better modeling of long-range dependencies, and more efficient computation of relationships between positions in a sequence compared to traditional sequence alignment methods used in RNNs and CNNs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The self-attention mechanism in transformers differs from traditional sequence alignment methods, such as those used in recurrent neural networks (RNNs) and convolutional neural networks (CNNs), in several key ways:\\n\\n1. **Parallelization**:\\n   - **Traditional Sequence Alignment (RNNs)**: These models process sequences sequentially, where the computation at each position \\\\( t \\\\) depends on the previous position \\\\( t-1 \\\\). This inherently sequential nature limits parallelization during training and inference.\\n   - **Self-Attention (Transformers)**: Self-attention mechanisms allow for parallel computation of representations at all positions in the input sequence. This enables significant parallelization, making the training process much faster.\\n\\n2. **Dependency Modeling**:\\n   - **Traditional Sequence Alignment (RNNs)**: Dependencies between distant positions in the sequence are typically harder to learn because each position depends on the prior positions linearly through time steps. This can lead to vanishing gradient issues and difficulty in capturing long-range dependencies.\\n   - **Self-Attention (Transformers)**: Self-attention computes dependencies between all positions directly in a single step, irrespective of their distance in the sequence. This allows the model to capture long-range dependencies more effectively.\\n\\n3. **Complexity**:\\n   - **Traditional Sequence Alignment (CNNs)**: The number of operations required to relate signals from two arbitrary positions grows with the distance between positions. For ConvS2S models, this grows linearly, and for ByteNet, it grows logarithmically.\\n   - **Self-Attention (Transformers)**: The number of operations required to relate two positions is constant. This efficiency is achieved by comparing each position to every other position in the sequence in parallel, though at the cost of increased computational overhead due to the quadratic scaling in the sequence length.\\n\\n4. **Flexibility and Generalization**:\\n   - **Traditional Sequence Alignment**: Typically tailored to specific sequence lengths and structures, making them less flexible in handling variable-length sequences or diverse tasks without significant modifications.\\n   - **Self-Attention (Transformers)**: Provides a more flexible framework that generalizes well across different tasks, such as translation, reading comprehension, and text summarization, without the need for significant changes in the model architecture.\\n\\nIn summary, the self-attention mechanism in transformers offers greater parallelization, better modeling of long-range dependencies, and more efficient computation of relationships between positions in a sequence compared to traditional sequence alignment methods used in RNNs and CNNs.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_pdf(pdf_qa,query):\n",
    "    print(\"QUERY: \",query)\n",
    "    result = pdf_qa.invoke({\"query\": query, \"chat_history\": []})\n",
    "    answer = result[\"result\"]\n",
    "    print(\"ANSWER\", answer)\n",
    "    return answer\n",
    "\n",
    "\n",
    "ask_pdf(pdf_qa,\"How does the self-attention mechanism in transformers differ from traditional sequence alignment methods?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  Quiz me with 3 simple questions on the positional encodings and the role they play in transformers.\n",
      "ANSWER Sure! Here are three simple questions about positional encodings and their role in transformers:\n",
      "\n",
      "1. **What is the primary purpose of positional encodings in the Transformer model?**\n",
      "\n",
      "2. **What type of positional encoding does the base Transformer model use?**\n",
      "\n",
      "3. **What was observed when sinusoidal positional encoding was replaced with learned positional embeddings in the transformer model, according to the provided context?**\n",
      "\n",
      "Feel free to answer these questions, and I can provide feedback if you'd like!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure! Here are three simple questions about positional encodings and their role in transformers:\\n\\n1. **What is the primary purpose of positional encodings in the Transformer model?**\\n\\n2. **What type of positional encoding does the base Transformer model use?**\\n\\n3. **What was observed when sinusoidal positional encoding was replaced with learned positional embeddings in the transformer model, according to the provided context?**\\n\\nFeel free to answer these questions, and I can provide feedback if you'd like!\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_questions = ask_pdf(pdf_qa, \"Quiz me with 3 simple questions on the positional encodings and the role they play in transformers.\")\n",
    "\n",
    "quiz_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template = f\"You take in text and spit out Python code doing what the user wants\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\"Return ONLY a PYTHON list containing the questions in this text: {questions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain = chat_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```python\\nquestions = [\\n    \"What is the primary purpose of positional encodings in the Transformer model?\",\\n    \"What type of positional encoding does the base Transformer model use?\",\\n    \"What was observed when sinusoidal positional encoding was replaced with learned positional embeddings in the transformer model, according to the provided context?\"\\n]\\n```', response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 134, 'total_tokens': 199}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'stop', 'logprobs': None}, id='run-7bcef4d0-40bb-4f23-9b6f-378fb93f5785-0', usage_metadata={'input_tokens': 134, 'output_tokens': 65, 'total_tokens': 199})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_chain.invoke({\"questions\": quiz_questions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_python_code(markdown_string):\n",
    "    pattern = r'```python\\n(.*?)\\n```'\n",
    "    matches = re.findall(pattern, markdown_string, re.DOTALL)\n",
    "\n",
    "    if matches:\n",
    "        python_code = matches[0]\n",
    "        return python_code\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "quiz_chain = chat_prompt | llm | RunnableLambda(lambda x: x.content) | extract_python_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: We haven't discussed runnable at length, but essentially they make up the core of the LCEL interface. \n",
    "\n",
    "`RunnableLambda` allows you to take in an output from part of the chain and pass it along after performing some transformation defined withint its lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = quiz_chain.invoke({\"questions\": quiz_questions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'questions = [\\n    \"What is the primary purpose of positional encodings in the Transformer model?\",\\n    \"What type of positional encoding does the base Transformer model use?\",\\n    \"What was observed when sinusoidal positional encoding was replaced with learned positional embeddings in the transformer model, according to the provided context?\"\\n]'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(questions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the primary purpose of positional encodings in the Transformer model?',\n",
       " 'What type of positional encoding does the base Transformer model use?',\n",
       " 'What was observed when sinusoidal positional encoding was replaced with learned positional embeddings in the transformer model, according to the provided context?']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  What is the primary purpose of positional encodings in the Transformer model?\n",
      "ANSWER The primary purpose of positional encodings in the Transformer model is to provide information about the position of each token in the input sequence. Since the Transformer model eschews recurrence and convolution mechanisms, which naturally handle sequential data by design, it lacks inherent positional information. Positional encodings are added to the input embeddings to ensure that the model can take the order of the sequence into account, allowing it to learn and utilize the relative positions of the tokens effectively.\n",
      "QUERY:  What type of positional encoding does the base Transformer model use?\n",
      "ANSWER The base Transformer model uses sinusoidal positional encoding.\n",
      "QUERY:  What was observed when sinusoidal positional encoding was replaced with learned positional embeddings in the transformer model, according to the provided context?\n",
      "ANSWER When sinusoidal positional encoding was replaced with learned positional embeddings in the transformer model, nearly identical results to the base model were observed.\n"
     ]
    }
   ],
   "source": [
    "# the questions variable was created within the string inside the `questions_list` variable.\n",
    "answers = []\n",
    "for q in questions:\n",
    "    answers.append(ask_pdf(pdf_qa,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  Is this: The primary purpose of positional encodings in the Transformer model is to provide information about the position of each token in the input sequence. Since the Transformer model eschews recurrence and convolution mechanisms, which naturally handle sequential data by design, it lacks inherent positional information. Positional encodings are added to the input embeddings to ensure that the model can take the order of the sequence into account, allowing it to learn and utilize the relative positions of the tokens effectively. the correct answer to this question: What is the primary purpose of positional encodings in the Transformer model? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: The base Transformer model uses sinusoidal positional encoding. the correct answer to this question: What type of positional encoding does the base Transformer model use? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: When sinusoidal positional encoding was replaced with learned positional embeddings in the transformer model, nearly identical results to the base model were observed. the correct answer to this question: What was observed when sinusoidal positional encoding was replaced with learned positional embeddings in the transformer model, according to the provided context? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['YES', 'YES', 'YES']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for q,a in zip(questions, answers):\n",
    "    # Check for results\n",
    "    evaluations.append(ask_pdf(pdf_qa,f\"Is this: {a} the correct answer to this question: {q} according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\"))\n",
    "\n",
    "evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "yes_count = evaluations.count('YES')\n",
    "scores.append(yes_count)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-chatgpt-apps",
   "language": "python",
   "name": "oreilly-chatgpt-apps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
