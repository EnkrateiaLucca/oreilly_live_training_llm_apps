{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "# os.environ[\"OPENAI_API_KEY\"] = '<your-api-key>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Techniques\n",
    "\n",
    "Now, let's walk through a simplified guide of prompt engineering techniques:\n",
    "\n",
    "- [Zero-shot Prompting](https://www.promptingguide.ai/techniques/zeroshot#:~:text=Large%20LLMs%20today,examples%20we%20used%3A)\n",
    "- [Few-shot Prompting](https://www.promptingguide.ai/techniques/fewshot#:~:text=few-shot%20prompting%20can%20be%20used%20as%20a%20technique%20to%20enable%20in-context%20learning%20where%20we%20provide%20demonstrations%20in%20the%20prompt%20to%20steer%20the%20model%20to%20better%20performance)\n",
    "- [Chain-of-Thought](https://www.promptingguide.ai/techniques/cot#:~:text=introduced%20in%20wei%20et%20al.%20(2022)%20(opens%20in%20a%20new%20tab)%2C%20chain-of-thought%20(cot)%20prompting%20enables%20complex%20reasoning%20capabilities%20through%20intermediate%20reasoning%20steps.%20you%20can%20combine%20it%20with%20few-shot%20prompting%20to%20get%20better%20results%20on%20more%20complex%20tasks%20that%20require%20reasoning%20before%20responding.)\n",
    "- [Self-consistency](https://www.promptingguide.ai/techniques/consistency#:~:text=Perhaps%20one%20of,and%20commonsense%20reasoning.)\n",
    "- [Generate Knowledge](https://www.promptingguide.ai/techniques/knowledge#:~:text=LLMs%20continue%20to,as%20commonsense%20reasoning%3F)\n",
    "- [Tree of thoughts (ToT)](https://www.promptingguide.ai/techniques/tot#:~:text=For%20complex%20tasks,with%20language%20models.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot Prompting\n",
    "\n",
    "[Zero-shot prompting](https://arxiv.org/pdf/2109.01652.pdf) is when you solve the task without showing any examples of what a solution might look like.\n",
    "\n",
    "For example consider a prompt like:\n",
    "\n",
    "```\n",
    "Classify the sentiment in this sentence as negative or positive:\n",
    "Text: I will go to a vacation\n",
    "Sentiment:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sentiment in the given text \"I don\\'t like studying at all!\" is negative.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(model=\"gpt-4o-mini\", \n",
    "                             messages=\n",
    "                             [\n",
    "                                 {\"role\": \"system\", \"content\": \"You are a savy guru with knowledge about existence and the secrets of life.\"},\n",
    "                                 {\"role\": \"user\", \"content\": prompt}   \n",
    "                             ],\n",
    "                             max_tokens=100,\n",
    "                             temperature=0.9,\n",
    "                             n = 1)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "prompt = \"\"\"Classify the sentiment in this sentence as negative or positive:\n",
    "Text: I don't like studying at all!.\n",
    "Sentiment:\"\"\"\n",
    "get_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a few more like:\n",
    "\n",
    "```\n",
    "What is the capital of Canada?\n",
    "Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ottawa'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the capital of Canada?\\nAnswer (one word):\"\n",
    "get_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so on and so forth, one can use this as the first try at a model to see what kinds of tasks that LLM can already solve out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot Prompting\n",
    "\n",
    "As the complexity of a task increases, you might need to provide information in the form of examples to the LLM.\n",
    "\n",
    "**Few-shot Prompting** is a prompting technique where you show a few examples of what a solution might look like.\n",
    "\n",
    "THe goal is to enable what is called 'in-context learning' where the model improves by learning contextual information about the task at hand.\n",
    "\n",
    "We do that by giving demonstrations that will serve as conditionning for subsequent examples where we would like the model to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sorry, but I can\\'t continue the example sentence as the word \"farduddle\" is not a commonly recognized term. Perhaps you might be thinking of a different word or concept that you\\'d like me to explore or explain?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the example was taken from here: https://www.promptingguide.ai/techniques/fewshot\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(model=\"gpt-4o-mini\", \n",
    "                             messages=\n",
    "                             [\n",
    "                                 {\"role\": \"system\", \"content\": \"You are a savy guru with knowledge about existence and the secrets of life.\"},\n",
    "                                 {\"role\": \"user\", \"content\": prompt}   \n",
    "                             ],\n",
    "                             max_tokens=100,\n",
    "                             temperature=0.9,\n",
    "                             n = 1)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "prompt = \"\"\"\n",
    "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses\n",
    "the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses\n",
    "the word farduddle is:\n",
    "\"\"\"\n",
    "get_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survey question!\n",
    "few_shot_prompt = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Thought\n",
    "\n",
    "This is a prompting technique where we induce step-by-step reasoning and planning within the prompt to enhance performance of the model.\n",
    "\n",
    "According to [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you are 17 years old and Sally is 5 years younger than you, then Sally is 17 - 5 = 12 years old. If Jack is 2 years older than Sally, then Jack is 12 + 2 = 14 years old. So, Jack is 14 years old.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the example was taken from here: https://www.promptingguide.ai/techniques/fewshot\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(model=\"gpt-4o-mini\", \n",
    "                             messages=\n",
    "                             [\n",
    "                                 {\"role\": \"system\", \"content\": \"You are a savy guru with knowledge about existence and the secrets of life.\"},\n",
    "                                 {\"role\": \"user\", \"content\": prompt}   \n",
    "                             ],\n",
    "                             max_tokens=100,\n",
    "                             temperature=0.9,\n",
    "                             n = 1)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "prompt = \"\"\"\n",
    "Q: I have one sister and one brother. I am 20 years of age. My sister is 5 years older and my brother 2 years younger than my sister.\n",
    "How old is my brother?\n",
    "A: If I am 20 years of age and my sister is 5 years older, my sister is 20+5=25 years old. If my brother is 2 years younger than my sister, my brother is 25-2=23 years old. The answer is 23 years old.\n",
    "\n",
    "Q: I have 2 friends, Jack and Sally. Jack is 2 years older than Sally. Sally is 5 years younger than me. I am 17 years old. How old is Jack?\n",
    "A:\n",
    "\"\"\"\n",
    "get_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If Sally is 5 years younger than you and you are 17 years old, then Sally is 17-5=12 years old.\\nSince Jack is 2 years older than Sally, Jack is 12+2=14 years old.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Survey prompt!\n",
    "prompt_CoT = \"?\"\n",
    "get_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine few-shot prompting with chain-of-thought to get better results on highly complex tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://www.promptingguide.ai/techniques/cot \n",
    "prompt = \"\"\"\n",
    "Q: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "Q:The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
    "\n",
    "Q:The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
    "\n",
    "Q:The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
    "Q:The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\"\"\"\n",
    "get_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-consistency\n",
    "\n",
    "You use few shot prompting and chain of thoughts to sample a bunch of reasoning paths and then use generations to select the most consistent answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n",
      "*\n",
      "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n",
      "*\n",
      "Adding all the odd numbers (15, 5, 13, 7, 1) gives a sum of 41. The answer is False.\n",
      "*\n",
      "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n",
      "*\n",
      "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "# source: https://arxiv.org/pdf/2203.11171.pdf\n",
    "few_shot_CoT_prompt = \"\"\"\n",
    "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: \n",
    "\"\"\"\n",
    "\n",
    "n_reasoning_paths = 5\n",
    "answers = []\n",
    "for i in range(n_reasoning_paths):\n",
    "    response = get_response(few_shot_CoT_prompt)\n",
    "    answers.append(response)\n",
    "    print(response)\n",
    "    print(\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Knowledge\n",
    "\n",
    "This technique is about inserting knowledge into the prompt in order to yield better performance, you use the model to generate knowledge about a field, and then use that generated knowledge to improve its performance on a downstream task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In golf, the objective is actually to complete each hole in as few strokes as possible, rather than accumulating a high point total. The player with the lowest number of strokes for the entire round wins. This might be referred to as trying to get a lower score or point total than others, rather than a higher one. Success in golf is typically determined by precision, strategy, and skill in hitting the ball accurately and efficiently, rather than maximizing a point total.', 'Part of golf is actually trying to achieve the lowest score possible. In golf, each stroke is counted as a point, and the objective is to complete the course in the fewest number of strokes. The player with the lowest total score at the end of the round is the winner. So, in golf, the aim is to have the least number of points, not the highest.', 'Part of golf is actually trying to have the lowest score possible, unlike other sports where the objective is to have a higher point total than others. In golf, the ultimate goal is to complete the course with the fewest number of strokes or shots possible. This is reflected in various aspects of the game such as handicap systems and scoring conventions like \"par\" and \"bogey\".']\n"
     ]
    }
   ],
   "source": [
    "# source: https://www.promptingguide.ai/techniques/knowledge\n",
    "prompt = \"\"\"Input: Greece is larger than mexico.\n",
    "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
    "Input: Glasses always fog up.\n",
    "Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n",
    "Input: A fish is capable of thinking.\n",
    "Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of ’higher’ vertebrates including non-human primates. Fish’s long-term memories help them keep track of complex social relationships.\n",
    "Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\n",
    "Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n",
    "Input: A rock is the same size as a pebble.\n",
    "Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n",
    "Input: Part of golf is trying to get a higher point total than others.\n",
    "Knowledge:\"\"\"\n",
    "knowledges = []\n",
    "num_knowledges = 3\n",
    "for i in range(num_knowledges):\n",
    "    knowledges.append(get_response(prompt))\n",
    "\n",
    "print(knowledges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We integrate the knowledge to get a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, the objective of golf is not to get a higher point total than others. In golf, the aim is to complete the course in the least number of strokes possible. Each stroke is counted as a point, and the player with the lowest total number of strokes at the end of the round is the winner. So, in golf, the goal is to achieve the lowest score possible, not a high point total.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://www.promptingguide.ai/techniques/knowledge\n",
    "prompt = \"\"\"Question: Part of golf is trying to get a higher point total than others. Yes or No?\n",
    "Knowledge: The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.\n",
    "Explain and Answer: \"\"\"\n",
    "\n",
    "get_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree of thoughts (ToT)\n",
    "\n",
    "\n",
    "ToT [Long (2023)](https://arxiv.org/pdf/2305.08291.pdf) is a framework that generalizes over chain-of-thought prompting and encourages exploration over thoughts that ser as intermediate steps for general problem solving with LMs.\n",
    "\n",
    "This technique involves a framework where a tree of thoughts is maintained, where a thought here means a coherent sequence of steps that represent moving forward in the solution. The LMs are given the ability to self-evaluate on how intermediate thoughts contribute towards progress solving the problem through a deliberate reasoning process which involves combining this evaluation ability with search algorithms to allow for backtracking and lookahead over the space of possible thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/ToT_framework.png)\n",
    "Image Source: [Yao et al. (2023)](https://arxiv.org/pdf/2305.08291.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many More but That's Enough\n",
    "\n",
    "There are many more prompt engineering techniques that grow in complexity like:\n",
    "- [Retrieval Augmented Generation (RAG)](https://www.promptingguide.ai/techniques/rag)\n",
    "- [Automatic Prompt Engineer](https://www.promptingguide.ai/techniques/ape)\n",
    "- [Active Prompt](https://www.promptingguide.ai/techniques/activeprompt)\n",
    "- [Directional Stimulus Prompting](https://www.promptingguide.ai/techniques/dsp)\n",
    "- [React Prompting](https://www.promptingguide.ai/techniques/react)\n",
    "- [Mulitmodal CoT](https://www.promptingguide.ai/techniques/multimodalcot)\n",
    "- [Graph Prompting](https://www.promptingguide.ai/techniques/graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-chatgpt-apps-test",
   "language": "python",
   "name": "oreilly-chatgpt-apps-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
