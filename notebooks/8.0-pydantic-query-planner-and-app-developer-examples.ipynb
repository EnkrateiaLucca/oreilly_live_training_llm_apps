{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime.datetime(2020, 1, 2, 3, 4, 5, tzinfo=TzInfo(UTC))\n",
      "(10, 20)\n"
     ]
    }
   ],
   "source": [
    "class Delivery(BaseModel):\n",
    "    timestamp: datetime\n",
    "    dimensions: Tuple[int, int]\n",
    "\n",
    "\n",
    "m = Delivery(timestamp='2020-01-02T03:04:05Z', dimensions=[10, 20])\n",
    "\n",
    "print(repr(m.timestamp))\n",
    "\n",
    "print(m.dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install graphviz\n",
    "# !pip install instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# This enables response_model keyword\n",
    "# from client.chat.completions.create\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "class UserDetail(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "user = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=UserDetail,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert isinstance(user, UserDetail)\n",
    "assert user.name == \"Jason\"\n",
    "assert user.age == 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "class UserExtract(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "model = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=UserExtract,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "assert isinstance(model, UserExtract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "from typing import List\n",
    "\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class QueryType(str, enum.Enum):\n",
    "    \"\"\"Enumeration representing the types of queries that can be asked to a question answer system.\"\"\"\n",
    "\n",
    "    SINGLE_QUESTION = \"SINGLE\"\n",
    "    MERGE_MULTIPLE_RESPONSES = \"MERGE_MULTIPLE_RESPONSES\"\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    \"\"\"Class representing a single question in a query plan.\"\"\"\n",
    "\n",
    "    id: int = Field(..., description=\"Unique id of the query\")\n",
    "    question: str = Field(\n",
    "        ...,\n",
    "        description=\"Question asked using a question answering system\",\n",
    "    )\n",
    "    dependancies: List[int] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of sub questions that need to be answered before asking this question\",\n",
    "    )\n",
    "    node_type: QueryType = Field(\n",
    "        default=QueryType.SINGLE_QUESTION,\n",
    "        description=\"Type of question, either a single question or a multi-question merge\",\n",
    "    )\n",
    "\n",
    "\n",
    "class QueryPlan(BaseModel):\n",
    "    \"\"\"Container class representing a tree of questions to ask a question answering system.\"\"\"\n",
    "\n",
    "    query_graph: List[Query] = Field(\n",
    "        ..., description=\"The query graph representing the plan\"\n",
    "    )\n",
    "\n",
    "    def _dependencies(self, ids: List[int]) -> List[Query]:\n",
    "        \"\"\"Returns the dependencies of a query given their ids.\"\"\"\n",
    "        return [q for q in self.query_graph if q.id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "# Apply the patch to the OpenAI client\n",
    "# enables response_model keyword\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "def query_planner(question: str) -> QueryPlan:\n",
    "    PLANNING_MODEL = \"gpt-4-0125-preview\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a world class query planning algorithm capable ofbreaking apart questions into its dependency queries such that the answers can be used to inform the parent question. Do not answer the questions, simply provide a correct compute graph with good specific questions to ask and relevant dependencies. Before you call the function, think step-by-step to get a better understanding of the problem.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Consider: {question}\\nGenerate the correct query plan.\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=PLANNING_MODEL,\n",
    "        temperature=0,\n",
    "        response_model=QueryPlan,\n",
    "        messages=messages,\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_graph': [{'id': 1,\n",
       "   'question': 'What is the population of Canada?',\n",
       "   'dependancies': [],\n",
       "   'node_type': <QueryType.SINGLE_QUESTION: 'SINGLE'>},\n",
       "  {'id': 2,\n",
       "   'question': \"What is Jason's home country?\",\n",
       "   'dependancies': [],\n",
       "   'node_type': <QueryType.SINGLE_QUESTION: 'SINGLE'>},\n",
       "  {'id': 3,\n",
       "   'question': 'What is the population of {country}?',\n",
       "   'dependancies': [2],\n",
       "   'node_type': <QueryType.SINGLE_QUESTION: 'SINGLE'>}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan = query_planner(\n",
    "    \"What is the difference in populations of Canada and the Jason's home country?\"\n",
    ")\n",
    "plan.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readme.md\n",
      "-\n",
      "# FastAPI App\n",
      "\n",
      "This is a FastAPI app that provides some basic math functions.\n",
      "\n",
      "## Usage\n",
      "\n",
      "To use this app, follow the instructions below:\n",
      "\n",
      "1. Install the required dependencies by running `pip install -r requirements.txt`.\n",
      "2. Start the app by running `uvicorn main:app --reload`.\n",
      "3. Open your browser and navigate to `http://localhost:8000/docs` to access the Swagger UI documentation.\n",
      "\n",
      "## Example\n",
      "\n",
      "To perform a basic math operation, you can use the following curl command:\n",
      "\n",
      "```bash\n",
      "curl -X POST -H \"Content-Type: application/json\" -d '{\"operation\": \"add\", \"a\": 2, \"b\": 3}' http://localhost:8000/calculate\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "main.py\n",
      "-\n",
      "from fastapi import FastAPI\n",
      "from pydantic import BaseModel\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "\n",
      "class Calculation(BaseModel):\n",
      "    operation: str\n",
      "    a: float\n",
      "    b: float\n",
      "\n",
      "\n",
      "@app.post('/calculate')\n",
      "async def calculate(calculation: Calculation):\n",
      "    if calculation.operation == 'add':\n",
      "        result = calculation.a + calculation.b\n",
      "    elif calculation.operation == 'subtract':\n",
      "        result = calculation.a - calculation.b\n",
      "    elif calculation.operation == 'multiply':\n",
      "        result = calculation.a * calculation.b\n",
      "    elif calculation.operation == 'divide':\n",
      "        result = calculation.a / calculation.b\n",
      "    else:\n",
      "        return {'error': 'Invalid operation'}\n",
      "    return {'result': result}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "requirements.txt\n",
      "-\n",
      "fastapi\n",
      "uvicorn\n",
      "pydantic\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import Field\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class File(BaseModel):\n",
    "    \"\"\"\n",
    "    Correctly named file with contents.\n",
    "    \"\"\"\n",
    "\n",
    "    file_name: str = Field(\n",
    "        ..., description=\"The name of the file including the extension\"\n",
    "    )\n",
    "    body: str = Field(..., description=\"Correct contents of a file\")\n",
    "\n",
    "    def save(self):\n",
    "        with open(self.file_name, \"w\") as f:\n",
    "            f.write(self.body)\n",
    "\n",
    "\n",
    "class Program(BaseModel):\n",
    "    \"\"\"\n",
    "    Set of files that represent a complete and correct program\n",
    "    \"\"\"\n",
    "\n",
    "    files: List[File] = Field(..., description=\"List of files\")\n",
    "    \n",
    "\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "# Apply the patch to the OpenAI client\n",
    "# enables response_model keyword\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "\n",
    "def develop(data: str) -> Program:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        temperature=0.1,\n",
    "        response_model=Program,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a world class programming AI capable of writing correct python scripts and modules. You will name files correct, include __init__.py files and write correct python code with correct imports.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": data,\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "\n",
    "program = develop(\n",
    "        \"\"\"\n",
    "        Create a fastapi app with a readme.md file and a main.py file with\n",
    "        some basic math functions. the datamodels should use pydantic and\n",
    "        the main.py should use fastapi. the readme.md should have a title\n",
    "        and a description. The readme should contain some helpful infromation\n",
    "        and a curl example\"\"\"\n",
    "    )\n",
    "\n",
    "for file in program.files:\n",
    "    print(file.file_name)\n",
    "    print(\"-\")\n",
    "    print(file.body)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Program' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 56\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    Changes that must be correctly made in a program's code repository defined as a\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    complete diff (Unified Format) file which will be used to `patch` the repository.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m      +to this document.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     diff: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m Field(\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\n\u001b[1;32m     48\u001b[0m         description\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m         ),\n\u001b[1;32m     52\u001b[0m     )\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrefactor\u001b[39m(new_requirements: \u001b[38;5;28mstr\u001b[39m, program: Program) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Diff:\n\u001b[1;32m     57\u001b[0m     program_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     58\u001b[0m         [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;241m.\u001b[39mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[[[\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m]]]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m code \u001b[38;5;129;01min\u001b[39;00m program\u001b[38;5;241m.\u001b[39mfiles]\n\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# model=\"gpt-3.5-turbo-0613\",\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     86\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Program' is not defined"
     ]
    }
   ],
   "source": [
    "# adding refactoring\n",
    "from pydantic import Field, BaseModel\n",
    "\n",
    "class Diff(BaseModel):\n",
    "    \"\"\"\n",
    "    Changes that must be correctly made in a program's code repository defined as a\n",
    "    complete diff (Unified Format) file which will be used to `patch` the repository.\n",
    "\n",
    "    Example:\n",
    "      --- /path/to/original timestamp\n",
    "      +++ /path/to/new  timestamp\n",
    "      @@ -1,3 +1,9 @@\n",
    "      +This is an important\n",
    "      +notice! It should\n",
    "      +therefore be located at\n",
    "      +the beginning of this\n",
    "      +document!\n",
    "      +\n",
    "       This part of the\n",
    "       document has stayed the\n",
    "       same from version to\n",
    "      @@ -8,13 +14,8 @@\n",
    "       compress the size of the\n",
    "       changes.\n",
    "      -This paragraph contains\n",
    "      -text that is outdated.\n",
    "      -It will be deleted in the\n",
    "      -near future.\n",
    "      -\n",
    "       It is important to spell\n",
    "      -check this dokument. On\n",
    "      +check this document. On\n",
    "       the other hand, a\n",
    "       misspelled word isn't\n",
    "       the end of the world.\n",
    "      @@ -22,3 +23,7 @@\n",
    "       this paragraph needs to\n",
    "       be changed. Things can\n",
    "       be added after it.\n",
    "      +\n",
    "      +This paragraph contains\n",
    "      +important new additions\n",
    "      +to this document.\n",
    "    \"\"\"\n",
    "\n",
    "    diff: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Changes in a code repository correctly represented in 'diff' format, \"\n",
    "            \"correctly escaped so it could be used in a JSON\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def refactor(new_requirements: str, program: Program) -> Diff:\n",
    "    program_description = \"\\n\".join(\n",
    "        [f\"{code.file_name}\\n[[[\\n{code.body}\\n]]]\\n\" for code in program.files]\n",
    "    )\n",
    "    return client.chat.completions.create(\n",
    "        # model=\"gpt-3.5-turbo-0613\",\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0,\n",
    "        response_model=Diff,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a world class programming AI capable of refactor \"\n",
    "                \"existing python repositories. You will name files correct, include \"\n",
    "                \"__init__.py files and write correct python code, with correct imports. \"\n",
    "                \"You'll deliver your changes in valid 'diff' format so that they could \"\n",
    "                \"be applied using the 'patch' command. \"\n",
    "                \"Make sure you put the correct line numbers, \"\n",
    "                \"and that all lines that must be changed are correctly marked.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": new_requirements,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": program_description,\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "\n",
    "changes = refactor(\n",
    "    new_requirements=\"Refactor this code to use flask instead.\",\n",
    "    program=program,\n",
    ")\n",
    "print(changes.diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering Questions with Validated Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use Instructor with validators to not only add citations to answers generated but also prevent hallucinations by ensuring that every statement made by the LLM is backed up by a direct quote from the context provided, and that those quotes exist!.Two Python classes, Fact and QuestionAnswer, are defined to encapsulate the information of individual facts and the entire answer, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field, BaseModel, model_validator, FieldValidationInfo\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "class Fact(BaseModel):\n",
    "    fact: str = Field(...)\n",
    "    substring_quote: List[str] = Field(...)\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_sources(self, info: FieldValidationInfo) -> \"Fact\":\n",
    "        text_chunks = info.context.get(\"text_chunk\", None)\n",
    "        spans = list(self.get_spans(text_chunks))\n",
    "        self.substring_quote = [text_chunks[span[0] : span[1]] for span in spans]\n",
    "        return self\n",
    "\n",
    "    def get_spans(self, context):\n",
    "        for quote in self.substring_quote:\n",
    "            yield from self._get_span(quote, context)\n",
    "\n",
    "    def _get_span(self, quote, context):\n",
    "        for match in re.finditer(re.escape(quote), context):\n",
    "            yield match.span()\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    question: str = Field(...)\n",
    "    answer: List[Fact] = Field(...)\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_sources(self) -> \"QuestionAnswer\":\n",
    "        self.answer = [fact for fact in self.answer if len(fact.substring_quote) > 0]\n",
    "        return self\n",
    "\n",
    "from openai import OpenAI\n",
    "import instructor\n",
    "\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "def ask_ai(question: str, context: str) -> QuestionAnswer:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        temperature=0,\n",
    "        response_model=QuestionAnswer,\n",
    "        messages=[{\n",
    "            \"role\": \"system\", \"content\": \"You are a world class algorithm to answer questions with correct and exact citations.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{context}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
    "        ],\n",
    "        validation_context={\"text_chunk\": context},\n",
    "    )\n",
    "    \n",
    "    question = \"What did the author do during college?\"\n",
    "context = \"\"\"\n",
    "My name is Jason Liu, and I grew up in Toronto Canada but I was born in China.\n",
    "I went to an arts high school but in university I studied Computational Mathematics and physics.\n",
    "As part of coop I worked at many companies including Stitchfix, Facebook.\n",
    "I also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.\n",
    "\"\"\"\n",
    "ask_ai(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnswer(question='What did the author do during college?', answer=[Fact(fact='The author studied Computational Mathematics and physics in university.', substring_quote=['in university I studied Computational Mathematics and physics.']), Fact(fact='The author started the Data Science club at the University of Waterloo and was the president of the club for 2 years.', substring_quote=['started the Data Science club at the University of Waterloo', 'president of the club for 2 years.'])])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-env",
   "language": "python",
   "name": "oreilly-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
