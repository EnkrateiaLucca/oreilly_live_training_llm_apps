{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install pydantic\n",
    "# !pip install generate\n",
    "!pip install python-dotenv\n",
    "!pip install instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# loading from a .env file\n",
    "# load_dotenv(dotenv_path=\"/full/path/to/your/.env\")\n",
    "\n",
    "# or \n",
    "# if you're on google colab just uncomment below and replace with your openai api key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<your-openai-api-key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime.datetime(2020, 1, 2, 3, 4, 5, tzinfo=TzInfo(UTC))\n",
      "(10, 20)\n"
     ]
    }
   ],
   "source": [
    "class Delivery(BaseModel):\n",
    "    timestamp: datetime\n",
    "    dimensions: Tuple[int, int]\n",
    "\n",
    "\n",
    "m = Delivery(timestamp='2020-01-02T03:04:05Z', dimensions=[10, 20])\n",
    "\n",
    "print(repr(m.timestamp))\n",
    "\n",
    "print(m.dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# This enables response_model keyword\n",
    "# from client.chat.completions.create\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "class UserDetail(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "user = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    response_model=UserDetail,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert isinstance(user, UserDetail)\n",
    "assert user.name == \"Jason\"\n",
    "assert user.age == 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "class UserExtract(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "model = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    response_model=UserExtract,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "assert isinstance(model, UserExtract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "from typing import List\n",
    "\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class QueryType(str, enum.Enum):\n",
    "    \"\"\"Enumeration representing the types of queries that can be asked to a question answer system.\"\"\"\n",
    "\n",
    "    SINGLE_QUESTION = \"SINGLE\"\n",
    "    MERGE_MULTIPLE_RESPONSES = \"MERGE_MULTIPLE_RESPONSES\"\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    \"\"\"Class representing a single question in a query plan.\"\"\"\n",
    "\n",
    "    id: int = Field(..., description=\"Unique id of the query\")\n",
    "    question: str = Field(\n",
    "        ...,\n",
    "        description=\"Question asked using a question answering system\",\n",
    "    )\n",
    "    dependancies: List[int] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of sub questions that need to be answered before asking this question\",\n",
    "    )\n",
    "    node_type: QueryType = Field(\n",
    "        default=QueryType.SINGLE_QUESTION,\n",
    "        description=\"Type of question, either a single question or a multi-question merge\",\n",
    "    )\n",
    "\n",
    "\n",
    "class QueryPlan(BaseModel):\n",
    "    \"\"\"Container class representing a tree of questions to ask a question answering system.\"\"\"\n",
    "\n",
    "    query_graph: List[Query] = Field(\n",
    "        ..., description=\"The query graph representing the plan\"\n",
    "    )\n",
    "\n",
    "    def _dependencies(self, ids: List[int]) -> List[Query]:\n",
    "        \"\"\"Returns the dependencies of a query given their ids.\"\"\"\n",
    "        return [q for q in self.query_graph if q.id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "# Apply the patch to the OpenAI client\n",
    "# enables response_model keyword\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "def query_planner(question: str) -> QueryPlan:\n",
    "    PLANNING_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a world class query planning algorithm capable of breaking apart questions into its dependency queries such that the answers can be used to inform the parent question. Do not answer the questions, simply provide a correct compute graph with good specific questions to ask and relevant dependencies. Before you call the function, think step-by-step to get a better understanding of the problem.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Consider: {question}\\nGenerate the correct query plan.\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=PLANNING_MODEL,\n",
    "        temperature=0,\n",
    "        response_model=QueryPlan,\n",
    "        messages=messages,\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_graph': [{'id': 1,\n",
       "   'question': 'What is the population of Canada?',\n",
       "   'dependancies': [],\n",
       "   'node_type': <QueryType.SINGLE_QUESTION: 'SINGLE'>},\n",
       "  {'id': 2,\n",
       "   'question': \"What is Jason's home country?\",\n",
       "   'dependancies': [],\n",
       "   'node_type': <QueryType.SINGLE_QUESTION: 'SINGLE'>},\n",
       "  {'id': 3,\n",
       "   'question': \"What is the population of Jason's home country?\",\n",
       "   'dependancies': [2],\n",
       "   'node_type': <QueryType.SINGLE_QUESTION: 'SINGLE'>},\n",
       "  {'id': 4,\n",
       "   'question': \"What is the difference in populations of Canada and Jason's home country?\",\n",
       "   'dependancies': [1, 3],\n",
       "   'node_type': <QueryType.SINGLE_QUESTION: 'SINGLE'>}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan = query_planner(\n",
    "    \"What is the difference in populations of Canada and the Jason's home country?\"\n",
    ")\n",
    "plan.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main.py\n",
      "-\n",
      "from fastapi import FastAPI\n",
      "from pydantic import BaseModel\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "class MathRequest(BaseModel):\n",
      "    a: float\n",
      "    b: float\n",
      "\n",
      "@app.post(\"/add/\")\n",
      "async def add(request: MathRequest):\n",
      "    return {\"result\": request.a + request.b}\n",
      "\n",
      "@app.post(\"/subtract/\")\n",
      "async def subtract(request: MathRequest):\n",
      "    return {\"result\": request.a - request.b}\n",
      "\n",
      "@app.post(\"/multiply/\")\n",
      "async def multiply(request: MathRequest):\n",
      "    return {\"result\": request.a * request.b}\n",
      "\n",
      "@app.post(\"/divide/\")\n",
      "async def divide(request: MathRequest):\n",
      "    if request.b == 0:\n",
      "        return {\"error\": \"Division by zero is not allowed.\"}\n",
      "    return {\"result\": request.a / request.b}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import Field\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class File(BaseModel):\n",
    "    \"\"\"\n",
    "    Correctly named file with contents.\n",
    "    \"\"\"\n",
    "\n",
    "    file_name: str = Field(\n",
    "        ..., description=\"The name of the file including the extension\"\n",
    "    )\n",
    "    body: str = Field(..., description=\"Correct contents of a file\")\n",
    "\n",
    "    def save(self):\n",
    "        with open(self.file_name, \"w\") as f:\n",
    "            f.write(self.body)\n",
    "\n",
    "\n",
    "class Program(BaseModel):\n",
    "    \"\"\"\n",
    "    Set of files that represent a complete and correct program\n",
    "    \"\"\"\n",
    "\n",
    "    files: List[File] = Field(..., description=\"List of files\")\n",
    "    \n",
    "\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "# Apply the patch to the OpenAI client\n",
    "# enables response_model keyword\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "\n",
    "def develop(data: str) -> Program:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.1,\n",
    "        response_model=Program,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a world class programming AI capable of writing correct python scripts and modules. You will name files correct, include __init__.py files and write correct python code with correct imports.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": data,\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "\n",
    "program = develop(\n",
    "        \"\"\"\n",
    "        Create a fastapi app with a readme.md file and a main.py file with\n",
    "        some basic math functions. the datamodels should use pydantic and\n",
    "        the main.py should use fastapi. the readme.md should have a title\n",
    "        and a description. The readme should contain some helpful infromation\n",
    "        and a curl example\"\"\"\n",
    "    )\n",
    "\n",
    "for file in program.files:\n",
    "    print(file.file_name)\n",
    "    print(\"-\")\n",
    "    print(file.body)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- main.py\t2023-10-01 12:00:00.000000000 +0000\n",
      "+++ main.py\t2023-10-01 12:00:00.000000000 +0000\n",
      "@@ -1,12 +1,12 @@\n",
      "-from fastapi import FastAPI\n",
      "-from pydantic import BaseModel\n",
      "+from flask import Flask, request, jsonify\n",
      "\n",
      "-app = FastAPI()\n",
      "+app = Flask(__name__)\n",
      "\n",
      "-class MathRequest(BaseModel):\n",
      "-    a: float\n",
      "-    b: float\n",
      "+class MathRequest:\n",
      "+    def __init__(self, a, b):\n",
      "+        self.a = a\n",
      "+        self.b = b\n",
      "\n",
      "-@app.post(\"/add/\")\n",
      "+@app.route(\"/add/\", methods=['POST'])\n",
      "-def add():\n",
      "-    return {\"result\": request.a + request.b}\n",
      "+    data = request.get_json()\n",
      "+    math_request = MathRequest(data['a'], data['b'])\n",
      "+    return jsonify(result=math_request.a + math_request.b)\n",
      "\n",
      "-@app.post(\"/subtract/\")\n",
      "+@app.route(\"/subtract/\", methods=['POST'])\n",
      "-def subtract():\n",
      "-    return {\"result\": request.a - request.b}\n",
      "+    data = request.get_json()\n",
      "+    math_request = MathRequest(data['a'], data['b'])\n",
      "+    return jsonify(result=math_request.a - math_request.b)\n",
      "\n",
      "-@app.post(\"/multiply/\")\n",
      "+@app.route(\"/multiply/\", methods=['POST'])\n",
      "-def multiply():\n",
      "-    return {\"result\": request.a * request.b}\n",
      "+    data = request.get_json()\n",
      "+    math_request = MathRequest(data['a'], data['b'])\n",
      "+    return jsonify(result=math_request.a * math_request.b)\n",
      "\n",
      "-@app.post(\"/divide/\")\n",
      "+@app.route(\"/divide/\", methods=['POST'])\n",
      "-def divide():\n",
      "-    if request.b == 0:\n",
      "-        return {\"error\": \"Division by zero is not allowed.\"}\n",
      "-    return {\"result\": request.a / request.b}\n",
      "+    data = request.get_json()\n",
      "+    math_request = MathRequest(data['a'], data['b'])\n",
      "+    if math_request.b == 0:\n",
      "+        return jsonify(error=\"Division by zero is not allowed.\"), 400\n",
      "+    return jsonify(result=math_request.a / math_request.b)\n",
      "\n",
      "-if __name__ == \"__main__\":\n",
      "-    import uvicorn\n",
      "-    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
      "+if __name__ == '__main__':\n",
      "+    app.run(host='0.0.0.0', port=8000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding refactoring\n",
    "from pydantic import Field, BaseModel\n",
    "\n",
    "class Diff(BaseModel):\n",
    "    \"\"\"\n",
    "    Changes that must be correctly made in a program's code repository defined as a\n",
    "    complete diff (Unified Format) file which will be used to `patch` the repository.\n",
    "\n",
    "    Example:\n",
    "      --- /path/to/original timestamp\n",
    "      +++ /path/to/new  timestamp\n",
    "      @@ -1,3 +1,9 @@\n",
    "      +This is an important\n",
    "      +notice! It should\n",
    "      +therefore be located at\n",
    "      +the beginning of this\n",
    "      +document!\n",
    "      +\n",
    "       This part of the\n",
    "       document has stayed the\n",
    "       same from version to\n",
    "      @@ -8,13 +14,8 @@\n",
    "       compress the size of the\n",
    "       changes.\n",
    "      -This paragraph contains\n",
    "      -text that is outdated.\n",
    "      -It will be deleted in the\n",
    "      -near future.\n",
    "      -\n",
    "       It is important to spell\n",
    "      -check this dokument. On\n",
    "      +check this document. On\n",
    "       the other hand, a\n",
    "       misspelled word isn't\n",
    "       the end of the world.\n",
    "      @@ -22,3 +23,7 @@\n",
    "       this paragraph needs to\n",
    "       be changed. Things can\n",
    "       be added after it.\n",
    "      +\n",
    "      +This paragraph contains\n",
    "      +important new additions\n",
    "      +to this document.\n",
    "    \"\"\"\n",
    "\n",
    "    diff: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Changes in a code repository correctly represented in 'diff' format, \"\n",
    "            \"correctly escaped so it could be used in a JSON\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def refactor(new_requirements: str, program: Program) -> Diff:\n",
    "    program_description = \"\\n\".join(\n",
    "        [f\"{code.file_name}\\n[[[\\n{code.body}\\n]]]\\n\" for code in program.files]\n",
    "    )\n",
    "    return client.chat.completions.create(\n",
    "        # model=\"gpt-3.5-turbo-0613\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        response_model=Diff,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a world class programming AI capable of refactor \"\n",
    "                \"existing python repositories. You will name files correct, include \"\n",
    "                \"__init__.py files and write correct python code, with correct imports. \"\n",
    "                \"You'll deliver your changes in valid 'diff' format so that they could \"\n",
    "                \"be applied using the 'patch' command. \"\n",
    "                \"Make sure you put the correct line numbers, \"\n",
    "                \"and that all lines that must be changed are correctly marked.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": new_requirements,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": program_description,\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "\n",
    "changes = refactor(\n",
    "    new_requirements=\"Refactor this code to use flask instead.\",\n",
    "    program=program,\n",
    ")\n",
    "print(changes.diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering Questions with Validated Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use Instructor with validators to not only add citations to answers generated but also prevent hallucinations by ensuring that every statement made by the LLM is backed up by a direct quote from the context provided, and that those quotes exist!.Two Python classes, Fact and QuestionAnswer, are defined to encapsulate the information of individual facts and the entire answer, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnswer(question='What did the author do during college?', answer=[Fact(fact='The author studied Computational Mathematics and physics at university.', substring_quote=['studied Computational Mathematics and physics']), Fact(fact='The author started the Data Science club at the University of Waterloo.', substring_quote=['started the Data Science club at the University of Waterloo']), Fact(fact='The author was the president of the Data Science club for 2 years.', substring_quote=['president of the club for 2 years']), Fact(fact='The author worked at many companies as part of a co-op program, including Stitchfix and Facebook.', substring_quote=['worked at many companies including Stitchfix, Facebook'])])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import Field, BaseModel, model_validator, FieldValidationInfo\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "class Fact(BaseModel):\n",
    "    fact: str = Field(...)\n",
    "    substring_quote: List[str] = Field(...)\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_sources(self, info: FieldValidationInfo) -> \"Fact\":\n",
    "        text_chunks = info.context.get(\"text_chunk\", None)\n",
    "        spans = list(self.get_spans(text_chunks))\n",
    "        self.substring_quote = [text_chunks[span[0] : span[1]] for span in spans]\n",
    "        return self\n",
    "\n",
    "    def get_spans(self, context):\n",
    "        for quote in self.substring_quote:\n",
    "            yield from self._get_span(quote, context)\n",
    "\n",
    "    def _get_span(self, quote, context):\n",
    "        for match in re.finditer(re.escape(quote), context):\n",
    "            yield match.span()\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    question: str = Field(...)\n",
    "    answer: List[Fact] = Field(...)\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_sources(self) -> \"QuestionAnswer\":\n",
    "        self.answer = [fact for fact in self.answer if len(fact.substring_quote) > 0]\n",
    "        return self\n",
    "\n",
    "from openai import OpenAI\n",
    "import instructor\n",
    "\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "def ask_ai(question: str, context: str) -> QuestionAnswer:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        response_model=QuestionAnswer,\n",
    "        messages=[{\n",
    "            \"role\": \"system\", \"content\": \"You are a world class algorithm to answer questions with correct and exact citations.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{context}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
    "        ],\n",
    "        validation_context={\"text_chunk\": context},\n",
    "    )\n",
    "    \n",
    "\n",
    "question = \"What did the author do during college?\"\n",
    "context = \"\"\"\n",
    "My name is Jason Liu, and I grew up in Toronto Canada but I was born in China.\n",
    "I went to an arts high school but in university I studied Computational Mathematics and physics.\n",
    "As part of coop I worked at many companies including Stitchfix, Facebook.\n",
    "I also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.\n",
    "\"\"\"\n",
    "ask_ai(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-chatgpt-apps",
   "language": "python",
   "name": "oreilly-chatgpt-apps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
