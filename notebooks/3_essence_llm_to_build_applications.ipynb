{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Blocks of LLM Apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What kinds of Problems can we Solve with LLMs?\n",
    "\n",
    "The quick simplified answer to that is, anything with text. Now, of course is not literally ANYTHING, but almost anything that one can describe as a text based problem, can at least be partially tackled with a powerful LLM. \n",
    "\n",
    "Let's look at some of these problems:\n",
    "\n",
    "- Text Summarization: given a piece of text the model can summarize that text effectively compressing the information to a smaller chunk of text\n",
    "- Question & Answering over text: given a piece of text and a question, the model can provide the appropriate answer\n",
    "- Text Generation: given a prompt to generate some text following certain rules, the model can adhere to those rules and generate the appropriate text\n",
    "\n",
    "There are many other potential applications for LLMs, but let's stick to these 3 core problems for now. \n",
    "\n",
    "Now what an LLM app would look like for problems like these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Core Concepts\n",
    "\n",
    "- __Prompt__\n",
    "\n",
    "- __Interface__\n",
    "\n",
    "A __Prompt__ is text input that will serve as the thing that the user gives to the model.\n",
    "\n",
    "An __Interface__, is the UI that the user will interact with to access the LLM model functionalities.\n",
    "\n",
    "The LLM App will join both these concepts into one environment that is well suited to solve the task/problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM App\n",
    "Let's start this by looking at an LLM App for what it is. An App! So as such, you'll naturally expect to have at least 2 major components:\n",
    "\n",
    "- A frontend\n",
    "- A backend\n",
    "\n",
    "\n",
    "<img src=\"./images/frontend_backend.png\" alt=\"Frontend Backend Image\" width=500>\n",
    "\n",
    "On the frontend side, you'd expect to see things like the UI, the user interface with which the user will interact.\n",
    "\n",
    "<img src=\"./images/frontend_llm_app.png\" alt=\"Frontend Backend Image\" width=500>\n",
    "\n",
    "On the backend side, you'd expect to have the actual model, the LLM communicating with the frontend through some intermediary process that manages the inputs from the user to the LLM and vice-versa.\n",
    "\n",
    "<img src=\"./images/backend_llm_app1.png\" alt=\"Backend LLM App\" width=\"500\">\n",
    "\n",
    "We'll also have some sort of prompt management going on in the backend, to make sure the app is working properly.\n",
    "\n",
    "<img src=\"./images/backend_llm_app2.png\" alt=\"Backend LLM App 2\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to provide some practical context, let's examine different abstraction layer levels while building LM apps. Our focus will be on a quiz generator as an illustrative example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Levels of an LLM App\n",
    "\n",
    "Let's take a practical approach, let's start by setting up access to some LLM, and let's interact with it a little bit at different levels of abstraction.\n",
    "\n",
    "## Level 1 - Calling the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Differentiate the following functions:\\n\\na) f(x) = 3x^2 + 4x^3 - 2x\\nb) g(x) = sin(x) + cos(x)\\nc) h(x) = 2e^x - 4ln(x)\\n\\n2. Find the indefinite integral of the following functions:\\n\\na) f(x) = 2x^3 + 5x^2 - 3x + 1\\nb) g(x) = sec^2(x) + tan(x)\\nc) h(x) = e^x + ln(x) - 1\\n\\n3. Use the chain rule to differentiate the following composite functions:\\n\\na) f(x) = (3x^2 + 5x)^4\\nb) g(x) = sin(2x^3 + 7x)\\nc) h(x) = e^(2x^2 + 4x)\\n\\n4. Calculate the limits of the following functions:\\n\\na) lim(x->2) (x^3 - 8)/(x - 2)\\nb) lim(x->0) (sin(2x))/x\\nc) lim(x->infinity) (3x^2 + 2x)/(4x^2 - 5)\\n\\n5. Find the definite integral of the following functions over the given intervals:\\n\\na) ∫(2x + 3) dx, from x = 1 to x = 4\\nb) ∫(cos(x)) dx, from x = 0 to x = π/2\\nc) ∫(x^2 + 2x + 3) dx, from x = -2 to x = 2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "def llm_model(prompt_question):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and\\\n",
    "            programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "prompt = \"Give me 5 exercises to practice calculus\"\n",
    "response = llm_model(prompt)\n",
    "response\n",
    "# Output\n",
    "# '1. Differentiate the following functions:\n",
    "# \\n\\na) f(x) = 3x^2 + 4x^3 - 2x\\nb) g(x) = sin(x) + cos(x)\\nc)\n",
    "# h(x) = 2e^x - 4ln(x)\\n\\n2. \n",
    "# ......\n",
    "# ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to give the user, **specialized freedom**.\n",
    "\n",
    "What I mean is freedom to specify anything within the confounds of the application and its context, in this case, anything related to the sucesfful creation of quizzes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the essential questions for building llm apps is how can we give the user specialized freedom to do that through clever pre and post prompting of the model? \n",
    "\n",
    "As well as any other type of preparation to make sure we are giving the most flexibility to the user within the context of our app?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2 - LLM API Call + UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "# Example of a language model\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8182d48efa2240df854aad36f4a3a327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Prompt:', placeholder='Enter the prompt for the model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae4b5ff1a564e7f99a45cbb64c6c9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Ask ChatGPT', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96de958506af4556a18b8e5d8732f8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='ChatGPT response:', disabled=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def llm_model(prompt_question):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and\\\n",
    "            programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def handle_submit(sender):\n",
    "    input_text = text_input.value\n",
    "    text_completion = llm_model(input_text)\n",
    "    output_text.value = text_completion\n",
    "\n",
    "# Create the text input widget and submit button.\n",
    "text_input = widgets.Text(placeholder='Enter the prompt for the model',\n",
    "                          description='Prompt:')\n",
    "submit_button = widgets.Button(description='Ask ChatGPT')\n",
    "# Create the output widget to display the generated random text.\n",
    "output_text = widgets.Textarea(description='ChatGPT response:', disabled=True)\n",
    "\n",
    "# Register the submit button's click event to call the handle_submit function.\n",
    "submit_button.on_click(handle_submit)\n",
    "\n",
    "# Display the widgets.\n",
    "display(text_input, submit_button, output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a super simple interface with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 3 - Adding Prompt Management\n",
    "\n",
    "\n",
    "### Pre-prompting example\n",
    "\n",
    "We can for example, specify some desirable behaviors we want from the LLM like\n",
    "\n",
    "*\"Act as an expert researcher and learning assistant and you will help students create instructive quizzes on any subject matter\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97a7391de31424f9417dd5dc925de01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Prompt:', placeholder='Enter the prompt for the model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ec23973cc64a2a94c99cd598bc8292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Ask ChatGPT', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bc5333ebcb4d70a5394785f122341b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='ChatGPT response:', disabled=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "# Example of a language model\n",
    "import openai\n",
    "\n",
    "def llm_model(prompt_question):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and\\\n",
    "            programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def handle_submit(sender):\n",
    "    pre_prompt = \"Act as an expert researcher and learning assistant and you will help students create instructive quizzes on any subject matter\"\n",
    "    input_text = text_input.value\n",
    "    input_text = pre_prompt + \" \" + input_text\n",
    "    print(\"Full prompt for the model: \", input_text)\n",
    "    text_completion = llm_model(input_text)\n",
    "    output_text.value = text_completion\n",
    "\n",
    "# Create the text input widget and submit button.\n",
    "text_input = widgets.Text(placeholder='Enter the prompt for the model',\n",
    "                          description='Prompt:')\n",
    "submit_button = widgets.Button(description='Ask ChatGPT')\n",
    "# Create the output widget to display the generated random text.\n",
    "output_text = widgets.Textarea(description='ChatGPT response:', disabled=True)\n",
    "# Register the submit button's click event to call the handle_submit function.\n",
    "submit_button.on_click(handle_submit)\n",
    "# Display the widgets.\n",
    "display(text_input, submit_button, output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-prompting example\n",
    "\n",
    "**Grammar check**\n",
    "\n",
    "**Post-Prompt**: *\"Correct any grammar mistakes in the following text and return the corrected text\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175021bde4464f009c48cf784dd48ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Prompt:', placeholder='Enter the prompt for the model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b2fda983304b48b9891ee1891d0a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Ask ChatGPT', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c2215ccbbc4ad489d4757bce1a5320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='ChatGPT response:', disabled=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full prompt for the model:  Act as an expert researcher and learning assistant and you will help students create instructive quizzes on any subject matter Write me 3 exercises about how the brain works.\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "# Example of a language model\n",
    "import openai\n",
    "\n",
    "def llm_model(prompt_question):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and\\\n",
    "            programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def handle_submit(sender):\n",
    "    pre_prompt = \"Act as an expert researcher and learning assistant and you will help students create instructive quizzes on any subject matter\"\n",
    "    input_text = text_input.value\n",
    "    input_text = pre_prompt + \" \" + input_text\n",
    "    print(\"Full prompt for the model: \", input_text)\n",
    "    text_completion = llm_model(input_text)\n",
    "    prompt_grammar = \"Correct any grammar mistakes in the following text and return the corrected text\"\n",
    "    text_completion_grammar_checked = llm_model(prompt_grammar + text_completion)\n",
    "    output_text.value = text_completion_grammar_checked\n",
    "\n",
    "# Create the text input widget and submit button.\n",
    "text_input = widgets.Text(placeholder='Enter the prompt for the model',\n",
    "                          description='Prompt:')\n",
    "submit_button = widgets.Button(description='Ask ChatGPT')\n",
    "# Create the output widget to display the generated random text.\n",
    "output_text = widgets.Textarea(description='ChatGPT response:', disabled=True)\n",
    "# Register the submit button's click event to call the handle_submit function.\n",
    "submit_button.on_click(handle_submit)\n",
    "# Display the widgets.\n",
    "display(text_input, submit_button, output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to building an LLM app, constructing an environment that prepares the LLM to solve the task at hand, as well as set up a UI that best matches the context of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Limits Lesson\n",
    "# Understanding Token Management in the ChatGPT API\n",
    "\n",
    "## Tokens: The Building Blocks of Text\n",
    "\n",
    "In language models, text is parsed and interpreted in chunks known as 'tokens'. In English, tokens can be as short as a character or as long as a word. For instance, \"a\" or \"apple\" can each represent a token. It's important to note that in different languages, tokens can vary in length.\n",
    "\n",
    "Consider the string \"ChatGPT is great!\". The language model breaks this down into six tokens: [\"Chat\", \"G\", \"PT\", \" is\", \" great\", \"!\"].\n",
    "\n",
    "## The Role of Tokens in API Calls\n",
    "\n",
    "Tokens play a crucial role in API calls. Their significance lies in three primary areas:\n",
    "\n",
    "1. **Cost:** Your API call cost is calculated per token.\n",
    "2. **Time:** The duration of your API call is influenced by the number of tokens as writing more tokens takes more time.\n",
    "3. **Functionality:** An API call can only function if the total tokens used are below the model's maximum limit (for example, 4096 tokens for gpt-3.5-turbo).\n",
    "\n",
    "Remember, both the input and output tokens count toward these quantities. For instance, if your API call uses 10 tokens for message input and receives 20 tokens in the message output, you'll be billed for a total of 30 tokens.\n",
    "\n",
    "To check the number of tokens used by an API call, you can refer to the usage field in the API response (for example, response['usage']['total_tokens']).\n",
    "\n",
    "## Counting Tokens in Chat API Calls\n",
    "\n",
    "The process of counting tokens for chat API calls requires careful attention. Despite chat models like gpt-3.5-turbo and gpt-4 using tokens similarly to models in the completions API, the message-based formatting makes it challenging to estimate the number of tokens used in a conversation.\n",
    "\n",
    "Let's take a deep dive into counting tokens for chat API calls.\n",
    "\n",
    "To help with this process, you can use the example function provided below, which counts tokens for messages passed to the gpt-3.5-turbo model. However, bear in mind that the method of converting messages into tokens may differ between models. Hence, the answers returned by this function may only be approximate for future model versions.\n",
    "\n",
    "```python\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo\"):\n",
    "  # Function definition goes here\n",
    "```\n",
    "\n",
    "You can create a message and pass it to the function defined above to see the token count. This should match the value returned by the API usage parameter. An example of such a function call is:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "]\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "print(f\"{num_tokens_from_messages(messages, model)} prompt tokens counted.\")\n",
    "```\n",
    "\n",
    "To verify the number generated by our function above, you can create a new Chat Completion and compare the value with the API returned token usage:\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens used.')\n",
    "```\n",
    "\n",
    "For examining token count in a text string without making an API call, you can use OpenAI’s `tiktoken` Python library. \n",
    "\n",
    "It's important to note that if a conversation surpasses a model's maximum token limit (e.g., more than 4096 tokens for gpt-3.5-turbo), you will need to truncate, omit, or shrink your text until it fits. Also, very long conversations are more likely to receive incomplete replies due to token constraints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `tiktoken` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count:  6\n",
      "Tokenized sentence:  ['Chat', 'G', 'PT', ' is', ' useful', '!']\n"
     ]
    }
   ],
   "source": [
    "# !pip install tiktoken\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "sentence = \"ChatGPT is useful!\"\n",
    "\n",
    "print(\"Token count: \", len(enc.encode(sentence)))\n",
    "\n",
    "print(\"Tokenized sentence: \", [enc.decode([tk]) for tk in enc.encode(sentence)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input Price ($/1K tokens)</th>\n",
       "      <th>Output Price ($/1K tokens)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-4 8K context</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4 32K context</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPT-3.5 Turbo 4K context</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GPT-3.5 Turbo 16K context</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  Input Price ($/1K tokens)  \\\n",
       "0           GPT-4 8K context                     0.0300   \n",
       "1          GPT-4 32K context                     0.0600   \n",
       "2   GPT-3.5 Turbo 4K context                     0.0015   \n",
       "3  GPT-3.5 Turbo 16K context                     0.0030   \n",
       "\n",
       "   Output Price ($/1K tokens)  \n",
       "0                       0.060  \n",
       "1                       0.120  \n",
       "2                       0.002  \n",
       "3                       0.004  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Model': ['GPT-4 8K context', 'GPT-4 32K context', 'GPT-3.5 Turbo 4K context', 'GPT-3.5 Turbo 16K context'],\n",
    "    'Input Price ($/1K tokens)': [0.03, 0.06, 0.0015, 0.003],\n",
    "    'Output Price ($/1K tokens)': [0.06, 0.12, 0.002, 0.004]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count:  4\n",
      "Price input: 2.6000000000000002e-05 - 0.004006 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.6000000000000002e-05, 0.004006)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_num_tokens = 1000\n",
    "prompt_tokens = 200\n",
    "\n",
    "\n",
    "def calculate_num_tokens(prompt):\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    prompt_size = len(enc.encode(prompt))\n",
    "    print(\"Token count: \", prompt_size)\n",
    "    \n",
    "    return prompt_size\n",
    "\n",
    "\n",
    "def calculate_cost_of_prompt(prompt, max_num_tokens, price_per_1000_tk_input = 0.0015, price_per_1000_tk_output = 0.002):\n",
    "    \n",
    "    prompt_tokens = calculate_num_tokens(prompt)\n",
    "    price_input = (prompt_tokens * price_per_1000_tk_input) / 1000\n",
    "    price_output_min = 10 * (price_per_1000_tk_output) / 1000\n",
    "    price_output_max = max_num_tokens * (price_per_1000_tk_output) / 1000\n",
    "    price_total_min = price_input + price_output_min\n",
    "    price_total_max = price_input + price_output_max\n",
    "    print(f\"Price input: {price_total_min} - {price_total_max} \", )\n",
    "    \n",
    "    return price_total_min, price_total_max\n",
    "\n",
    "\n",
    "prompt = \"Tell me a joke\"\n",
    "max_num_tokens = 2000\n",
    "calculate_cost_of_prompt(prompt, max_num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "# let's verify the function above matches the OpenAI API response\n",
    "\n",
    "import openai\n",
    "\n",
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"New synergies will help drive top-line growth.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Things working well together will increase revenue.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for model in [\n",
    "    \"gpt-3.5-turbo-0301\",\n",
    "    \"gpt-3.5-turbo-0613\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4-0314\",\n",
    "    \"gpt-4-0613\",\n",
    "    \"gpt-4\",\n",
    "    ]:\n",
    "    print(model)\n",
    "    # example token count from the function defined above\n",
    "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
    "    # example token count from the OpenAI API\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=example_messages,\n",
    "        temperature=0,\n",
    "        max_tokens=1,  # we're only counting input tokens here, so let's not waste tokens on the output\n",
    "    )\n",
    "    print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Build LLM Systems with the ChatGPT API](https://learn.deeplearning.ai/chatgpt-building-system/lesson/2/language-models,-the-chat-format-and-tokens)\n",
    "\n",
    "- [FSDL LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/llmops/)\n",
    "- [Visual guide to llm-powered app architecture](https://medium.com/@remitoffoli/a-visual-guide-to-llm-powered-app-architecture-57e47426a92f)\n",
    "- [Anatomy of LLM-Based Chatbot Applications](https://towardsdatascience.com/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e)\n",
    "- [streamlit langchain llm app architecture](https://blog.streamlit.io/langchain-tutorial-1-build-an-llm-powered-app-in-18-lines-of-code/)\n",
    "- [ChatGPT API documentation](https://platform.openai.com/docs/guides/gpt/chat-completions-api)\n",
    "- [Token Limits](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb)\n",
    "- [Openai Cookbook - How to format inputs for ChatGPT models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly_env",
   "language": "python",
   "name": "oreilly_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
