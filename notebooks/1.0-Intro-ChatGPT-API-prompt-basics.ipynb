{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ChatGPT API\n",
    "\n",
    "- Where does ChatGPT fit into this chaotic universe?\n",
    "- The ChatGPT API (what’s the deal?)\n",
    "- How to use it, basics, parameters, simple examples, etc…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request body for the CHATGPT API involves many parameters, but let's focus on the following:\n",
    "\n",
    "- model: ID of the model to use.\n",
    "- messages: a list of messages comprising the conversation up to that point\n",
    "- temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
    "- n: number of chat completion choices to generate for each input message\n",
    "- max_tokens: the maximum number of tokens to generate in the chat completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# loading from a .env file\n",
    "# load_dotenv(dotenv_path=\"/full/path/to/your/.env\")\n",
    "\n",
    "# or \n",
    "# if you're on google colab just uncomment below and replace with your openai api key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<your-openai-api-key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9MwGTXLCatD2bA2G0YAdFXmS2ASdu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The meaning of life is a deeply personal and subjective concept that can vary greatly from person to person. Some people find meaning in relationships, others in their work or passions, and some find meaning in spiritual beliefs. Ultimately, the meaning of life is something each individual must explore and define for themselves. It can be found through introspection, self-discovery, and living in alignment with one's values and purpose.\", role='assistant', function_call=None, tool_calls=None))], created=1715254213, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=82, prompt_tokens=34, total_tokens=116))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the meaning of life?\"\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "                             model=\"gpt-3.5-turbo\",\n",
    "                             messages=\n",
    "                             [\n",
    "                                 {\"role\": \"system\", \"content\": \"You are a savy guru with knowledge about existence and the secrets of life.\"},\n",
    "                                 {\"role\": \"user\", \"content\": prompt}   \n",
    "                             ],\n",
    "                             max_tokens=100, # the maximum number of tokens to generate\n",
    "                             temperature=0.9, # the level of randomness in the generated text, so close to 0 means super precise, close to 2 means more creative\n",
    "                             n = 1)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response ID: chatcmpl-9MwGTXLCatD2bA2G0YAdFXmS2ASdu\n",
      "Choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The meaning of life is a deeply personal and subjective concept that can vary greatly from person to person. Some people find meaning in relationships, others in their work or passions, and some find meaning in spiritual beliefs. Ultimately, the meaning of life is something each individual must explore and define for themselves. It can be found through introspection, self-discovery, and living in alignment with one's values and purpose.\", role='assistant', function_call=None, tool_calls=None))]\n",
      "Created: 1715254213\n",
      "Model: gpt-3.5-turbo-0125\n",
      "System Fingerprint: None\n",
      "Object Type: chat.completion\n",
      "Usage: CompletionUsage(completion_tokens=82, prompt_tokens=34, total_tokens=116)\n"
     ]
    }
   ],
   "source": [
    "print(\"Response ID:\", response.id)\n",
    "print(\"Choices:\", response.choices)\n",
    "print(\"Created:\", response.created)\n",
    "print(\"Model:\", response.model)\n",
    "print(\"System Fingerprint:\", response.system_fingerprint)\n",
    "print(\"Object Type:\", response.object)\n",
    "print(\"Usage:\", response.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://platform.openai.com/docs/api-reference/chat/object\n",
    "\n",
    "### Chat Completion Object\n",
    "This object represents a chat completion response returned by the model, based on the provided input.\n",
    "\n",
    "#### Fields\n",
    "\n",
    "- **id**  \n",
    "  `string`  \n",
    "  A unique identifier for the chat completion.\n",
    "\n",
    "- **choices**  \n",
    "  `array`  \n",
    "  A list of chat completion choices. Can be more than one if `n` is greater than 1.\n",
    "\n",
    "#### Additional Properties\n",
    "\n",
    "- **created**  \n",
    "  `integer`  \n",
    "  The Unix timestamp (in seconds) of when the chat completion was created.\n",
    "\n",
    "- **model**  \n",
    "  `string`  \n",
    "  The model used for the chat completion.\n",
    "\n",
    "- **system_fingerprint**  \n",
    "  `string`  \n",
    "  This fingerprint represents the backend configuration that the model runs with. It can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.\n",
    "\n",
    "- **object**  \n",
    "  `string`  \n",
    "  The object type, which is always `chat.completion`.\n",
    "\n",
    "- **usage**  \n",
    "  `object`  \n",
    "  Usage statistics for the completion request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bamboo munching bears,\n",
      "Black and white, so adorable,\n",
      "Pandas do not care.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def get_response(prompt, system_message=\"You are a savy guru with knowledge about existence and the secrets of life.\"):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(model=\"gpt-3.5-turbo-1106\", \n",
    "                             messages=\n",
    "                             [\n",
    "                                 {\"role\": \"system\", \"content\": system_message},\n",
    "                                 {\"role\": \"user\", \"content\": prompt}   \n",
    "                             ],\n",
    "                             max_tokens=100, # max number of tokens in a response\n",
    "                             temperature=0.9, # level of randomness in the response\n",
    "                             n = 1) # number of responses we expect from the model\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "prompt = \"Tell me a joke as a haiku about Pandas\"\n",
    "response = get_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, my name is Lucas Soares and I like to give courses on language models. [english]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message_prompt=\"You are translation engine for any language.\\\n",
    "    The user will feed you with some text and the target language \\\n",
    "        in between brackets: like this: '[english]' \\\n",
    "            and you will output only the translation.\" \n",
    "\n",
    "def translate(prompt, system_message):\n",
    "    client = OpenAI()\n",
    "    \n",
    "    response = client.chat.completions.create(model=\"gpt-3.5-turbo-1106\",\n",
    "                                messages=\n",
    "                                [\n",
    "                                    {\"role\": \"system\", \"content\": system_message},\n",
    "                                    {\"role\": \"user\", \"content\": prompt}   \n",
    "                                ],\n",
    "                                max_tokens=100,\n",
    "                                temperature=0.9,\n",
    "                                n = 1)\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "translate(\"Ola meu nome e Lucas Soares e eu gosto de dar cursos sobre models de linguage.\",system_message_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Basics\n",
    "\n",
    "A prompt is a piece of text that conveys to a LLM what the user wants. What the user wants can be many things like:\n",
    "\n",
    "- Asking a question\n",
    "- Giving an instruction\n",
    "- Etc...\n",
    "\n",
    "The key components of a prompt are:\n",
    "1. Instruction: where you describe what you want\n",
    "2. Context: additional information to help with performance\n",
    "3. Input data: data the model has not seem to illustrate what you need\n",
    "4. Output indicator: How you prime the model to output what you want, for example asking the model [\"Let's think step by step\" and the end of a prompt can boost reasoning performance](https://arxiv.org/pdf/2201.11903.pdf). You can also write \"Output:\" to prime the model to just write the output and nothing else.\n",
    "\n",
    "[Prompts can also be seen as a form of programming that can customize the outputs and interactions with an LLM.](https://ar5iv.labs.arxiv.org/html/2302.11382#:~:text=prompts%20are%20also%20a%20form%20of%20programming%20that%20can%20customize%20the%20outputs%20and%20interactions%20with%20an%20llm.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Classify this sentence into positive or negative:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Data & Context Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"Text: I like eating pancakes.\"\n",
    "\n",
    "context = \"You are a text annotation engine.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_indicator = \"Output:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How you ask what you want, and the heavily relies on what you want from the model.\n",
    "# Instruction prompt: \n",
    "\n",
    "instruction_prompt = f\"{context}.\\n{instruction}.\\n{input_data}. {output_indicator}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a text annotation engine..\\nClassify this sentence into positive or negative:.\\nText: I like eating pancakes.. Output:\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Output: Positive'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "get_response(instruction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_indicator = \"The output should ONLY be either the word 'positive' or 'negative' and nothing else. Output:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_prompt = f\"{context}.\\n{instruction}.\\n{input_data}. {output_indicator}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(instruction_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first super simple prompt engineering experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTICE ROUND\n",
    "# WRITE A SIMPLE PROMPT OUTLINING EACH COMPONENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_prompt = \"\"\n",
    "context = \"\"\n",
    "input_data = \"\"\n",
    "output_indicator = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AF!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, broccoli is considered to be a healthy food. It is low in calories and high in nutrients, including fiber, vitamins C and K, folate, and minerals. It is also rich in antioxidants and has been associated with various health benefits, such as reducing the risk of certain cancers and improving digestion.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = \"You are a healthy diet assistant you answer questions about food\"\n",
    "instruction = \"Is healthy to eat the following food?\" \n",
    "input_data = \"[food]: brocoli\"\n",
    "output_indicator = \"[answer]: \"\n",
    "\n",
    "instruction_prompt = f\"{context}.\\n{instruction}.\\n{input_data}. {output_indicator}\"\n",
    "\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": instruction_prompt}\n",
    "])\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's a joke for you: Why don't scientists trust atoms? Because they make up everything!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "def get_response(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # describes overall behavior\n",
    "            {\"role\": \"user\", \"content\": prompt} #message from the user\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "    \n",
    "prompt = \"Tell me a joke\"\n",
    "get_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT](https://ar5iv.labs.arxiv.org/html/2302.11382)\n",
    "- [Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)\n",
    "- [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "- [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/pdf/2107.13586.pdf)\n",
    "- [prompt engineering guide - zero shot prompting example](https://www.promptingguide.ai/techniques/zeroshot)\n",
    "- [Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)\n",
    "- [prompt engineering guide - few shot prompting](https://www.promptingguide.ai/techniques/fewshot)\n",
    "- [prompt engineering guide - chain of thought prompting](https://www.promptingguide.ai/techniques/cot)\n",
    "- [Wei et al. (2022)](https://arxiv.org/abs/2201.11903)\n",
    "- [prompt engineering guide - self-consistency](https://www.promptingguide.ai/techniques/consistency)\n",
    "- [prompt engineering guide - generate knowledge](https://www.promptingguide.ai/techniques/knowledge)\n",
    "- [Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf)\n",
    "- [prompt engineering guide - tree of thoughts (ToT)](https://www.promptingguide.ai/techniques/tot)\n",
    "- [Prompt Engineering by Lilian Weng](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n",
    "- [Prompt Engineering vs. Blind Prompting](https://mitchellh.com/writing/prompt-engineering-vs-blind-prompting#the-demonstration-set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-chatgpt-apps",
   "language": "python",
   "name": "oreilly-chatgpt-apps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
