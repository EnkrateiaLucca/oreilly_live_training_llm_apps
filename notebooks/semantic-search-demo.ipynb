{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T03:30:15.210098Z",
     "iopub.status.busy": "2025-01-08T03:30:15.209656Z",
     "iopub.status.idle": "2025-01-08T03:30:15.416294Z",
     "shell.execute_reply": "2025-01-08T03:30:15.416018Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def semantic_search(query, pdf_text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": f\"You answer \\\n",
    "            semantic search questions ONLY ABOUT THE FOLLOWING TEXT: \\\n",
    "                {pdf_text}\"},\n",
    "                  {\"role\": \"user\", \"content\": query}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T03:30:15.417706Z",
     "iopub.status.busy": "2025-01-08T03:30:15.417636Z",
     "iopub.status.idle": "2025-01-08T03:30:21.352924Z",
     "shell.execute_reply": "2025-01-08T03:30:21.352132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. \\n\\nIn the context of the Transformer model, there are two primary forms of attention used:\\n\\n1. **Scaled Dot-Product Attention**: This involves computing the dot products of the query with all keys, dividing each by the square root of the key dimension, and applying a softmax function to obtain the weights on the values. The formula for this attention is:\\n   \\\\[\\n   Attention(Q, K, V) = softmax\\\\left(\\\\frac{QK^T}{\\\\sqrt{d_k}}\\\\right)V\\n   \\\\]\\n\\n2. **Multi-Head Attention**: Instead of performing a single attention function, the model linearly projects the queries, keys, and values multiple times (h times with different learned linear projections) and performs the attention function in parallel. This allows the model to jointly attend to information from different representation subspaces at different positions. The formula for multi-head attention is:\\n   \\\\[\\n   MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W_O\\n   \\\\]\\n   where each head is computed as:\\n   \\\\[\\n   head_i = Attention(QW_Q^i, KW_K^i, V W_V^i)\\n   \\\\]\\n\\nThese attention mechanisms allow the model to capture dependencies between different input and output positions, enhancing its ability to model complex sequences.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf_text(file_path):\n",
    "    '''Loads text from a PDF file.'''\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(file_path)\n",
    "\n",
    "    # extracting text from page\n",
    "    text = \"\\n\\n\".join([page.extract_text() for page in reader.pages])\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "pdf_path = \"./assets-resources/attention-paper.pdf\"\n",
    "pdf_text = load_pdf_text(pdf_path)\n",
    "\n",
    "query = \"What is the attention mechanism?\"\n",
    "semantic_search(query, pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T03:30:21.390984Z",
     "iopub.status.busy": "2025-01-08T03:30:21.390733Z",
     "iopub.status.idle": "2025-01-08T03:30:23.959176Z",
     "shell.execute_reply": "2025-01-08T03:30:23.958000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SemanticSearchResponse(answer='An attention mechanism is a technique in neural networks that enables the model to focus on specific parts of the input sequence when producing each element of the output sequence. It works by mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, with the weights determined by a compatibility function that measures how well each key matches the query. In the case of the Transformer model, a specific type called \"Scaled Dot-Product Attention\" is employed, where the compatibility is calculated by taking the dot product of the query and keys, scaling by the square root of the dimensionality of the keys, and applying a softmax function to obtain the weights on the values.', quote_sources=['Section 3.2 Attention', 'Section 3.2.1 Scaled Dot-Product Attention'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "class SemanticSearchResponse(BaseModel):\n",
    "    answer: str = Field(..., description=\"The answer to the semantic search question\")\n",
    "    quote_sources: List[str] = Field(..., description=\"The source of the quote\")\n",
    "\n",
    "def structured_semantic_search(query, pdf_text):\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": f\"You answer \\\n",
    "            semantic search questions ONLY ABOUT THE FOLLOWING TEXT: \\\n",
    "                {pdf_text}\"},\n",
    "                  {\"role\": \"user\", \"content\": query}],\n",
    "        response_format=SemanticSearchResponse\n",
    "    )\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "query = \"What is the attention mechanism?\"\n",
    "output = structured_semantic_search(query, pdf_text)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T03:30:23.962443Z",
     "iopub.status.busy": "2025-01-08T03:30:23.962258Z",
     "iopub.status.idle": "2025-01-08T03:30:23.965635Z",
     "shell.execute_reply": "2025-01-08T03:30:23.965020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Section 3.2 Attention', 'Section 3.2.1 Scaled Dot-Product Attention']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.answer\n",
    "output.quote_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-chatgpt-apps",
   "language": "python",
   "name": "oreilly-chatgpt-apps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
